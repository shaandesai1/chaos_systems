\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hornik_multilayer_1989}
\citation{he_mask_2018,devlin_bert_2019,toussaint_differentiable_2018,yao_tensormol-01_2018}
\citation{greydanus_hamiltonian_2019,pukrittayakamee_simultaneous_2009}
\citation{mattheakis_hamiltonian_2020,greydanus_hamiltonian_2019}
\citation{cranmer_lagrangian_2020,lutter_deep_2019}
\citation{chen_neural_2018}
\citation{raissi_physics_2017}
\citation{battaglia_interaction_2016,sanchez-gonzalez_hamiltonian_2019}
\citation{lutter_deep_2019,zhong_dissipative_2020}
\citation{zhong_dissipative_2020}
\citation{greydanus_hamiltonian_2019}
\citation{greydanus_hamiltonian_2019}
\newlabel{eqn.hamiltonian}{{1}{1}{}{equation.2.1}{}}
\newlabel{eqn.action_int}{{2}{1}{}{equation.2.2}{}}
\citation{zhong_dissipative_2020}
\citation{zhong_dissipative_2020}
\citation{raissi_physics_2017,raissi_physics-informed_2019}
\citation{mattheakis_hamiltonian_2020}
\citation{chen_neural_2018}
\citation{zhu_deep_2020}
\citation{battaglia_interaction_2016}
\citation{sanchez-gonzalez_graph_2018,sanchez-gonzalez_learning_2020,cranmer_lagrangian_2020}
\citation{cranmer_lagrangian_2020}
\citation{greydanus_hamiltonian_2019}
\citation{lutter_deep_2019}
\citation{finzi_generalizing_2020}
\newlabel{eqn.pham}{{3}{2}{}{equation.2.3}{}}
\newlabel{eqn.pham1}{{4}{3}{Theory}{equation.4.4}{}}
\newlabel{fig.architecture}{{1}{3}{Architectures used to learn dynamics in this paper. The naive extension of classic NN and Hamiltonian NN (top left) is to incorporate time as an additional input variable (top right). Our innovation, which exploits Port-Hamiltonians, explicitly learns the force $F_{\theta _2}$ as well as the damping coefficient $\nu _{\theta _3}$}{figure.1}{}}
\newlabel{eqn.loss}{{5}{3}{Training}{equation.4.5}{}}
\citation{greydanus_hamiltonian_2019}
\newlabel{mspring}{{5}{6}{The simple mass-spring system has no explicit time dependence. We see that TDHNN4 can almost recover the dynamics as well as in HNN. The baseline NN and TDHNN are unable to achieve the same test state error as they are only reliable for time steps that are within the training regime}{figure.5}{}}
\newlabel{damped}{{8}{7}{Damped mass-spring setting: The baseline NN and TDHNN4 recover the underlying dynamics. TDHNN4 is also able to accurately learn the damping coefficient since the predicted damping is indistinguishable from the ground truth}{figure.8}{}}
\newlabel{fig.fmspring1}{{11}{7}{Forced mass-spring (I): HNN cannot learn the underlying dynamics as it has no explicit-time dependence. TDHNN4 shows the best performance as it explicitly learns a time-dependent force}{figure.11}{}}
\newlabel{fig.fmspring2}{{14}{8}{Forced mass-spring (II):The time dependent force here is non-trivial, but TDHNN4 shows it can recover it}{figure.14}{}}
\newlabel{fig.duffing}{{17}{8}{Duffing equation: TDHNN4 significantly outperforms the other methods and is able to extract the ground truth force and damping coefficient}{figure.17}{}}
\newlabel{duffing_ham}{{21}{10}{Learnt $\mathcal {H}_{reg}$ components across methods in the non-chaotic Duffing setting. HNN and TDHNN learn distorted Hamiltonians that strongly depend on the input time-variable}{figure.21}{}}
