
@misc{noauthor_[1703.06870]_nodate,
	title = {[1703.06870] {Mask} {R}-{CNN}},
	url = {https://arxiv.org/abs/1703.06870},
	urldate = {2019-09-18},
	file = {[1703.06870] Mask R-CNN:/Users/shaan/Zotero/storage/FSP9QKDP/1703.html:text/html}
}

@misc{noauthor_[1901.01892]_nodate,
	title = {[1901.01892] {Scale}-{Aware} {Trident} {Networks} for {Object} {Detection}},
	url = {https://arxiv.org/abs/1901.01892},
	urldate = {2019-09-18},
	file = {[1901.01892] Scale-Aware Trident Networks for Object Detection:/Users/shaan/Zotero/storage/VVN5FPHD/1901.html:text/html}
}

@article{iten_discovering_2018,
	title = {Discovering physical concepts with neural networks},
	url = {http://arxiv.org/abs/1807.10300},
	abstract = {We introduce a neural network architecture that models the physical reasoning process and that can be used to extract simple physical concepts from experimental data without being provided with additional prior knowledge. We apply the neural network to a variety of simple physical examples in classical and quantum mechanics, like damped pendulums, two-particle collisions, and qubits. The network finds the physically relevant parameters, exploits conservation laws to make predictions, and can be used to gain conceptual insights. For example, given a time series of the positions of the Sun and Mars as observed from Earth, the network discovers the heliocentric model of the solar system - that is, it encodes the data into the angles of the two planets as seen from the Sun. Our work provides a first step towards answering the question whether the traditional ways by which physicists model nature naturally arise from the experimental data without any mathematical and physical pre-knowledge, or if there are alternative elegant formalisms, which may solve some of the fundamental conceptual problems in modern physics, such as the measurement problem in quantum mechanics.},
	urldate = {2019-10-16},
	journal = {arXiv:1807.10300 [physics, physics:quant-ph]},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jul,
	year = {2018},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics},
	file = {arXiv\:1807.10300 PDF:/Users/shaan/Zotero/storage/ADGUHULJ/Iten et al. - 2018 - Discovering physical concepts with neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/FVYAHFJP/1807.html:text/html}
}

@misc{noauthor_[1906.01563]_nodate,
	title = {[1906.01563] {Hamiltonian} {Neural} {Networks}},
	url = {https://arxiv.org/abs/1906.01563},
	urldate = {2019-10-17},
	file = {[1906.01563] Hamiltonian Neural Networks:/Users/shaan/Zotero/storage/FEVHBIDP/1906.html:text/html}
}

@article{de_silva_discovery_2019,
	title = {Discovery of {Physics} from {Data}: {Universal} {Laws} and {Discrepancy} {Models}},
	shorttitle = {Discovery of {Physics} from {Data}},
	url = {http://arxiv.org/abs/1906.07906},
	abstract = {Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of subtle and nuanced issues that must be addressed by modern data-driven methods for the automated discovery of physics. Specifically, we show that measurement noise and complex secondary physical mechanisms, such as unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. Without proposing an appropriate discrepancy model to handle these drag forces, the data supports an Aristotelian, versus a Galilean, theory of gravitation. Using the sparse identification of nonlinear dynamics (SINDy) algorithm, with the additional assumption that each separate falling object is governed by the same physical law, we are able to identify a viable discrepancy model to account for the fluid dynamic forces that explain the mismatch between a posited universal law of gravity and the measurement data. This work highlights the fact that the simple application of ML/AI will generally be insufficient to extract universal physical laws without further modification.},
	urldate = {2019-10-23},
	journal = {arXiv:1906.07906 [physics, stat]},
	author = {de Silva, Brian and Higdon, David M. and Brunton, Steven L. and Kutz, J. Nathan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.07906},
	keywords = {Computer Science - Machine Learning, Physics - Classical Physics, Statistics - Machine Learning},
	file = {arXiv\:1906.07906 PDF:/Users/shaan/Zotero/storage/3U4ZWMK3/de Silva et al. - 2019 - Discovery of Physics from Data Universal Laws and.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/NUMJY7A2/1906.html:text/html}
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	copyright = {©  . Freely available online through the PNAS open access option.},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/113/15/3932},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	language = {en},
	number = {15},
	urldate = {2019-10-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pmid = {27035946},
	keywords = {dynamical systems, machine learning, optimization, sparse regression, system identification},
	pages = {3932--3937},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/LPKUCJ8T/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf:application/pdf;Snapshot:/Users/shaan/Zotero/storage/GAKELMNY/3932.html:text/html}
}

@misc{noauthor_data-driven_nodate,
	title = {Data-driven {Discovery} of {Governing} {Physical} {Laws}},
	url = {https://sinews.siam.org/Details-Page/data-driven-discovery-of-governing-physical-laws},
	abstract = {By Steven L. Brunton, J. Nathan Kutz, and Joshua L. Proctor
Ordinary and partial differential equations are widely used throughout the engineering, physical, and biological sciences to describe the physical laws underlying a given system of interest. We implicitly assume that the governing equations are known and justified by first principles, such as conservation of mass or momentum and/or empirical observations. From the Schrödinger equation of quantum mechanics to Maxwell’s equations for...},
	language = {en-US},
	urldate = {2019-10-23},
	journal = {SIAM News},
	file = {Snapshot:/Users/shaan/Zotero/storage/YYUDDMEW/data-driven-discovery-of-governing-physical-laws.html:text/html}
}

@article{kwon_predicting_nodate,
	title = {Predicting {Future} {Frames} {Using} {Retrospective} {Cycle} {GAN}},
	abstract = {Recent advances in deep learning have signiﬁcantly improved the performance of video prediction, however, topperforming algorithms start to generate blurry predictions as they attempt to predict farther future frames. In this paper, we propose a uniﬁed generative adversarial network for predicting accurate and temporally consistent future frames over time, even in a challenging environment. The key idea is to train a single generator that can predict both future and past frames while enforcing the consistency of bi-directional prediction using the retrospective cycle constraints. Moreover, we employ two discriminators not only to identify fake frames but also to distinguish fake contained image sequences from the real sequence. The latter discriminator, the sequence discriminator, plays a crucial role in predicting temporally consistent future frames. We experimentally verify the proposed framework using various realworld videos captured by car-mounted cameras, surveillance cameras, and arbitrary devices with state-of-the-art methods.},
	language = {en},
	author = {Kwon, Yong-Hoon and Park, Min-Gyu},
	pages = {10},
	file = {Kwon and Park - Predicting Future Frames Using Retrospective Cycle.pdf:/Users/shaan/Zotero/storage/5IUIASNG/Kwon and Park - Predicting Future Frames Using Retrospective Cycle.pdf:application/pdf}
}

@article{saemundsson_variational_2019,
	title = {Variational {Integrator} {Networks} for {Physically} {Meaningful} {Embeddings}},
	url = {http://arxiv.org/abs/1910.09349},
	abstract = {Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose variational integrator networks, a class of neural network architectures designed to ensure faithful representations of the dynamics under study. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.},
	urldate = {2019-10-28},
	journal = {arXiv:1910.09349 [cs, stat]},
	author = {Saemundsson, Steindor and Terenin, Alexander and Hofmann, Katja and Deisenroth, Marc Peter},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.09349},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/SIIDF62M/Saemundsson et al. - 2019 - Variational Integrator Networks for Physically Mea.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/VMBCRTJC/1910.html:text/html}
}

@article{liang_dual_2017,
	title = {Dual {Motion} {GAN} for {Future}-{Flow} {Embedded} {Video} {Prediction}},
	url = {http://arxiv.org/abs/1708.00284},
	abstract = {Future frame prediction in videos is a promising avenue for unsupervised video representation learning. Video frames are naturally generated by the inherent pixel flows from preceding frames based on the appearance and motion dynamics in the video. However, existing methods focus on directly hallucinating pixel values, resulting in blurry predictions. In this paper, we develop a dual motion Generative Adversarial Net (GAN) architecture, which learns to explicitly enforce future-frame predictions to be consistent with the pixel-wise flows in the video through a dual-learning mechanism. The primal future-frame prediction and dual future-flow prediction form a closed loop, generating informative feedback signals to each other for better video prediction. To make both synthesized future frames and flows indistinguishable from reality, a dual adversarial training method is proposed to ensure that the future-flow prediction is able to help infer realistic future-frames, while the future-frame prediction in turn leads to realistic optical flows. Our dual motion GAN also handles natural motion uncertainty in different pixel locations with a new probabilistic motion encoder, which is based on variational autoencoders. Extensive experiments demonstrate that the proposed dual motion GAN significantly outperforms state-of-the-art approaches on synthesizing new video frames and predicting future flows. Our model generalizes well across diverse visual scenes and shows superiority in unsupervised video representation learning.},
	urldate = {2019-10-28},
	journal = {arXiv:1708.00284 [cs]},
	author = {Liang, Xiaodan and Lee, Lisa and Dai, Wei and Xing, Eric P.},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.00284},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/HZV4YCS6/Liang et al. - 2017 - Dual Motion GAN for Future-Flow Embedded Video Pre.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/7FLW6AGT/1708.html:text/html}
}

@article{mukherjee_clustergan_2019,
	title = {{ClusterGAN} : {Latent} {Space} {Clustering} in {Generative} {Adversarial} {Networks}},
	shorttitle = {{ClusterGAN}},
	url = {http://arxiv.org/abs/1809.03627},
	abstract = {Generative Adversarial networks (GANs) have obtained remarkable success in many unsupervised learning tasks and unarguably, clustering is an important unsupervised learning problem. While one can potentially exploit the latent-space back-projection in GANs to cluster, we demonstrate that the cluster structure is not retained in the GAN latent space. In this paper, we propose ClusterGAN as a new mechanism for clustering using GANs. By sampling latent variables from a mixture of one-hot encoded variables and continuous latent variables, coupled with an inverse network (which projects the data to the latent space) trained jointly with a clustering specific loss, we are able to achieve clustering in the latent space. Our results show a remarkable phenomenon that GANs can preserve latent space interpolation across categories, even though the discriminator is never exposed to such vectors. We compare our results with various clustering baselines and demonstrate superior performance on both synthetic and real datasets.},
	urldate = {2019-10-28},
	journal = {arXiv:1809.03627 [cs, stat]},
	author = {Mukherjee, Sudipto and Asnani, Himanshu and Lin, Eugene and Kannan, Sreeram},
	month = jan,
	year = {2019},
	note = {arXiv: 1809.03627},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/GWC5CD3I/Mukherjee et al. - 2019 - ClusterGAN  Latent Space Clustering in Generative.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/E8EUKA3L/1809.html:text/html}
}

@misc{noauthor_gan_nodate,
	title = {{GAN} video generator - {Google} {Search}},
	url = {https://www.google.com/search?sxsrf=ACYBGNQ6sDZaGRtbyBPDul2PIW68iZo3Xw%3A1573037922139&ei=YqfCXbCKCPyk1fAPrLedmA0&q=GAN+video+generator&oq=GAN+video+generator&gs_l=psy-ab.3..0i22i30j0i8i13i30l2.5152.7980..8069...1.3..0.132.2096.0j18......0....1..gws-wiz.......0i71j0j0i22i10i30j35i39j0i10j0i131i20i263j0i67j0i20i263.uBZRdexTxpU&ved=0ahUKEwiww8T0ttXlAhV8UhUIHaxbB9MQ4dUDCAs&uact=5},
	urldate = {2019-11-06},
	file = {GAN video generator - Google Search:/Users/shaan/Zotero/storage/9Q35RGCX/search.html:text/html}
}

@article{arjovsky_wasserstein_2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	urldate = {2019-11-06},
	journal = {arXiv:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	month = dec,
	year = {2017},
	note = {arXiv: 1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/FVD3YM2W/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/CENP5NN5/1701.html:text/html}
}

@article{salimans_improved_2016,
	title = {Improved {Techniques} for {Training} {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans ﬁnd visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classiﬁcation on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as conﬁrmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
	language = {en},
	urldate = {2019-11-06},
	journal = {arXiv:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.03498},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:/Users/shaan/Zotero/storage/LU4N7P33/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf}
}

@article{mullachery_bayesian_2018,
	title = {Bayesian {Neural} {Networks}},
	url = {http://arxiv.org/abs/1801.07710},
	abstract = {This paper describes and discusses Bayesian Neural Network (BNN). The paper showcases a few different applications of them for classification and regression problems. BNNs are comprised of a Probabilistic Model and a Neural Network. The intent of such a design is to combine the strengths of Neural Networks and Stochastic modeling. Neural Networks exhibit continuous function approximator capabilities. Stochastic models allow direct specification of a model with known interaction between parameters to generate data. During the prediction phase, stochastic models generate a complete posterior distribution and produce probabilistic guarantees on the predictions. Thus BNNs are a unique combination of neural network and stochastic models with the stochastic model forming the core of this integration. BNNs can then produce probabilistic guarantees on it's predictions and also generate the distribution of parameters that it has learnt from the observations. That means, in the parameter space, one can deduce the nature and shape of the neural network's learnt parameters. These two characteristics makes them highly attractive to theoreticians as well as practitioners. Recently there has been a lot of activity in this area, with the advent of numerous probabilistic programming libraries such as: PyMC3, Edward, Stan etc. Further this area is rapidly gaining ground as a standard machine learning approach for numerous problems},
	urldate = {2019-11-11},
	journal = {arXiv:1801.07710 [cs, stat]},
	author = {Mullachery, Vikram and Khera, Aniruddh and Husain, Amir},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.07710},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/JGAYY3RX/Mullachery et al. - 2018 - Bayesian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/JNDRH53T/1801.html:text/html}
}

@article{gupta_social_2018,
	title = {Social {GAN}: {Socially} {Acceptable} {Trajectories} with {Generative} {Adversarial} {Networks}},
	shorttitle = {Social {GAN}},
	url = {http://arxiv.org/abs/1803.10892},
	abstract = {Understanding human motion behavior is critical for autonomous moving platforms (like self-driving cars and social robots) if they are to navigate human-centric environments. This is challenging because human motion is inherently multimodal: given a history of human motion paths, there are many socially plausible ways that people could move in the future. We tackle this problem by combining tools from sequence prediction and generative adversarial networks: a recurrent sequence-to-sequence model observes motion histories and predicts future behavior, using a novel pooling mechanism to aggregate information across people. We predict socially plausible futures by training adversarially against a recurrent discriminator, and encourage diverse predictions with a novel variety loss. Through experiments on several datasets we demonstrate that our approach outperforms prior work in terms of accuracy, variety, collision avoidance, and computational complexity.},
	urldate = {2019-11-18},
	journal = {arXiv:1803.10892 [cs]},
	author = {Gupta, Agrim and Johnson, Justin and Fei-Fei, Li and Savarese, Silvio and Alahi, Alexandre},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.10892},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/N8KLLHQS/Gupta et al. - 2018 - Social GAN Socially Acceptable Trajectories with .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/IWW2LX3A/1803.html:text/html}
}

@article{yang_enforcing_2019,
	title = {Enforcing {Deterministic} {Constraints} on {Generative} {Adversarial} {Networks} for {Emulating} {Physical} {Systems}},
	url = {http://arxiv.org/abs/1911.06671},
	abstract = {Generative adversarial networks (GANs) are initially proposed to generate images by learning from a large number of samples. Recently, GANs have been used to emulate complex physical systems such as turbulent flows. However, a critical question must be answered before GANs can be considered trusted emulators for physical systems: do GANs-generated samples conform to the various physical constraints? These include both deterministic constraints (e.g., conservation laws) and statistical constraints (e.g., energy spectrum in turbulent flows). The latter has been studied in a companion paper (Wu et al. 2019. Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. arxiv:1905.06841). In the present work, we enforce deterministic yet approximate constraints on GANs by incorporating them into the loss function of the generator. We evaluate the performance of physics-constrained GANs on two representative tasks with geometrical constraints (generating points on circles) and differential constraints (generating divergence-free flow velocity fields), respectively. In both cases, the constrained GANs produced samples that precisely conform to the underlying constraints, even though the constraints are only enforced approximately. More importantly, the imposed constraints significantly accelerate the convergence and improve the robustness in the training. These improvements are noteworthy, as the convergence and robustness are two well-known obstacles in the training of GANs.},
	urldate = {2019-11-20},
	journal = {arXiv:1911.06671 [physics, stat]},
	author = {Yang, Zeng and Wu, Jin-Long and Xiao, Heng},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06671
version: 1},
	keywords = {Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/YITTXL8Y/Yang et al. - 2019 - Enforcing Deterministic Constraints on Generative .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/LYW5GPTE/1911.html:text/html}
}

@inproceedings{oliveira_tips_2017,
	title = {Tips and {Tricks} for {Training} {GANs} with {Physics} {Constraints}},
	abstract = {Generative Adversarial Networks (GANs) have seen immense interest and success in recent years. However, most tasks and successes have existed solely within the domain of natural images. In contrast, we provide an overview of modifications and tricks necessary to make GANs work on data from high energy particle physics. We provide select examples of domain-specific thought processes with respect to improving GAN training procedures, aiming to be a resource for researchers in any physical or applied science wishing to apply Generative Adversarial Networks to a choice problem.},
	author = {Oliveira, Luke de},
	year = {2017},
	keywords = {applied research, Generative adversarial networks},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/6R6X59WN/Oliveira - 2017 - Tips and Tricks for Training GANs with Physics Con.pdf:application/pdf}
}

@article{sanchez-gonzalez_hamiltonian_2019,
	title = {Hamiltonian {Graph} {Networks} with {ODE} {Integrators}},
	url = {http://arxiv.org/abs/1909.12790},
	abstract = {We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains.},
	urldate = {2019-11-21},
	journal = {arXiv:1909.12790 [physics]},
	author = {Sanchez-Gonzalez, Alvaro and Bapst, Victor and Cranmer, Kyle and Battaglia, Peter},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12790},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/BLWLZ57H/Sanchez-Gonzalez et al. - 2019 - Hamiltonian Graph Networks with ODE Integrators.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/U2S94TPM/Sanchez-Gonzalez et al. - 2019 - Hamiltonian Graph Networks with ODE Integrators.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9AM82WLE/1909.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/EV8S52WA/1909.html:text/html}
}

@article{owen_simple_nodate,
	title = {Simple pendulum via {Lagrangian} mechanics},
	language = {en},
	author = {Owen, Frank},
	pages = {3},
	file = {Owen - Simple pendulum via Lagrangian mechanics.pdf:/Users/shaan/Zotero/storage/4K8XMERX/Owen - Simple pendulum via Lagrangian mechanics.pdf:application/pdf}
}

@article{yi_clevrer:_2019,
	title = {{CLEVRER}: {CoLlision} {Events} for {Video} {REpresentation} and {Reasoning}},
	shorttitle = {{CLEVRER}},
	url = {http://arxiv.org/abs/1910.01442},
	abstract = {The ability to reason about temporal and causal events from videos lies at the core of human intelligence. Most video reasoning benchmarks, however, focus on pattern recognition from complex visual and language input, instead of on causal structure. We study the complementary problem, exploring the temporal and causal structures behind videos of objects with simple visual appearance. To this end, we introduce the CoLlision Events for Video REpresentation and Reasoning (CLEVRER), a diagnostic video dataset for systematic evaluation of computational models on a wide range of reasoning tasks. Motivated by the theory of human casual judgment, CLEVRER includes four types of questions: descriptive (e.g., "what color"), explanatory ("what is responsible for"), predictive ("what will happen next"), and counterfactual ("what if"). We evaluate various state-of-the-art models for visual reasoning on our benchmark. While these models thrive on the perception-based task (descriptive), they perform poorly on the causal tasks (explanatory, predictive and counterfactual), suggesting that a principled approach for causal reasoning should incorporate the capability of both perceiving complex visual and language inputs, and understanding the underlying dynamics and causal relations. We also study an oracle model that explicitly combines these components via symbolic representations.},
	urldate = {2019-12-30},
	journal = {arXiv:1910.01442 [cs]},
	author = {Yi, Kexin and Gan, Chuang and Li, Yunzhu and Kohli, Pushmeet and Wu, Jiajun and Torralba, Antonio and Tenenbaum, Joshua B.},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.01442},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XYTSFE7H/Yi et al. - 2019 - CLEVRER CoLlision Events for Video REpresentation.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/PKMJNJJN/1910.html:text/html}
}

@article{cranmer_learning_2019,
	title = {Learning {Symbolic} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/1909.05862},
	abstract = {We introduce an approach for imposing physically motivated inductive biases on graph networks to learn interpretable representations and improved zero-shot generalization. Our experiments show that our graph network models, which implement this inductive bias, can learn message representations equivalent to the true force vector when trained on n-body gravitational and spring-like simulations. We use symbolic regression to fit explicit algebraic equations to our trained model's message function and recover the symbolic form of Newton's law of gravitation without prior knowledge. We also show that our model generalizes better at inference time to systems with more bodies than had been experienced during training. Our approach is extensible, in principle, to any unknown interaction law learned by a graph network, and offers a valuable technique for interpreting and inferring explicit causal theories about the world from implicit knowledge captured by deep learning.},
	urldate = {2019-12-30},
	journal = {arXiv:1909.05862 [astro-ph, physics:physics, stat]},
	author = {Cranmer, Miles D. and Xu, Rui and Battaglia, Peter and Ho, Shirley},
	month = nov,
	year = {2019},
	note = {arXiv: 1909.05862},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics, Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/PRW99HKG/Cranmer et al. - 2019 - Learning Symbolic Physics with Graph Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/UTMV5TLS/1909.html:text/html}
}

@article{sanchez-gonzalez_graph_2018,
	title = {Graph networks as learnable physics engines for inference and control},
	url = {http://arxiv.org/abs/1806.01242},
	abstract = {Understanding and interacting with everyday physical scenes requires rich knowledge about the structure of the world, represented either implicitly in a value or policy function, or explicitly in a transition model. Here we introduce a new class of learnable models--based on graph networks--which implement an inductive bias for object- and relation-centric representations of complex, dynamical systems. Our results show that as a forward model, our approach supports accurate predictions from real and simulated data, and surprisingly strong and efficient generalization, across eight distinct physical systems which we varied parametrically and structurally. We also found that our inference model can perform system identification. Our models are also differentiable, and support online planning via gradient-based trajectory optimization, as well as offline policy optimization. Our framework offers new opportunities for harnessing and exploiting rich knowledge about the world, and takes a key step toward building machines with more human-like representations of the world.},
	urldate = {2019-12-30},
	journal = {arXiv:1806.01242 [cs, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Heess, Nicolas and Springenberg, Jost Tobias and Merel, Josh and Riedmiller, Martin and Hadsell, Raia and Battaglia, Peter},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.01242},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XKDNJQRV/Sanchez-Gonzalez et al. - 2018 - Graph networks as learnable physics engines for in.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/RZBQ25EM/Sanchez-Gonzalez et al. - 2018 - Graph networks as learnable physics engines for in.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/BNL63N55/1806.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/XP7LSVDX/1806.html:text/html}
}

@article{hansson_10_2015,
	title = {The 10 {Biggest} {Unsolved} {Problems} in {Physics}},
	volume = {1},
	abstract = {In 1900, the British physicist Lord Kelvin declared: “There is nothing new to discover in physics. All that remains is to more accurately measure its quantities.” In the same year quantum physics was born and three decades later it, and Einstein’s theory of relativity, had completely revolutionized and transformed physics. Today, hardly anyone would dare say that our knowledge of the universe, and everything in it, is almost complete. On the contrary, every new discovery appears to open a Pandora’s Box of larger and deeper issues. I have selected some of today’s biggest unsolved riddles in physics. Just like Moses, I stop arbitrarily at 10. Here follow these “Ten Com...plications” with a brief explanation/justiﬁcation. They may be seen as a roadmap for future important work.},
	language = {en},
	number = {1},
	author = {Hansson, Johan},
	year = {2015},
	pages = {5},
	file = {Hansson - 2015 - The 10 Biggest Unsolved Problems in Physics.pdf:/Users/shaan/Zotero/storage/F9WP9M9F/Hansson - 2015 - The 10 Biggest Unsolved Problems in Physics.pdf:application/pdf}
}

@article{smith_modeling_nodate,
	title = {Modeling {Expectation} {Violation} in {Intuitive} {Physics} with {Coarse} {Probabilistic} {Object} {Representations}},
	abstract = {From infancy, humans have expectations about how objects will move and interact. Even young children expect objects not to move through one another, teleport, or disappear. They are surprised by mismatches between physical expectations and perceptual observations, even in unfamiliar scenes with completely novel objects. A model that exhibits human-like understanding of physics should be similarly surprised, and adjust its beliefs accordingly. We propose ADEPT, a model that uses a coarse (approximate geometry) object-centric representation for dynamic 3D scene understanding. Inference integrates deep recognition networks, extended probabilistic physical simulation, and particle ﬁltering for forming predictions and expectations across occlusion. We also present a new test set for measuring violations of physical expectations, using a range of scenarios derived from developmental psychology. We systematically compare ADEPT, baseline models, and human expectations on this test set. ADEPT outperforms standard network architectures in discriminating physically implausible scenes, and often performs this discrimination at the same level as people.},
	language = {en},
	author = {Smith, Kevin A and Mei, Lingjie and Yao, Shunyu and Wu, Jiajun and Spelke, Elizabeth and Tenenbaum, Joshua B and Ullman, Tomer D},
	pages = {11},
	file = {Smith et al. - Modeling Expectation Violation in Intuitive Physic.pdf:/Users/shaan/Zotero/storage/PIFXDE67/Smith et al. - Modeling Expectation Violation in Intuitive Physic.pdf:application/pdf;Smith et al. - Modeling Expectation Violation in Intuitive Physic.pdf:/Users/shaan/Zotero/storage/RGINUGBP/Smith et al. - Modeling Expectation Violation in Intuitive Physic.pdf:application/pdf}
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2020-01-13},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6XL5RILM/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/LHQWPJ3T/1806.html:text/html}
}

@article{mao_neuro-symbolic_2019,
	title = {{THE} {NEURO}-{SYMBOLIC} {CONCEPT} {LEARNER}: {INTERPRETING} {SCENES}, {WORDS}, {AND} {SENTENCES} {FROM} {NATURAL} {SUPERVISION}},
	abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efﬁciency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
	language = {en},
	author = {Mao, Jiayuan and Gan, Chuang},
	year = {2019},
	pages = {28},
	file = {Mao and Gan - 2019 - THE NEURO-SYMBOLIC CONCEPT LEARNER INTERPRETING S.pdf:/Users/shaan/Zotero/storage/4L8UI8IU/Mao and Gan - 2019 - THE NEURO-SYMBOLIC CONCEPT LEARNER INTERPRETING S.pdf:application/pdf}
}

@article{xue_visual_2019,
	title = {Visual {Dynamics}: {Stochastic} {Future} {Generation} via {Layered} {Cross} {Convolutional} {Networks}},
	volume = {41},
	issn = {0162-8828, 2160-9292, 1939-3539},
	shorttitle = {Visual {Dynamics}},
	url = {https://ieeexplore.ieee.org/document/8409321/},
	doi = {10.1109/TPAMI.2018.2854726},
	abstract = {We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods that have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, and on real-world video frames. We present analyses of the learned network representations, showing it is implicitly learning a compact encoding of object appearance and motion. We also demonstrate a few of its applications, including visual analogy-making and video extrapolation.},
	language = {en},
	number = {9},
	urldate = {2020-01-13},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xue, Tianfan and Wu, Jiajun and Bouman, Katherine L. and Freeman, William T.},
	month = sep,
	year = {2019},
	pages = {2236--2250},
	file = {Xue et al. - 2019 - Visual Dynamics Stochastic Future Generation via .pdf:/Users/shaan/Zotero/storage/8VWS2ZGH/Xue et al. - 2019 - Visual Dynamics Stochastic Future Generation via .pdf:application/pdf}
}

@article{janner_reasoning_2019,
	title = {{REASONING} {ABOUT} {PHYSICAL} {INTERACTIONS} {WITH} {OBJECT}-{ORIENTED} {PREDICTION} {AND} {PLANNING}},
	abstract = {Object-based factorizations provide a useful level of abstraction for interacting with the world. Building explicit object representations, however, often requires supervisory signals that are difﬁcult to obtain in practice. We present a paradigm for learning object-centric representations for physical scene understanding without direct supervision of object properties. Our model, Object-Oriented Prediction and Planning (O2P2), jointly learns a perception function to map from image observations to object representations, a pairwise physics interaction function to predict the time evolution of a collection of objects, and a rendering function to map objects back to pixels. For evaluation, we consider not only the accuracy of the physical predictions of the model, but also its utility for downstream tasks that require an actionable representation of intuitive physics. After training our model on an image prediction task, we can use its learned representations to build block towers more complicated than those observed during training.},
	language = {en},
	author = {Janner, Michael and Levine, Sergey and Freeman, William T and Tenenbaum, Joshua B and Finn, Chelsea and Wu, Jiajun},
	year = {2019},
	pages = {12},
	file = {Janner et al. - 2019 - REASONING ABOUT PHYSICAL INTERACTIONS WITH OBJECT-.pdf:/Users/shaan/Zotero/storage/EF5CPJCI/Janner et al. - 2019 - REASONING ABOUT PHYSICAL INTERACTIONS WITH OBJECT-.pdf:application/pdf}
}

@article{zhang_learning_nodate,
	title = {Learning to {Reconstruct} {Shapes} from {Unseen} {Classes}},
	abstract = {From a single image, humans are able to perceive the full 3D shape of an object by exploiting learned shape priors from everyday life. Contemporary single-image 3D reconstruction algorithms aim to solve this task in a similar fashion, but often end up with priors that are highly biased by training classes. Here we present an algorithm, Generalizable Reconstruction (GenRe), designed to capture more generic, class-agnostic shape priors. We achieve this with an inference network and training procedure that combine 2.5D representations of visible surfaces (depth and silhouette), spherical shape representations of both visible and non-visible surfaces, and 3D voxel-based representations, in a principled manner that exploits the causal structure of how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe performs well on single-view shape reconstruction, and generalizes to diverse novel objects from categories not seen during training.},
	language = {en},
	author = {Zhang, Xiuming and Tenenbaum, Joshua B and Zhang, Zhoutong and Zhang, Chengkai and Freeman, William T and Wu, Jiajun},
	pages = {12},
	file = {Zhang et al. - Learning to Reconstruct Shapes from Unseen Classes.pdf:/Users/shaan/Zotero/storage/YD7U2T5I/Zhang et al. - Learning to Reconstruct Shapes from Unseen Classes.pdf:application/pdf}
}

@article{xu_unsupervised_2019,
	title = {{UNSUPERVISED} {DISCOVERY} {OF} {PARTS}, {STRUCTURE}, {AND} {DYNAMICS}},
	abstract = {Humans easily recognize object parts and their hierarchical structure by watching how they move; they can then predict how each part moves in the future. In this paper, we propose a novel formulation that simultaneously learns a hierarchical, disentangled object representation and a dynamics model for object parts from unlabeled videos. Our Parts, Structure, and Dynamics (PSD) model learns to, ﬁrst, recognize the object parts via a layered image representation; second, predict hierarchy via a structural descriptor that composes low-level concepts into a hierarchical structure; and third, model the system dynamics by predicting the future. Experiments on multiple real and synthetic datasets demonstrate that our PSD model works well on all three tasks: segmenting object parts, building their hierarchical structure, and capturing their motion distributions.},
	language = {en},
	author = {Xu, Zhenjia and Liu, Zhijian and Sun, Chen and Murphy, Kevin and Freeman, William T and Tenenbaum, Joshua B and Wu, Jiajun},
	year = {2019},
	pages = {15},
	file = {Xu et al. - 2019 - UNSUPERVISED DISCOVERY OF PARTS, STRUCTURE, AND DY.pdf:/Users/shaan/Zotero/storage/JME2A9BT/Xu et al. - 2019 - UNSUPERVISED DISCOVERY OF PARTS, STRUCTURE, AND DY.pdf:application/pdf}
}

@article{han_visual_nodate,
	title = {Visual {Concept}-{Metaconcept} {Learning}},
	abstract = {Humans reason with concepts and metaconcepts: we recognize red and green from visual input; we also understand that they describe the same property of objects (i.e., the color). In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and green describe the same property of objects, we generalize to the fact that cube and sphere also describe the same property of objects, since they both categorize the shape of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data. From just a few examples of purple cubes we can understand a new color purple, which resembles the hue of the cubes instead of the shape of them. Evaluation on both synthetic and real-world datasets validates our claims.},
	language = {en},
	author = {Han, Chi and Mao, Jiayuan and Gan, Chuang},
	pages = {12},
	file = {Han et al. - Visual Concept-Metaconcept Learning.pdf:/Users/shaan/Zotero/storage/QGZDQXPC/Han et al. - Visual Concept-Metaconcept Learning.pdf:application/pdf}
}

@article{toth_hamiltonian_2019,
	title = {Hamiltonian {Generative} {Networks}},
	url = {http://arxiv.org/abs/1909.13789},
	abstract = {The Hamiltonian formalism plays a central role in classical and quantum physics. Hamiltonians are the main tool for modelling the continuous time evolution of systems with conserved quantities, and they come equipped with many useful properties, like time reversibility and smooth interpolation in time. These properties are important for many machine learning problems - from sequence prediction to reinforcement learning and density modelling - but are not typically provided out of the box by standard tools such as recurrent neural networks. In this paper, we introduce the Hamiltonian Generative Network (HGN), the first approach capable of consistently learning Hamiltonian dynamics from high-dimensional observations (such as images) without restrictive domain assumptions. Once trained, we can use HGN to sample new trajectories, perform rollouts both forward and backward in time and even speed up or slow down the learned dynamics. We demonstrate how a simple modification of the network architecture turns HGN into a powerful normalising flow model, called Neural Hamiltonian Flow (NHF), that uses Hamiltonian dynamics to model expressive densities. We hope that our work serves as a first practical demonstration of the value that the Hamiltonian formalism can bring to deep learning.},
	urldate = {2020-01-15},
	journal = {arXiv:1909.13789 [cs, stat]},
	author = {Toth, Peter and Rezende, Danilo Jimenez and Jaegle, Andrew and Racanière, Sébastien and Botev, Aleksandar and Higgins, Irina},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.13789},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/YVWHI3MJ/Toth et al. - 2019 - Hamiltonian Generative Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8G6GGX24/1909.html:text/html}
}

@article{anand_unsupervised_2019,
	title = {Unsupervised {State} {Representation} {Learning} in {Atari}},
	url = {http://arxiv.org/abs/1906.08226},
	abstract = {State representation learning, or the ability to capture latent generative factors of an environment, is crucial for building intelligent agents that can perform a wide variety of tasks. Learning such representations without supervision from rewards is a challenging open problem. We introduce a method that learns state representations by maximizing mutual information across spatially and temporally distinct features of a neural encoder of the observations. We also introduce a new benchmark based on Atari 2600 games where we evaluate representations based on how well they capture the ground truth state variables. We believe this new framework for evaluating representation learning models will be crucial for future representation learning research. Finally, we compare our technique with other state-of-the-art generative and contrastive representation learning methods. The code associated with this work is available at https://github.com/mila-iqia/atari-representation-learning},
	urldate = {2020-01-20},
	journal = {arXiv:1906.08226 [cs, stat]},
	author = {Anand, Ankesh and Racah, Evan and Ozair, Sherjil and Bengio, Yoshua and Côté, Marc-Alexandre and Hjelm, R. Devon},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.08226},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/UG53SX8U/Anand et al. - 2019 - Unsupervised State Representation Learning in Atar.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/C4N659G9/1906.html:text/html}
}

@article{zhang_advances_2018,
	title = {Advances in {Variational} {Inference}},
	url = {http://arxiv.org/abs/1711.05597},
	abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully used in various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
	urldate = {2020-01-20},
	journal = {arXiv:1711.05597 [cs, stat]},
	author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
	month = oct,
	year = {2018},
	note = {arXiv: 1711.05597},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/2NR66JSH/Zhang et al. - 2018 - Advances in Variational Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8DGBJWPD/1711.html:text/html}
}

@article{shahriari_taking_2016,
	title = {Taking the {Human} {Out} of the {Loop}: {A} {Review} of {Bayesian} {Optimization}},
	volume = {104},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Taking the {Human} {Out} of the {Loop}},
	url = {https://ieeexplore.ieee.org/document/7352306/},
	doi = {10.1109/JPROC.2015.2494218},
	abstract = {Big data applications are typically associated with systems involving large numbers of users, massive complex software systems, and large-scale heterogeneous computing and storage architectures. The construction of such systems involves many distributed design choices. The end products (e.g., recommendation systems, medical analysis tools, real-time game engines, speech recognizers) thus involves many tunable conﬁguration parameters. These parameters are often speciﬁed and hard-coded into the software by various developers or teams. If optimized jointly, these parameters can result in signiﬁcant improvements. Bayesian optimization is a powerful tool for the joint optimization of design choices that is gaining great popularity in recent years. It promises greater automation so as to increase both product quality and human productivity. This review paper introduces Bayesian optimization, highlights some of its methodological aspects, and showcases a wide range of applications.},
	language = {en},
	number = {1},
	urldate = {2020-01-20},
	journal = {Proceedings of the IEEE},
	author = {Shahriari, Bobak and Swersky, Kevin and Wang, Ziyu and Adams, Ryan P. and de Freitas, Nando},
	month = jan,
	year = {2016},
	pages = {148--175},
	file = {Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf:/Users/shaan/Zotero/storage/7YNZZVQD/Shahriari et al. - 2016 - Taking the Human Out of the Loop A Review of Baye.pdf:application/pdf}
}

@article{zhong_symplectic_2019,
	title = {Symplectic {ODE}-{Net}: {Learning} {Hamiltonian} {Dynamics} with {Control}},
	shorttitle = {Symplectic {ODE}-{Net}},
	url = {http://arxiv.org/abs/1909.12077},
	abstract = {In this paper, we introduce Symplectic ODE-Net (SymODEN), a deep learning framework which can infer the dynamics of a physical system from observed state trajectories. To achieve better generalization with fewer training samples, SymODEN incorporates appropriate inductive bias by designing the associated computation graph in a physics-informed manner. In particular, we enforce Hamiltonian dynamics with control to learn the underlying dynamics in a transparent way which can then be leveraged to draw insight about relevant physical aspects of the system, such as mass and potential energy. In addition, we propose a parametrization which can enforce this Hamiltonian formalism even when the generalized coordinate data is embedded in a high-dimensional space or we can only access velocity data instead of generalized momentum. This framework, by offering interpretable, physically-consistent models for physical systems, opens up new possibilities for synthesizing model-based control strategies.},
	urldate = {2020-01-20},
	journal = {arXiv:1909.12077 [physics, stat]},
	author = {Zhong, Yaofeng Desmond and Dey, Biswadip and Chakraborty, Amit},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12077},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/7S998UH5/Zhong et al. - 2019 - Symplectic ODE-Net Learning Hamiltonian Dynamics .pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/TP6ZJ47C/Zhong et al. - 2020 - Symplectic ODE-Net Learning Hamiltonian Dynamics .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/33TIDQCB/1909.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/E9E3EMGN/1909.html:text/html}
}

@article{choudhary_physics_2019,
	title = {Physics enhanced neural networks predict order and chaos},
	url = {http://arxiv.org/abs/1912.01958},
	abstract = {Conventional artificial neural networks are powerful tools in science and industry, but they can fail when applied to nonlinear systems where order and chaos coexist. We use neural networks that incorporate the structures and symmetries of Hamiltonian dynamics to predict phase space trajectories even as nonlinear systems transition from order to chaos. We demonstrate Hamiltonian neural networks on the canonical Henon-Heiles system, which models diverse dynamics from astrophysics to chemistry. The power of the technique and the ubiquity of chaos suggest widespread utility.},
	urldate = {2020-01-20},
	journal = {arXiv:1912.01958 [physics]},
	author = {Choudhary, Anshul and Lindner, John F. and Holliday, Elliott G. and Miller, Scott T. and Sinha, Sudeshna and Ditto, William L.},
	month = nov,
	year = {2019},
	note = {arXiv: 1912.01958},
	keywords = {Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/L6QXMPJR/Choudhary et al. - 2019 - Physics enhanced neural networks predict order and.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/Y8QIIGZD/1912.html:text/html}
}

@incollection{shafahi_adversarial_2019,
	title = {Adversarial training for free!},
	url = {http://papers.nips.cc/paper/8597-adversarial-training-for-free.pdf},
	urldate = {2020-01-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {3353--3364},
	file = {NIPS Full Text PDF:/Users/shaan/Zotero/storage/ZJNEQBYW/Shafahi et al. - 2019 - Adversarial training for free!.pdf:application/pdf;NIPS Snapshot:/Users/shaan/Zotero/storage/CA44SQ2C/8597-adversarial-training-for-free.html:text/html}
}

@article{kipf_neural_2018,
	title = {Neural {Relational} {Inference} for {Interacting} {Systems}},
	url = {http://arxiv.org/abs/1802.04687},
	abstract = {Interacting systems are prevalent in nature, from dynamical systems in physics to complex societal dynamics. The interplay of components can give rise to complex behavior, which can often be explained using a simple model of the system's constituent parts. In this work, we introduce the neural relational inference (NRI) model: an unsupervised model that learns to infer interactions while simultaneously learning the dynamics purely from observational data. Our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction graph and the reconstruction is based on graph neural networks. In experiments on simulated physical systems, we show that our NRI model can accurately recover ground-truth interactions in an unsupervised manner. We further demonstrate that we can find an interpretable structure and predict complex dynamics in real motion capture and sports tracking data.},
	urldate = {2020-02-03},
	journal = {arXiv:1802.04687 [cs, stat]},
	author = {Kipf, Thomas and Fetaya, Ethan and Wang, Kuan-Chieh and Welling, Max and Zemel, Richard},
	month = jun,
	year = {2018},
	note = {arXiv: 1802.04687},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/CL3AN9C5/Kipf et al. - 2018 - Neural Relational Inference for Interacting System.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9TJXA585/1802.html:text/html}
}

@article{burgess_monet_2019,
	title = {{MONet}: {Unsupervised} {Scene} {Decomposition} and {Representation}},
	shorttitle = {{MONet}},
	url = {http://arxiv.org/abs/1901.11390},
	abstract = {The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.},
	urldate = {2020-02-03},
	journal = {arXiv:1901.11390 [cs, stat]},
	author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11390
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/HW9L4RN3/Burgess et al. - 2019 - MONet Unsupervised Scene Decomposition and Repres.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/TKYYZQ4C/Burgess et al. - 2019 - MONet Unsupervised Scene Decomposition and Repres.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/TYTGLCWS/1901.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HMDNU2DY/1901.html:text/html}
}

@article{pukrittayakamee_simultaneous_2009,
	title = {Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks},
	volume = {130},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.3095491},
	doi = {10.1063/1.3095491},
	language = {en},
	number = {13},
	urldate = {2020-02-17},
	journal = {The Journal of Chemical Physics},
	author = {Pukrittayakamee, A. and Malshe, M. and Hagan, M. and Raff, L. M. and Narulkar, R. and Bukkapatnum, S. and Komanduri, R.},
	month = apr,
	year = {2009},
	pages = {134101},
	file = {Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:/Users/shaan/Zotero/storage/JRKC7JQS/Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:application/pdf;Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:/Users/shaan/Zotero/storage/49I8EKV2/Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:application/pdf}
}

@article{betancourt_symplectic_2018,
	title = {On {Symplectic} {Optimization}},
	url = {http://arxiv.org/abs/1802.03653},
	abstract = {Accelerated gradient methods have had significant impact in machine learning -- in particular the theoretical side of machine learning -- due to their ability to achieve oracle lower bounds. But their heuristic construction has hindered their full integration into the practical machine-learning algorithmic toolbox, and has limited their scope. In this paper we build on recent work which casts acceleration as a phenomenon best explained in continuous time, and we augment that picture by providing a systematic methodology for converting continuous-time dynamics into discrete-time algorithms while retaining oracle rates. Our framework is based on ideas from Hamiltonian dynamical systems and symplectic integration. These ideas have had major impact in many areas in applied mathematics, but have not yet been seen to have a relationship with optimization.},
	urldate = {2020-02-17},
	journal = {arXiv:1802.03653 [stat]},
	author = {Betancourt, Michael and Jordan, Michael I. and Wilson, Ashia C.},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.03653},
	keywords = {Statistics - Computation},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/9HIUWF6X/Betancourt et al. - 2018 - On Symplectic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/RDH4ZSA8/1802.html:text/html}
}

@article{marmo_symplectic_1996,
	title = {{SYMPLECTIC} {STRUCTURES} {AND} {QUANTUM} {MECHANICS}},
	volume = {10},
	issn = {0217-9849, 1793-6640},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0217984996000602},
	doi = {10.1142/S0217984996000602},
	abstract = {Canonical coordinates for the Schro¨dinger equation are introduced, making more transparent its Hamiltonian structure. It is shown that the Schro¨dinger equation, considered as a classical ﬁeld theory, shares with Liouville completely integrable ﬁeld theories the existence of a recursion operator which allows for the inﬁnitely many conserved functionals pairwise commuting with respect to the corresponding Poisson bracket.},
	language = {en},
	number = {12},
	urldate = {2020-02-17},
	journal = {Modern Physics Letters B},
	author = {Marmo, Giuseppe and Vilasi, Gaetano},
	month = may,
	year = {1996},
	pages = {545--553},
	file = {Marmo and Vilasi - 1996 - SYMPLECTIC STRUCTURES AND QUANTUM MECHANICS.pdf:/Users/shaan/Zotero/storage/BMQUPBVW/Marmo and Vilasi - 1996 - SYMPLECTIC STRUCTURES AND QUANTUM MECHANICS.pdf:application/pdf}
}

@article{rezek_operator_nodate,
	title = {An {Operator} {Interpretation} of {Message} {Passing}},
	abstract = {Message passing algorithms may be viewed from a purely probabilistic or statistical physics perspective. This works describes an alternative, linear algebraic view of message passing in trees. We demonstrate the construction of global belief operators on Markov Chains and Trees and compare these with classical results. By interpreting message passing as ﬁnding a global stable solution we wish to avoid application dependent development of message passing methods of more complex graph structures.},
	language = {en},
	author = {Rezek, I and Roberts, S J},
	pages = {8},
	file = {Rezek and Roberts - An Operator Interpretation of Message Passing.pdf:/Users/shaan/Zotero/storage/SQIV6B94/Rezek and Roberts - An Operator Interpretation of Message Passing.pdf:application/pdf}
}

@article{battaglia_interaction_2016,
	title = {Interaction {Networks} for {Learning} about {Objects}, {Relations} and {Physics}},
	url = {http://arxiv.org/abs/1612.00222},
	abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
	urldate = {2020-02-19},
	journal = {arXiv:1612.00222 [cs]},
	author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
	month = dec,
	year = {2016},
	note = {arXiv: 1612.00222},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/HYM4C5YB/Battaglia et al. - 2016 - Interaction Networks for Learning about Objects, R.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HE7UH3XF/1612.html:text/html}
}

@article{benton_function-space_2019,
	title = {Function-{Space} {Distributions} over {Kernels}},
	url = {http://arxiv.org/abs/1910.13565},
	abstract = {Gaussian processes are flexible function approximators, with inductive biases controlled by a covariance kernel. Learning the kernel is the key to representation learning and strong predictive performance. In this paper, we develop functional kernel learning (FKL) to directly infer functional posteriors over kernels. In particular, we place a transformed Gaussian process over a spectral density, to induce a non-parametric distribution over kernel functions. The resulting approach enables learning of rich representations, with support for any stationary kernel, uncertainty over the values of the kernel, and an interpretable specification of a prior directly over kernels, without requiring sophisticated initialization or manual intervention. We perform inference through elliptical slice sampling, which is especially well suited to marginalizing posteriors with the strongly correlated priors typical to function space modelling. We develop our approach for non-uniform, large-scale, multi-task, and multidimensional data, and show promising performance in a wide range of settings, including interpolation, extrapolation, and kernel recovery experiments.},
	urldate = {2020-02-25},
	journal = {arXiv:1910.13565 [cs, stat]},
	author = {Benton, Gregory W. and Maddox, Wesley J. and Salkey, Jayson P. and Albinati, Julio and Wilson, Andrew Gordon},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.13565},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/9R4967DU/Benton et al. - 2019 - Function-Space Distributions over Kernels.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/SAKBXKSL/1910.html:text/html}
}

@article{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
	urldate = {2020-03-18},
	journal = {arXiv:1711.10561 [cs, math, stat]},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10561},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/GRNXH7IQ/Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/96Z7VY4B/1711.html:text/html}
}

@article{marrero_discrete_2006,
	title = {Discrete {Lagrangian} and {Hamiltonian} {Mechanics} on {Lie} groupoids},
	volume = {19},
	issn = {0951-7715, 1361-6544},
	url = {http://arxiv.org/abs/math/0506299},
	doi = {10.1088/0951-7715/19/6/006},
	abstract = {The purpose of this paper is to describe geometrically discrete Lagrangian and Hamiltonian Mechanics on Lie groupoids. From a variational principle we derive the discrete Euler-Lagrange equations and we introduce a symplectic 2-section, which is preserved by the Lagrange evolution operator. In terms of the discrete Legendre transformations we define the Hamiltonian evolution operator which is a symplectic map with respect to the canonical symplectic 2-section on the prolongation of the dual of the Lie algebroid of the given groupoid. The equations we get include as particular cases the classical discrete Euler-Lagrange equations, the discrete Euler-Poincar{\textbackslash}'e and discrete Lagrange-Poincar{\textbackslash}'e equations. Our results can be important for the construction of geometric integrators for continuous Lagrangian systems.},
	number = {6},
	urldate = {2020-03-18},
	journal = {Nonlinearity},
	author = {Marrero, J. C. and de Diego, D. Martín and Martínez, E.},
	month = jun,
	year = {2006},
	note = {arXiv: math/0506299},
	keywords = {17B66, 22A22, 70G45, 70Hxx, Mathematical Physics, Mathematics - Differential Geometry},
	pages = {1313--1348},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/GR89IGVW/Marrero et al. - 2006 - Discrete Lagrangian and Hamiltonian Mechanics on L.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/CPEYDTUL/Marrero et al. - 2006 - Discrete Lagrangian and Hamiltonian Mechanics on L.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/IS4Q4Z6Z/0506299.html:text/html}
}

@article{witkoskie_neural_2005,
	title = {Neural {Network} {Models} of {Potential} {Energy} {Surfaces}: {Prototypical} {Examples}},
	volume = {1},
	issn = {1549-9618, 1549-9626},
	shorttitle = {Neural {Network} {Models} of {Potential} {Energy} {Surfaces}},
	url = {https://pubs.acs.org/doi/10.1021/ct049976i},
	doi = {10.1021/ct049976i},
	abstract = {Neural networks can be used generate potential energy hypersurfaces by fitting to a data set of energies at discrete geometries, as might be obtained from ab initio calculations. Prior work has shown that this method can generate accurate fits in complex systems of several dimensions. The present paper explores fundamental properties of neural network potential representations in some simple prototypes of one, two, and three dimensions. Optimal fits to the data are achieved by adjusting the network parameters using an extended Kalman filtering algorithm, which is described in detail. The examples provide insight into the relationships between the form of the function being fit, the amount of data needed for an adequate fit, and the optimal network configuration and number of neurons needed. The quality of the network interpolation is substantially improved if gradients as well as the energy are available for fitting. The fitting algorithm is effective in providing an accurate interpolation of the underlying potential function even when random noise is added to the data used in the fit.},
	language = {en},
	number = {1},
	urldate = {2020-03-24},
	journal = {Journal of Chemical Theory and Computation},
	author = {Witkoskie, James B. and Doren, Douglas J.},
	month = jan,
	year = {2005},
	pages = {14--23},
	file = {Witkoskie and Doren - 2005 - Neural Network Models of Potential Energy Surfaces.pdf:/Users/shaan/Zotero/storage/5QX48BMK/Witkoskie and Doren - 2005 - Neural Network Models of Potential Energy Surfaces.pdf:application/pdf;Witkoskie and Doren - 2005 - Neural Network Models of Potential Energy Surfaces.pdf:/Users/shaan/Zotero/storage/JJ6R5JHI/Witkoskie and Doren - 2005 - Neural Network Models of Potential Energy Surfaces.pdf:application/pdf}
}

@article{lamb_graph_2020,
	title = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}: {A} {Survey} and {Perspective}},
	shorttitle = {Graph {Neural} {Networks} {Meet} {Neural}-{Symbolic} {Computing}},
	url = {http://arxiv.org/abs/2003.00330},
	abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNN) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as its relationship to current developments in neural-symbolic computing.},
	urldate = {2020-03-24},
	journal = {arXiv:2003.00330 [cs]},
	author = {Lamb, Luis and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.00330},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Logic in Computer Science},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/9ECXQFZ2/Lamb et al. - 2020 - Graph Neural Networks Meet Neural-Symbolic Computi.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/ADYSULE7/2003.html:text/html}
}

@article{bondesan_learning_2019,
	title = {Learning {Symmetries} of {Classical} {Integrable} {Systems}},
	url = {http://arxiv.org/abs/1906.04645},
	abstract = {The solution of problems in physics is often facilitated by a change of variables. In this work we present neural transformations to learn symmetries of Hamiltonian mechanical systems. Maintaining the Hamiltonian structure requires novel network architectures that parametrize symplectic transformations. We demonstrate the utility of these architectures by learning the structure of integrable models. Our work exemplifies the adaptation of neural transformations to a family constrained by more than the condition of invertibility, which we expect to be a common feature of applications of these methods.},
	urldate = {2020-03-24},
	journal = {arXiv:1906.04645 [physics]},
	author = {Bondesan, Roberto and Lamacraft, Austen},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04645},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/EDN7VTJI/Bondesan and Lamacraft - 2019 - Learning Symmetries of Classical Integrable System.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/GRTSME8B/Bondesan and Lamacraft - 2019 - Learning Symmetries of Classical Integrable System.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9QRYAKZ8/1906.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HR7PQ74A/1906.html:text/html}
}

@article{cranmer_lagrangian_2020,
	title = {Lagrangian {Neural} {Networks}},
	url = {http://arxiv.org/abs/2003.04630},
	abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
	urldate = {2020-04-06},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04630},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning, Physics - Computational Physics, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/X9II2XSE/Cranmer et al. - 2020 - Lagrangian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/26M6U33V/2003.html:text/html}
}

@article{seo_differentiable_2019,
	title = {Differentiable {Physics}-informed {Graph} {Networks}},
	url = {http://arxiv.org/abs/1902.02950},
	abstract = {While physics conveys knowledge of nature built from an interplay between observations and theory, it has been considered less importantly in deep neural networks. Especially, there are few works leveraging physics behaviors when the knowledge is given less explicitly. In this work, we propose a novel architecture called Differentiable Physics-informed Graph Networks (DPGN) to incorporate implicit physics knowledge which is given from domain experts by informing it in latent space. Using the concept of DPGN, we demonstrate that climate prediction tasks are significantly improved. Besides the experiment results, we validate the effectiveness of the proposed module and provide further applications of DPGN, such as inductive learning and multistep predictions.},
	urldate = {2020-04-06},
	journal = {arXiv:1902.02950 [cs, stat]},
	author = {Seo, Sungyong and Liu, Yan},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.02950},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/5NS94SEF/Seo and Liu - 2019 - Differentiable Physics-informed Graph Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/96RDBJFN/1902.html:text/html}
}

@article{dormand_families_1987,
	title = {Families of {Runge}-{Kutta}-{Nystrom} {Formulae}},
	volume = {7},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article-lookup/doi/10.1093/imanum/7.2.235},
	doi = {10.1093/imanum/7.2.235},
	language = {en},
	number = {2},
	urldate = {2020-04-07},
	journal = {IMA Journal of Numerical Analysis},
	author = {Dormand, J. R. and El-Mikkawy, M. E. A. and Prince, P. J.},
	year = {1987},
	pages = {235--250},
	file = {Dormand et al. - 1987 - Families of Runge-Kutta-Nystrom Formulae.pdf:/Users/shaan/Zotero/storage/3EXXYGZH/Dormand et al. - 1987 - Families of Runge-Kutta-Nystrom Formulae.pdf:application/pdf}
}

@article{seo_physics-aware_2020,
	title = {{PHYSICS}-{AWARE} {DIFFERENCE} {GRAPH} {NETWORKS} {FOR} {SPARSELY}-{OBSERVED} {DYNAMICS}},
	abstract = {Sparsely available data points cause numerical error on ﬁnite differences which hinders us from modeling the dynamics of physical systems. The discretization error becomes even larger when the sparse data are irregularly distributed or deﬁned on an unstructured grid, making it hard to build deep learning models to handle physics-governing observations on the unstructured grid. In this paper, we propose a novel architecture, Physics-aware Difference Graph Networks (PA-DGN), which exploits neighboring information to learn ﬁnite differences inspired by physics equations. PA-DGN leverages data-driven end-to-end learning to discover underlying dynamical relations between the spatial and temporal differences in given sequential observations. We demonstrate the superiority of PA-DGN in the approximation of directional derivatives and the prediction of graph signals on the synthetic data and the real-world climate observations from weather stations.},
	language = {en},
	author = {Seo, Sungyong and Meng, Chuizheng and Liu, Yan},
	year = {2020},
	pages = {15},
	file = {Seo et al. - 2020 - PHYSICS-AWARE DIFFERENCE GRAPH NETWORKS FOR SPARSE.pdf:/Users/shaan/Zotero/storage/2Z7ZYYGZ/Seo et al. - 2020 - PHYSICS-AWARE DIFFERENCE GRAPH NETWORKS FOR SPARSE.pdf:application/pdf;Seo et al. - 2020 - PHYSICS-AWARE DIFFERENCE GRAPH NETWORKS FOR SPARSE.pdf:/Users/shaan/Zotero/storage/6R8LH98U/Seo et al. - 2020 - PHYSICS-AWARE DIFFERENCE GRAPH NETWORKS FOR SPARSE.pdf:application/pdf}
}

@article{sanchez-gonzalez_learning_2020,
	title = {Learning to {Simulate} {Complex} {Physics} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2002.09405},
	abstract = {Here we present a general framework for learning simulation, and provide a single model implementation that yields state-of-the-art performance across a variety of challenging physical domains, involving fluids, rigid solids, and deformable materials interacting with one another. Our framework---which we term "Graph Network-based Simulators" (GNS)---represents the state of a physical system with particles, expressed as nodes in a graph, and computes dynamics via learned message-passing. Our results show that our model can generalize from single-timestep predictions with thousands of particles during training, to different initial conditions, thousands of timesteps, and at least an order of magnitude more particles at test time. Our model was robust to hyperparameter choices across various evaluation metrics: the main determinants of long-term performance were the number of message-passing steps, and mitigating the accumulation of error by corrupting the training data with noise. Our GNS framework is the most accurate general-purpose learned physics simulator to date, and holds promise for solving a wide range of complex forward and inverse problems.},
	urldate = {2020-04-09},
	journal = {arXiv:2002.09405 [physics, stat]},
	author = {Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and Ying, Rex and Leskovec, Jure and Battaglia, Peter W.},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.09405},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/JTCSU7U4/Sanchez-Gonzalez et al. - 2020 - Learning to Simulate Complex Physics with Graph Ne.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/2DLK93SH/2002.html:text/html}
}

@article{tronarp_bayesian_2020,
	title = {Bayesian {ODE} {Solvers}: {The} {Maximum} {A} {Posteriori} {Estimate}},
	shorttitle = {Bayesian {ODE} {Solvers}},
	url = {http://arxiv.org/abs/2004.00623},
	abstract = {It has recently been established that the numerical solution of ordinary differential equations can be posed as a nonlinear Bayesian inference problem, which can be approximately solved via Gaussian filtering and smoothing, whenever a Gauss--Markov prior is used. In this paper the class of \${\textbackslash}nu\$ times differentiable linear time invariant Gauss--Markov priors is considered. A taxonomy of Gaussian estimators is established, with the maximum a posteriori estimate at the top of the hierarchy, which can be computed with the iterated extended Kalman smoother. The remaining three classes are termed explicit, semi-implicit, and implicit, which are in similarity with the classical notions corresponding to conditions on the vector field, under which the filter update produces a local maximum a posteriori estimate. The maximum a posteriori estimate corresponds to an optimal interpolant in the reproducing Hilbert space associated with the prior, which in the present case is equivalent to a Sobolev space of smoothness \${\textbackslash}nu+1\$. Consequently, using methods from scattered data approximation and nonlinear analysis in Sobolev spaces, it is shown that the maximum a posteriori estimate converges to the true solution at a polynomial rate in the fill-distance (maximum step size) subject to mild conditions on the vector field. The methodology developed provides a novel and more natural approach to study the convergence of these estimators than classical methods of convergence analysis. The methods and theoretical results are demonstrated in numerical examples.},
	urldate = {2020-04-09},
	journal = {arXiv:2004.00623 [cs, math, stat]},
	author = {Tronarp, Filip and Sarkka, Simo and Hennig, Philipp},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.00623},
	keywords = {Statistics - Machine Learning, Mathematics - Numerical Analysis, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/7GMHQBVJ/Tronarp et al. - 2020 - Bayesian ODE Solvers The Maximum A Posteriori Est.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/2GUYYCNS/2004.html:text/html}
}

@article{grathwohl_ffjord_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
	urldate = {2020-04-16},
	journal = {arXiv:1810.01367 [cs, stat]},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.01367},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition}
}

@article{rubanova_latent_2019,
	title = {Latent {ODEs} for {Irregularly}-{Sampled} {Time} {Series}},
	url = {http://arxiv.org/abs/1907.03907},
	abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
	urldate = {2020-04-16},
	journal = {arXiv:1907.03907 [cs, stat]},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.03907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/RDBYRN5L/Rubanova et al. - 2019 - Latent ODEs for Irregularly-Sampled Time Series.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/TJDAPKUP/1907.html:text/html}
}

@article{marsden_discrete_2001,
	title = {Discrete mechanics and variational integrators},
	volume = {10},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/product/identifier/S096249290100006X/type/journal_article},
	doi = {10.1017/S096249290100006X},
	abstract = {This paper gives a review of integration algorithms for finite dimensional 
mechanical systems that are based on discrete variational principles. The 
variational technique gives a unified treatment of many symplectic schemes, 
including those of higher order, as well as a natural treatment of the discrete 
Noether theorem. The approach also allows us to include forces, dissipation 
and constraints in a natural way. Amongst the many specific schemes treated 
as examples, the Verlet, SHAKE, RATTLE, Newmark, and the symplectic 
partitioned Runge–Kutta schemes are presented.},
	language = {en},
	urldate = {2020-04-19},
	journal = {Acta Numerica},
	author = {Marsden, J. E. and West, M.},
	month = may,
	year = {2001},
	pages = {357--514},
	file = {Marsden and West - 2001 - Discrete mechanics and variational integrators.pdf:/Users/shaan/Zotero/storage/EYTKJXCM/Marsden and West - 2001 - Discrete mechanics and variational integrators.pdf:application/pdf}
}

@misc{noauthor_pii_nodate,
	title = {{PII}: {S0307}-{904X}(99)00006-2 {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {{PII}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0307904X99000062?token=465C6C27818E4B6F74120DC732B5AE6F7FD1537F450C53646075ECE7D2280E0625F5C523CB6DE53219D9859C80831630},
	language = {en},
	urldate = {2020-04-25},
	doi = {10.1016/S0307-904X(99)00006-2},
	note = {Library Catalog: reader.elsevier.com},
	file = {Snapshot:/Users/shaan/Zotero/storage/77T9X2WM/S0307904X99000062.html:text/html}
}

@article{e_proposal_2017,
	title = {A {Proposal} on {Machine} {Learning} via {Dynamical} {Systems}},
	volume = {5},
	issn = {2194-671X},
	url = {https://doi.org/10.1007/s40304-017-0103-z},
	doi = {10.1007/s40304-017-0103-z},
	abstract = {We discuss the idea of using continuous dynamical systems to model general high-dimensional nonlinear functions used in machine learning. We also discuss the connection with deep learning.},
	language = {en},
	number = {1},
	urldate = {2020-04-28},
	journal = {Communications in Mathematics and Statistics},
	author = {E, Weinan},
	month = mar,
	year = {2017},
	pages = {1--11},
	file = {Springer Full Text PDF:/Users/shaan/Zotero/storage/RBHWN2JK/E - 2017 - A Proposal on Machine Learning via Dynamical Syste.pdf:application/pdf}
}

@article{chang_reversible_2017,
	title = {Reversible {Architectures} for {Arbitrarily} {Deep} {Residual} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1709.03698},
	abstract = {Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memory-efficient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efficacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.},
	urldate = {2020-04-28},
	journal = {arXiv:1709.03698 [cs, stat]},
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	month = nov,
	year = {2017},
	note = {arXiv: 1709.03698},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/8A8XDRJ8/Chang et al. - 2017 - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf;arXiv Fulltext PDF:/Users/shaan/Zotero/storage/I3QY5H8Y/Chang et al. - 2017 - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/EKTWD423/1709.html:text/html;arXiv.org Snapshot:/Users/shaan/Zotero/storage/FW59ISEL/1709.html:text/html}
}

@incollection{watters_visual_2017,
	title = {Visual {Interaction} {Networks}: {Learning} a {Physics} {Simulator} from {Video}},
	shorttitle = {Visual {Interaction} {Networks}},
	url = {http://papers.nips.cc/paper/7040-visual-interaction-networks-learning-a-physics-simulator-from-video.pdf},
	urldate = {2020-04-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Watters, Nicholas and Zoran, Daniel and Weber, Theophane and Battaglia, Peter and Pascanu, Razvan and Tacchetti, Andrea},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {4539--4547},
	file = {NIPS Full Text PDF:/Users/shaan/Zotero/storage/FMQNQ728/Watters et al. - 2017 - Visual Interaction Networks Learning a Physics Si.pdf:application/pdf;NIPS Snapshot:/Users/shaan/Zotero/storage/GYQV5PBL/7040-visual-interaction-networks-learning-a-physics-simulator-from-video.html:text/html}
}

@article{wang_machine_2019,
	title = {Machine {Learning} of {Coarse}-{Grained} {Molecular} {Dynamics} {Force} {Fields}},
	volume = {5},
	issn = {2374-7943},
	url = {https://doi.org/10.1021/acscentsci.8b00913},
	doi = {10.1021/acscentsci.8b00913},
	abstract = {Atomistic or ab initio molecular dynamics simulations are widely used to predict thermodynamics and kinetics and relate them to molecular structure. A common approach to go beyond the time- and length-scales accessible with such computationally expensive simulations is the definition of coarse-grained molecular models. Existing coarse-graining approaches define an effective interaction potential to match defined properties of high-resolution models or experimental data. In this paper, we reformulate coarse-graining as a supervised machine learning problem. We use statistical learning theory to decompose the coarse-graining error and cross-validation to select and compare the performance of different models. We introduce CGnets, a deep learning approach, that learns coarse-grained free energy functions and can be trained by a force-matching scheme. CGnets maintain all physically relevant invariances and allow one to incorporate prior physics knowledge to avoid sampling of unphysical structures. We show that CGnets can capture all-atom explicit-solvent free energy surfaces with models using only a few coarse-grained beads and no solvent, while classical coarse-graining methods fail to capture crucial features of the free energy surface. Thus, CGnets are able to capture multibody terms that emerge from the dimensionality reduction.},
	number = {5},
	urldate = {2020-04-28},
	journal = {ACS Central Science},
	author = {Wang, Jiang and Olsson, Simon and Wehmeyer, Christoph and Pérez, Adrià and Charron, Nicholas E. and de Fabritiis, Gianni and Noé, Frank and Clementi, Cecilia},
	month = may,
	year = {2019},
	note = {Publisher: American Chemical Society},
	pages = {755--767},
	file = {ACS Full Text Snapshot:/Users/shaan/Zotero/storage/PHSEBQZD/acscentsci.html:text/html;Full Text PDF:/Users/shaan/Zotero/storage/FCP8D3DK/Wang et al. - 2019 - Machine Learning of Coarse-Grained Molecular Dynam.pdf:application/pdf}
}

@article{lew_overview_nodate,
	title = {{AN} {OVERVIEW} {OF} {VARIATIONAL} {INTEGRATORS}},
	abstract = {The purpose of this paper is to survey some recent advances in variational integrators for both ﬁnite dimensional mechanical systems as well as continuum mechanics. These advances include the general development of discrete mechanics, applications to dissipative systems, collisions, spacetime integration algorithms, AVI’s (Asynchronous Variational Integrators), as well as reduction for discrete mechanical systems. To keep the article within the set limits, we will only treat each topic brieﬂy and will not attempt to develop any particular topic in any depth. We hope, nonetheless, that this paper serves as a useful guide to the literature as well as to future directions and open problems in the subject.},
	language = {en},
	journal = {Finite Element Methods},
	author = {Lew, Adrian and Marsden, Jerrold E and Ortiz, Michael and West, Matthew},
	pages = {18},
	file = {Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf:/Users/shaan/Zotero/storage/QNP89CA3/Lew et al. - AN OVERVIEW OF VARIATIONAL INTEGRATORS.pdf:application/pdf}
}

@book{noauthor_geometric_nodate,
	title = {Geometric {Numerical} {Integration}: {Structure}-{Preserving} {Algorith}}
}

@book{hairer_geometric_2006,
	address = {Berlin ; New York},
	edition = {2nd ed},
	series = {Springer series in computational mathematics},
	title = {Geometric numerical integration: structure-preserving algorithms for ordinary differential equations},
	isbn = {978-3-540-30663-4},
	shorttitle = {Geometric numerical integration},
	language = {en},
	number = {31},
	publisher = {Springer},
	author = {Hairer, E. and Lubich, Christian and Wanner, Gerhard},
	year = {2006},
	note = {OCLC: ocm69223213},
	keywords = {Differential equations, Hamiltonian systems, Numerical integration, Numerical solutions},
	file = {Hairer et al. - 2006 - Geometric numerical integration structure-preserv.pdf:/Users/shaan/Zotero/storage/LRQ2QWVP/Hairer et al. - 2006 - Geometric numerical integration structure-preserv.pdf:application/pdf;Hairer et al. - 2006 - Geometric numerical integration structure-preserv.pdf:/Users/shaan/Zotero/storage/X5MYJ5ZC/Hairer et al. - 2006 - Geometric numerical integration structure-preserv.pdf:application/pdf}
}

@article{lew_variational_2004,
	title = {Variational time integrators},
	volume = {60},
	issn = {1097-0207},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.958},
	doi = {10.1002/nme.958},
	abstract = {The purpose of this paper is to review and further develop the subject of variational integration algorithms as it applies to mechanical systems of engineering interest. In particular, the conservation properties of both synchronous and asynchronous variational integrators (AVIs) are discussed in detail. We present selected numerical examples which demonstrate the excellent accuracy, conservation and convergence characteristics of AVIs. In these tests, AVIs are found to result in substantial speed-ups, at equal accuracy, relative to explicit Newmark. A mathematical proof of convergence of the AVIs is also presented in this paper. Finally, we develop the subject of horizontal variations and configurational forces in discrete dynamics. This theory leads to exact path-independent characterizations of the configurational forces acting on discrete systems. Notable examples are the configurational forces acting on material nodes in a finite element discretisation; and the J-integral at the tip of a crack in a finite element mesh. Copyright © 2004 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2020-04-28},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Lew, A. and Marsden, J. E. and Ortiz, M. and West, M.},
	year = {2004},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/nme.958},
	keywords = {discrete mechanics, elastodynamics, geometric integration, multi-time-step, subcycling, variational integrators},
	pages = {153--212},
	file = {Snapshot:/Users/shaan/Zotero/storage/WGVJUJ8H/nme.html:text/html;Submitted Version:/Users/shaan/Zotero/storage/JBXMFYJS/Lew et al. - 2004 - Variational time integrators.pdf:application/pdf}
}

@article{hills_algorithm_2015,
	title = {An algorithm for discovering {Lagrangians} automatically from data},
	volume = {1},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-31},
	doi = {10.7717/peerj-cs.31},
	abstract = {An activity fundamental to science is building mathematical models. These models are used to both predict the results of future experiments and gain insight into the structure of the system under study. We present an algorithm that automates the model building process in a scientifically principled way. The algorithm can take observed trajectories from a wide variety of mechanical systems and, without any other prior knowledge or tuning of parameters, predict the future evolution of the system. It does this by applying the principle of least action and searching for the simplest Lagrangian that describes the system’s behaviour. By generating this Lagrangian in a human interpretable form, it can also provide insight into the workings of the system.},
	language = {en},
	urldate = {2020-05-05},
	journal = {PeerJ Computer Science},
	author = {Hills, Daniel J.A. and Grütter, Adrian M. and Hudson, Jonathan J.},
	month = nov,
	year = {2015},
	pages = {e31},
	file = {Hills et al. - 2015 - An algorithm for discovering Lagrangians automatic.pdf:/Users/shaan/Zotero/storage/ECHUNLLG/Hills et al. - 2015 - An algorithm for discovering Lagrangians automatic.pdf:application/pdf}
}

@article{chang_compositional_2017,
	title = {A {Compositional} {Object}-{Based} {Approach} to {Learning} {Physical} {Dynamics}},
	url = {http://arxiv.org/abs/1612.00341},
	abstract = {We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds. By comparing to less structured architectures, we show that the NPE's compositional representation of the structure in physical interactions improves its ability to predict movement, generalize across variable object count and different scene configurations, and infer latent properties of objects such as mass.},
	urldate = {2020-05-13},
	journal = {arXiv:1612.00341 [cs]},
	author = {Chang, Michael B. and Ullman, Tomer and Torralba, Antonio and Tenenbaum, Joshua B.},
	month = mar,
	year = {2017},
	note = {arXiv: 1612.00341},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/GR338UAU/Chang et al. - 2017 - A Compositional Object-Based Approach to Learning .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9RI63QIS/1612.html:text/html}
}

@article{schmidt_distilling_2009,
	title = {Distilling {Free}-{Form} {Natural} {Laws} from {Experimental} {Data}},
	volume = {324},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/324/5923/81},
	doi = {10.1126/science.1165893},
	abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the “alphabet” used to describe those systems.},
	number = {5923},
	journal = {Science},
	author = {Schmidt, Michael and Lipson, Hod},
	year = {2009},
	note = {Publisher: American Association for the Advancement of Science
\_eprint: https://science.sciencemag.org/content/324/5923/81.full.pdf},
	pages = {81--85},
	file = {Snapshot:/Users/shaan/Zotero/storage/626QJWC4/81.html:text/html}
}

@article{iten_discovering_2020,
	title = {Discovering physical concepts with neural networks},
	volume = {124},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1807.10300},
	doi = {10.1103/PhysRevLett.124.010508},
	abstract = {Despite the success of neural networks at solving concrete physics problems, their use as a general-purpose tool for scientific discovery is still in its infancy. Here, we approach this problem by modelling a neural network architecture after the human physical reasoning process, which has similarities to representation learning. This allows us to make progress towards the long-term goal of machine-assisted scientific discovery from experimental data without making prior assumptions about the system. We apply this method to toy examples and show that the network finds the physically relevant parameters, exploits conservation laws to make predictions, and can help to gain conceptual insights, e.g. Copernicus' conclusion that the solar system is heliocentric.},
	number = {1},
	urldate = {2020-05-30},
	journal = {Physical Review Letters},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jan,
	year = {2020},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics},
	pages = {010508},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/DBHMIDYG/Iten et al. - 2020 - Discovering physical concepts with neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/LZFQK78V/1807.html:text/html}
}

@article{rupp_fast_2012,
	title = {Fast and {Accurate} {Modeling} of {Molecular} {Atomization} {Energies} with {Machine} {Learning}},
	volume = {108},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.108.058301},
	doi = {10.1103/PhysRevLett.108.058301},
	abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10 kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.},
	number = {5},
	urldate = {2020-05-30},
	journal = {Physical Review Letters},
	author = {Rupp, Matthias and Tkatchenko, Alexandre and Müller, Klaus-Robert and von Lilienfeld, O. Anatole},
	month = jan,
	year = {2012},
	note = {Publisher: American Physical Society},
	pages = {058301},
	file = {Accepted Version:/Users/shaan/Zotero/storage/8QG675AK/Rupp et al. - 2012 - Fast and Accurate Modeling of Molecular Atomizatio.pdf:application/pdf;APS Snapshot:/Users/shaan/Zotero/storage/4SGKEKVD/PhysRevLett.108.html:text/html}
}

@article{smith_ani-1_2017,
	title = {{ANI}-1: an extensible neural network potential with {DFT} accuracy at force field computational cost},
	volume = {8},
	issn = {2041-6520, 2041-6539},
	shorttitle = {{ANI}-1},
	url = {http://xlink.rsc.org/?DOI=C6SC05720A},
	doi = {10.1039/C6SC05720A},
	abstract = {We demonstrate how a deep neural network (NN) trained on a data set of quantum mechanical (QM) DFT calculated energies can learn an accurate and transferable atomistic potential for organic molecules containing H, C, N, and O atoms.
          , 
            Deep learning is revolutionizing many areas of science and technology, especially image, text, and speech recognition. In this paper, we demonstrate how a deep neural network (NN) trained on quantum mechanical (QM) DFT calculations can learn an accurate and transferable potential for organic molecules. We introduce ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI for short. ANI is a new method designed with the intent of developing transferable neural network potentials that utilize a highly-modified version of the Behler and Parrinello symmetry functions to build single-atom atomic environment vectors (AEV) as a molecular representation. AEVs provide the ability to train neural networks to data that spans both configurational and conformational space, a feat not previously accomplished on this scale. We utilized ANI to build a potential called ANI-1, which was trained on a subset of the GDB databases with up to 8 heavy atoms in order to predict total energies for organic molecules containing four atom types: H, C, N, and O. To obtain an accelerated but physically relevant sampling of molecular potential surfaces, we also proposed a Normal Mode Sampling (NMS) method for generating molecular conformations. Through a series of case studies, we show that ANI-1 is chemically accurate compared to reference DFT calculations on much larger molecular systems (up to 54 atoms) than those included in the training data set.},
	language = {en},
	number = {4},
	urldate = {2020-05-30},
	journal = {Chemical Science},
	author = {Smith, J. S. and Isayev, O. and Roitberg, A. E.},
	year = {2017},
	pages = {3192--3203},
	file = {Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:/Users/shaan/Zotero/storage/9MY3XJRR/Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:application/pdf;Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:/Users/shaan/Zotero/storage/PAUEWMAN/Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:application/pdf;Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:/Users/shaan/Zotero/storage/VCZT7DED/Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:application/pdf}
}

@article{chmiela_machine_2017,
	title = {Machine learning of accurate energy-conserving molecular force fields},
	volume = {3},
	issn = {2375-2548},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5419702/},
	doi = {10.1126/sciadv.1603015},
	abstract = {The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields., Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol−1 for energies and 1 kcal mol−1 Å̊−1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.},
	number = {5},
	urldate = {2020-05-30},
	journal = {Science Advances},
	author = {Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E. and Poltavsky, Igor and Schütt, Kristof T. and Müller, Klaus-Robert},
	month = may,
	year = {2017},
	pmid = {28508076},
	pmcid = {PMC5419702},
	file = {PubMed Central Full Text PDF:/Users/shaan/Zotero/storage/DIDIV6MU/Chmiela et al. - 2017 - Machine learning of accurate energy-conserving mol.pdf:application/pdf}
}

@article{ayed_learning_2019,
	title = {Learning {Dynamical} {Systems} from {Partial} {Observations}},
	url = {http://arxiv.org/abs/1902.11136},
	abstract = {We consider the problem of forecasting complex, nonlinear space-time processes when observations provide only partial information of on the system's state. We propose a natural data-driven framework, where the system's dynamics are modelled by an unknown time-varying differential equation, and the evolution term is estimated from the data, using a neural network. Any future state can then be computed by placing the associated differential equation in an ODE solver. We first evaluate our approach on shallow water and Euler simulations. We find that our method not only demonstrates high quality long-term forecasts, but also learns to produce hidden states closely resembling the true states of the system, without direct supervision on the latter. Additional experiments conducted on challenging, state of the art ocean simulations further validate our findings, while exhibiting notable improvements over classical baselines.},
	urldate = {2020-05-30},
	journal = {arXiv:1902.11136 [physics]},
	author = {Ayed, Ibrahim and de Bézenac, Emmanuel and Pajot, Arthur and Brajard, Julien and Gallinari, Patrick},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.11136},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Dynamical Systems, Physics - Atmospheric and Oceanic Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/DQPVZD4B/Ayed et al. - 2019 - Learning Dynamical Systems from Partial Observatio.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/FZHFVCF4/1902.html:text/html}
}

@article{chang_equivalence_2002,
	title = {The {Equivalence} of {Controlled} {Lagrangian} and {Controlled} {Hamiltonian} {Systems}},
	volume = {8},
	issn = {1292-8119, 1262-3377},
	url = {https://www.cambridge.org/core/journals/esaim-control-optimisation-and-calculus-of-variations/article/equivalence-of-controlled-lagrangian-and-controlled-hamiltonian-systems/610112F255B4CF6036EEB99EBB11DFB8},
	doi = {10.1051/cocv:2002045},
	abstract = {The purpose of this paper is to show that the method of controlled
Lagrangians and its Hamiltonian counterpart (based on the notion
of passivity) are equivalent under rather general hypotheses. We
study the particular case of simple mechanical control systems
(where the underlying Lagrangian is kinetic minus potential
energy) subject to controls and external forces in some detail.
The equivalence makes use of almost Poisson structures (Poisson
brackets that may fail to satisfy the Jacobi identity) on the
Hamiltonian side, which is the Hamiltonian counterpart of a class
of gyroscopic forces on the Lagrangian side.},
	language = {en},
	urldate = {2020-05-30},
	journal = {ESAIM: Control, Optimisation and Calculus of Variations},
	author = {Chang, Dong Eui and Bloch, Anthony M. and Leonard, Naomi E. and Marsden, Jerrold E. and Woolsey, Craig A.},
	year = {2002},
	note = {Publisher: EDP Sciences},
	keywords = {controlled Hamiltonian, Controlled Lagrangian, energy
      shaping, equivalence., Lyapunov stability, passivity},
	pages = {393--422},
	file = {Full Text:/Users/shaan/Zotero/storage/EM6JEPUH/Chang et al. - 2002 - The Equivalence of Controlled Lagrangian and Contr.pdf:application/pdf;Snapshot:/Users/shaan/Zotero/storage/RXSADELJ/610112F255B4CF6036EEB99EBB11DFB8.html:text/html}
}

@incollection{greydanus_hamiltonian_2019,
	title = {Hamiltonian {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/9672-hamiltonian-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Greydanus, Samuel and Dzamba, Misko and Yosinski, Jason},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {15379--15389}
}

@incollection{chen_neural_2018,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {6571--6583}
}

@incollection{de_avila_belbute-peres_end--end_2018,
	title = {End-to-{End} {Differentiable} {Physics} for {Learning} and {Control}},
	url = {http://papers.nips.cc/paper/7948-end-to-end-differentiable-physics-for-learning-and-control.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {de Avila Belbute-Peres, Filipe and Smith, Kevin and Allen, Kelsey and Tenenbaum, Josh and Kolter, J. Zico},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {7178--7189}
}

@article{yao_tensormol-01_2018,
	title = {The {TensorMol}-0.1 model chemistry: a neural network augmented with long-range physics},
	volume = {9},
	issn = {2041-6520, 2041-6539},
	shorttitle = {The {TensorMol}-0.1 model chemistry},
	url = {http://xlink.rsc.org/?DOI=C7SC04934J},
	doi = {10.1039/C7SC04934J},
	abstract = {We construct a robust chemistry consisting of a nearsighted neural network potential, TensorMol-0.1, with screened long-range electrostatic and van der Waals physics. It is offered in an open-source Python package and achieves millihartree accuracy and a scalability to tens-of-thousands of atoms on ordinary laptops.
          , 
            
              Traditional force fields cannot model chemical reactivity, and suffer from low generality without re-fitting. Neural network potentials promise to address these problems, offering energies and forces with near
              ab initio
              accuracy at low cost. However a data-driven approach is naturally inefficient for long-range interatomic forces that have simple physical formulas. In this manuscript we construct a hybrid model chemistry consisting of a nearsighted neural network potential with screened long-range electrostatic and van der Waals physics. This trained potential, simply dubbed “TensorMol-0.1”, is offered in an open-source Python package capable of many of the simulation types commonly used to study chemistry: geometry optimizations, harmonic spectra, open or periodic molecular dynamics, Monte Carlo, and nudged elastic band calculations. We describe the robustness and speed of the package, demonstrating its millihartree accuracy and scalability to tens-of-thousands of atoms on ordinary laptops. We demonstrate the performance of the model by reproducing vibrational spectra, and simulating the molecular dynamics of a protein. Our comparisons with electronic structure theory and experimental data demonstrate that neural network molecular dynamics is poised to become an important tool for molecular simulation, lowering the resource barrier to simulating chemistry.},
	language = {en},
	number = {8},
	urldate = {2020-05-30},
	journal = {Chemical Science},
	author = {Yao, Kun and Herr, John E. and Toth, David W. and Mckintyre, Ryker and Parkhill, John},
	year = {2018},
	pages = {2261--2269},
	file = {Yao et al. - 2018 - The TensorMol-0.1 model chemistry a neural networ.pdf:/Users/shaan/Zotero/storage/YKF5TJ8J/Yao et al. - 2018 - The TensorMol-0.1 model chemistry a neural networ.pdf:application/pdf}
}

@article{zhu_deep_2020,
	title = {Deep {Hamiltonian} networks based on symplectic integrators},
	url = {http://arxiv.org/abs/2004.13830},
	abstract = {HNets is a class of neural networks on grounds of physical prior for learning Hamiltonian systems. This paper explains the influences of different integrators as hyper-parameters on the HNets through error analysis. If we define the network target as the map with zero empirical loss on arbitrary training data, then the non-symplectic integrators cannot guarantee the existence of the network targets of HNets. We introduce the inverse modified equations for HNets and prove that the HNets based on symplectic integrators possess network targets and the differences between the network targets and the original Hamiltonians depend on the accuracy orders of the integrators. Our numerical experiments show that the phase flows of the Hamiltonian systems obtained by symplectic HNets do not exactly preserve the original Hamiltonians, but preserve the network targets calculated; the loss of the network target for the training data and the test data is much less than the loss of the original Hamiltonian; the symplectic HNets have more powerful generalization ability and higher accuracy than the non-symplectic HNets in addressing predicting issues. Thus, the symplectic integrators are of critical importance for HNets.},
	urldate = {2020-06-05},
	journal = {arXiv:2004.13830 [cs, math]},
	author = {Zhu, Aiqing and Jin, Pengzhan and Tang, Yifa},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13830},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/8HN8XIRT/Zhu et al. - 2020 - Deep Hamiltonian networks based on symplectic inte.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/YJJS7VGN/2004.html:text/html}
}

@article{poli_graph_2020,
	title = {Graph {Neural} {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1911.07532},
	abstract = {We introduce the framework of continuous--depth graph neural networks (GNNs). Graph neural ordinary differential equations (GDEs) are formalized as the counterpart to GNNs where the input-output relationship is determined by a continuum of GNN layers, blending discrete topological structures and differential equations. The proposed framework is shown to be compatible with various static and autoregressive GNN models. Results prove general effectiveness of GDEs: in static settings they offer computational advantages by incorporating numerical methods in their forward pass; in dynamic settings, on the other hand, they are shown to improve performance by exploiting the geometry of the underlying dynamics.},
	urldate = {2020-06-05},
	journal = {arXiv:1911.07532 [cs, stat]},
	author = {Poli, Michael and Massaroli, Stefano and Park, Junyoung and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},
	month = feb,
	year = {2020},
	note = {arXiv: 1911.07532},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/T2WMALZ3/Poli et al. - 2020 - Graph Neural Ordinary Differential Equations.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/TVR97WYU/1911.html:text/html}
}

@article{bapst_unveiling_2020,
	title = {Unveiling the predictive power of static structure in glassy systems},
	volume = {16},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1745-2481},
	url = {https://www.nature.com/articles/s41567-020-0842-8},
	doi = {10.1038/s41567-020-0842-8},
	abstract = {Despite decades of theoretical studies, the nature of the glass transition remains elusive and debated, while the existence of structural predictors of its dynamics is a major open question. Recent approaches propose inferring predictors from a variety of human-defined features using machine learning. Here we determine the long-time evolution of a glassy system solely from the initial particle positions and without any handcrafted features, using graph neural networks as a powerful model. We show that this method outperforms current state-of-the-art methods, generalizing over a wide range of temperatures, pressures and densities. In shear experiments, it predicts the locations of rearranging particles. The structural predictors learned by our network exhibit a correlation length that increases with larger timescales to reach the size of our system. Beyond glasses, our method could apply to many other physical systems that map to a graph of local interaction. The physics that underlies the glass transition is both subtle and non-trivial. A machine learning approach based on graph networks is now shown to accurately predict the dynamics of glasses over a wide range of temperatures, pressures and densities.},
	language = {en},
	number = {4},
	urldate = {2020-06-05},
	journal = {Nature Physics},
	author = {Bapst, V. and Keck, T. and Grabska-Barwińska, A. and Donner, C. and Cubuk, E. D. and Schoenholz, S. S. and Obika, A. and Nelson, A. W. R. and Back, T. and Hassabis, D. and Kohli, P.},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Nature Publishing Group},
	pages = {448--454},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/AMBJGJWL/Bapst et al. - 2020 - Unveiling the predictive power of static structure.pdf:application/pdf;Snapshot:/Users/shaan/Zotero/storage/P7MHZY6B/s41567-020-0842-8.html:text/html}
}

@article{kim_hamilton-jacobi-bellman_2020,
	title = {Hamilton-{Jacobi}-{Bellman} {Equations} for {Q}-{Learning} in {Continuous} {Time}},
	url = {http://arxiv.org/abs/1912.10697},
	abstract = {In this paper, we introduce Hamilton-Jacobi-Bellman (HJB) equations for Q-functions in continuous time optimal control problems with Lipschitz continuous controls. The standard Q-function used in reinforcement learning is shown to be the unique viscosity solution of the HJB equation. A necessary and sufficient condition for optimality is provided using the viscosity solution framework. By using the HJB equation, we develop a Q-learning method for continuous-time dynamical systems. A DQN-like algorithm is also proposed for high-dimensional state and control spaces. The performance of the proposed Q-learning algorithm is demonstrated using 1-, 10- and 20-dimensional dynamical systems.},
	urldate = {2020-06-08},
	journal = {arXiv:1912.10697 [cs, eess, math]},
	author = {Kim, Jeongho and Yang, Insoon},
	month = may,
	year = {2020},
	note = {arXiv: 1912.10697},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/654JEP4S/Kim and Yang - 2020 - Hamilton-Jacobi-Bellman Equations for Q-Learning i.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/CC5TNH3D/1912.html:text/html}
}

@article{qing_introduction_nodate,
	title = {{INTRODUCTION} {TO} {THE} {OPTIMAL} {CONTROL} {THEORY} {AND} {SOME} {APPLICATIONS}},
	abstract = {This paper aims to give a brief introduction to the optimal control theory and attempts to derive some of the central results of the subject, including the Hamilton-Jacobi-Bellman PDE and the Pontryagin Maximal Principle. Along the way, some of the more rigorous mathematical tools, such as Hamilton-Jacobi equations, viscosity solutions for PDEs, and the method of characteristics, will be introduced. Finally, some particular examples will be studied at the end of the paper using the developed theorems.},
	language = {en},
	author = {Qing, Yutong},
	pages = {18},
	file = {Qing - INTRODUCTION TO THE OPTIMAL CONTROL THEORY AND SOM.pdf:/Users/shaan/Zotero/storage/2TRFDX94/Qing - INTRODUCTION TO THE OPTIMAL CONTROL THEORY AND SOM.pdf:application/pdf}
}

@article{finzi_generalizing_2020,
	title = {Generalizing {Convolutional} {Neural} {Networks} for {Equivariance} to {Lie} {Groups} on {Arbitrary} {Continuous} {Data}},
	url = {http://arxiv.org/abs/2002.12880},
	abstract = {The translation equivariance of convolutional layers enables convolutional neural networks to generalize well on image problems. While translation equivariance provides a powerful inductive bias for images, we often additionally desire equivariance to other transformations, such as rotations, especially for non-image data. We propose a general method to construct a convolutional layer that is equivariant to transformations from any specified Lie group with a surjective exponential map. Incorporating equivariance to a new group requires implementing only the group exponential and logarithm maps, enabling rapid prototyping. Showcasing the simplicity and generality of our method, we apply the same model architecture to images, ball-and-stick molecular data, and Hamiltonian dynamical systems. For Hamiltonian systems, the equivariance of our models is especially impactful, leading to exact conservation of linear and angular momentum.},
	urldate = {2020-06-09},
	journal = {arXiv:2002.12880 [cs, stat]},
	author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
	month = may,
	year = {2020},
	note = {arXiv: 2002.12880},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XQ8MRMKQ/Finzi et al. - 2020 - Generalizing Convolutional Neural Networks for Equ.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/IJ8FQLSJ/2002.html:text/html}
}

@article{xu_implementing_2020,
	title = {Implementing {Inductive} bias for different navigation tasks through diverse {RNN} attractors},
	url = {http://arxiv.org/abs/2002.02496},
	abstract = {Navigation is crucial for animal behavior and is assumed to require an internal representation of the external environment, termed a cognitive map. The precise form of this representation is often considered to be a metric representation of space. An internal representation, however, is judged by its contribution to performance on a given task, and may thus vary between different types of navigation tasks. Here we train a recurrent neural network that controls an agent performing several navigation tasks in a simple environment. To focus on internal representations, we split learning into a task-agnostic pre-training stage that modifies internal connectivity and a task-specific Q learning stage that controls the network's output. We show that pre-training shapes the attractor landscape of the networks, leading to either a continuous attractor, discrete attractors or a disordered state. These structures induce bias onto the Q-Learning phase, leading to a performance pattern across the tasks corresponding to metric and topological regularities. By combining two types of networks in a modular structure, we could get better performance for both regularities. Our results show that, in recurrent networks, inductive bias takes the form of attractor landscapes -- which can be shaped by pre-training and analyzed using dynamical systems methods. Furthermore, we demonstrate that non-metric representations are useful for navigation tasks, and their combination with metric representation leads to flexibile multiple-task learning.},
	urldate = {2020-06-09},
	journal = {arXiv:2002.02496 [q-bio]},
	author = {Xu, Tie and Barak, Omri},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.02496},
	keywords = {Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/I7JNFPLZ/Xu and Barak - 2020 - Implementing Inductive bias for different navigati.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/MKEZXCIA/2002.html:text/html}
}

@article{atkinson_bayesian_2020,
	title = {Bayesian {Hidden} {Physics} {Models}: {Uncertainty} {Quantification} for {Discovery} of {Nonlinear} {Partial} {Differential} {Operators} from {Data}},
	shorttitle = {Bayesian {Hidden} {Physics} {Models}},
	url = {http://arxiv.org/abs/2006.04228},
	abstract = {What do data tell us about physics-and what don't they tell us? There has been a surge of interest in using machine learning models to discover governing physical laws such as differential equations from data, but current methods lack uncertainty quantification to communicate their credibility. This work addresses this shortcoming from a Bayesian perspective. We introduce a novel model comprising "leaf" modules that learn to represent distinct experiments' spatiotemporal functional data as neural networks and a single "root" module that expresses a nonparametric distribution over their governing nonlinear differential operator as a Gaussian process. Automatic differentiation is used to compute the required partial derivatives from the leaf functions as inputs to the root. Our approach quantifies the reliability of the learned physics in terms of a posterior distribution over operators and propagates this uncertainty to solutions of novel initial-boundary value problem instances. Numerical experiments demonstrate the method on several nonlinear PDEs.},
	urldate = {2020-06-09},
	journal = {arXiv:2006.04228 [cs, stat]},
	author = {Atkinson, Steven},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04228},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/H6IY888L/Atkinson - 2020 - Bayesian Hidden Physics Models Uncertainty Quanti.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/AQ8UI4FS/2006.html:text/html}
}

@article{kaxiras_first_2020,
	title = {The first 100 days: modeling the evolution of the {COVID}-19 pandemic},
	shorttitle = {The first 100 days},
	url = {http://arxiv.org/abs/2004.14664},
	abstract = {A simple analytical model for modeling the evolution of the 2020 COVID-19 pandemic is presented. The model is based on the numerical solution of the widely used Susceptible-Infectious-Removed (SIR) populations model for describing epidemics. We consider an expanded version of the original Kermack-McKendrick model, which includes a decaying value of the parameter \${\textbackslash}beta\$ (the effective contact rate) due to externally imposed conditions, to which we refer as the forced-SIR (FSIR) model. We introduce an approximate analytical solution to the differential equations that represent the FSIR model which gives very reasonable fits to real data for a number of countries over a period of 100 days (from the first onset of exponential increase, in China). The proposed model contains 3 adjustable parameters which are obtained by fitting actual data (up to April 28, 2020). We analyze these results to infer the physical meaning of the parameters involved. We use the model to make predictions about the total expected number of infections in each country as well as the date when the number of infections will have reached 99\% of this total. We also compare key findings of the model with recently reported results on the high contagiousness and rapid spread of the disease.},
	urldate = {2020-06-10},
	journal = {arXiv:2004.14664 [q-bio]},
	author = {Kaxiras, Efthimios and Neofotistos, George and Angelaki, Eleni},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.14664},
	keywords = {Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/DW92I5A4/Kaxiras et al. - 2020 - The first 100 days modeling the evolution of the .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/E2TG48CZ/2004.html:text/html}
}

@article{mana_favour_2011,
	title = {In favour of the time variable in classical {thermoDYNAMICS}},
	url = {http://arxiv.org/abs/1012.3091},
	abstract = {A case for the teaching of classical thermodynamics with an explicit time variable, with phenomena involving changes in time, is made by presenting and solving a exercise in textbook style, and pointing out that a solution accords with experiment. The exercise requires an explicit treatment of the time variable. Further arguments are given for the advantages of an explicit time variable in classical thermodynamics, and against some standard terminology in this theory.},
	urldate = {2020-06-12},
	journal = {arXiv:1012.3091 [physics]},
	author = {Mana, PierGianLuca Porta},
	month = jan,
	year = {2011},
	note = {arXiv: 1012.3091},
	keywords = {Physics - Classical Physics, 80-01, 97M50, 34-04, Physics - Physics Education},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ESDQRSBW/Mana - 2011 - In favour of the time variable in classical thermo.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8PC9ISCV/1012.html:text/html}
}

@article{stutt_modelling_2020,
	title = {A modelling framework to assess the likely effectiveness of facemasks in combination with ‘lock-down’ in managing the {COVID}-19 pandemic},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2020.0376},
	doi = {10.1098/rspa.2020.0376},
	language = {en},
	number = {2238},
	urldate = {2020-06-14},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Stutt, Richard O. J. H. and Retkute, Renata and Bradley, Michael and Gilligan, Christopher A. and Colvin, John},
	month = jun,
	year = {2020},
	pages = {20200376},
	file = {Stutt et al. - 2020 - A modelling framework to assess the likely effecti.pdf:/Users/shaan/Zotero/storage/ZGSBXL6Y/Stutt et al. - 2020 - A modelling framework to assess the likely effecti.pdf:application/pdf}
}

@article{yan_interpretable_2020,
	title = {An interpretable mortality prediction model for {COVID}-19 patients},
	volume = {2},
	copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-0180-7},
	doi = {10.1038/s42256-020-0180-7},
	abstract = {The sudden increase in COVID-19 cases is putting high pressure on healthcare services worldwide. At this stage, fast, accurate and early clinical assessment of the disease severity is vital. To support decision making and logistical planning in healthcare systems, this study leverages a database of blood samples from 485 infected patients in the region of Wuhan, China, to identify crucial predictive biomarkers of disease mortality. For this purpose, machine learning tools selected three biomarkers that predict the mortality of individual patients more than 10 days in advance with more than 90\% accuracy: lactic dehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein (hs-CRP). In particular, relatively high levels of LDH alone seem to play a crucial role in distinguishing the vast majority of cases that require immediate medical attention. This finding is consistent with current medical knowledge that high LDH levels are associated with tissue breakdown occurring in various diseases, including pulmonary disorders such as pneumonia. Overall, this Article suggests a simple and operable decision rule to quickly predict patients at the highest risk, allowing them to be prioritized and potentially reducing the mortality rate.},
	language = {en},
	number = {5},
	urldate = {2020-06-14},
	journal = {Nature Machine Intelligence},
	author = {Yan, Li and Zhang, Hai-Tao and Goncalves, Jorge and Xiao, Yang and Wang, Maolin and Guo, Yuqi and Sun, Chuan and Tang, Xiuchuan and Jing, Liang and Zhang, Mingyang and Huang, Xiang and Xiao, Ying and Cao, Haosen and Chen, Yanyan and Ren, Tongxin and Wang, Fang and Xiao, Yaru and Huang, Sufang and Tan, Xi and Huang, Niannian and Jiao, Bo and Cheng, Cheng and Zhang, Yong and Luo, Ailin and Mombaerts, Laurent and Jin, Junyang and Cao, Zhiguo and Li, Shusheng and Xu, Hui and Yuan, Ye},
	month = may,
	year = {2020},
	note = {Number: 5
Publisher: Nature Publishing Group},
	pages = {283--288},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/JB6JN7KM/Yan et al. - 2020 - An interpretable mortality prediction model for CO.pdf:application/pdf;Snapshot:/Users/shaan/Zotero/storage/F8E7Q9A7/s42256-020-0180-7.html:text/html}
}

@article{stutt_modelling_2020-1,
	title = {A modelling framework to assess the likely effectiveness of facemasks in combination with ‘lock-down’ in managing the {COVID}-19 pandemic},
	volume = {476},
	issn = {1364-5021, 1471-2946},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2020.0376},
	doi = {10.1098/rspa.2020.0376},
	language = {en},
	number = {2238},
	urldate = {2020-06-14},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Stutt, Richard O. J. H. and Retkute, Renata and Bradley, Michael and Gilligan, Christopher A. and Colvin, John},
	month = jun,
	year = {2020},
	pages = {20200376},
	file = {Stutt et al. - 2020 - A modelling framework to assess the likely effecti.pdf:/Users/shaan/Zotero/storage/R7EA8GAW/Stutt et al. - 2020 - A modelling framework to assess the likely effecti.pdf:application/pdf}
}

@article{friston_second_2020,
	title = {Second waves, social distancing, and the spread of {COVID}-19 across {America}},
	url = {http://arxiv.org/abs/2004.13017},
	abstract = {We recently described a dynamic causal model of a COVID-19 outbreak within a single region. Here, we combine several of these (epidemic) models to create a (pandemic) model of viral spread among regions. Our focus is on a second wave of new cases that may result from loss of immunity--and the exchange of people between regions--and how mortality rates can be ameliorated under different strategic responses. In particular, we consider hard or soft social distancing strategies predicated on national (Federal) or regional (State) estimates of the prevalence of infection in the population. The modelling is demonstrated using timeseries of new cases and deaths from the United States to estimate the parameters of a factorial (compartmental) epidemiological model of each State and, crucially, coupling between States. Using Bayesian model reduction, we identify the effective connectivity between States that best explains the initial phases of the outbreak in the United States. Using the ensuing posterior parameter estimates, we then evaluate the likely outcomes of different policies in terms of mortality, working days lost due to lockdown and demands upon critical care. The provisional results of this modelling suggest that social distancing and loss of immunity are the two key factors that underwrite a return to endemic equilibrium.},
	urldate = {2020-06-16},
	journal = {arXiv:2004.13017 [q-bio]},
	author = {Friston, Karl J. and Parr, Thomas and Zeidman, Peter and Razi, Adeel and Flandin, Guillaume and Daunizeau, Jean and Hulme, Oliver J. and Billig, Alexander J. and Litvak, Vladimir and Price, Cathy J. and Moran, Rosalyn J. and Lambert, Christian},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13017},
	keywords = {Quantitative Biology - Populations and Evolution, q-bio.QM, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6832NTKS/Friston et al. - 2020 - Second waves, social distancing, and the spread of.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/UBUNBFES/2004.html:text/html}
}

@article{ball_ready_2020,
	title = {Ready {Policy} {One}: {World} {Building} {Through} {Active} {Learning}},
	shorttitle = {Ready {Policy} {One}},
	url = {http://arxiv.org/abs/2002.02693},
	abstract = {Model-Based Reinforcement Learning (MBRL) offers a promising direction for sample efficient learning, often achieving state of the art results for continuous control tasks. However, many existing MBRL methods rely on combining greedy policies with exploration heuristics, and even those which utilize principled exploration bonuses construct dual objectives in an ad hoc fashion. In this paper we introduce Ready Policy One (RP1), a framework that views MBRL as an active learning problem, where we aim to improve the world model in the fewest samples possible. RP1 achieves this by utilizing a hybrid objective function, which crucially adapts during optimization, allowing the algorithm to trade off reward v.s. exploration at different stages of learning. In addition, we introduce a principled mechanism to terminate sample collection once we have a rich enough trajectory batch to improve the model. We rigorously evaluate our method on a variety of continuous control tasks, and demonstrate statistically significant gains over existing approaches.},
	urldate = {2020-06-16},
	journal = {arXiv:2002.02693 [cs, stat]},
	author = {Ball, Philip and Parker-Holder, Jack and Pacchiano, Aldo and Choromanski, Krzysztof and Roberts, Stephen},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.02693},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/RFPNKT4W/Ball et al. - 2020 - Ready Policy One World Building Through Active Le.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HAM6MCTE/2002.html:text/html}
}

@article{janner_when_2019,
	title = {When to {Trust} {Your} {Model}: {Model}-{Based} {Policy} {Optimization}},
	shorttitle = {When to {Trust} {Your} {Model}},
	url = {http://arxiv.org/abs/1906.08253},
	abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
	urldate = {2020-06-16},
	journal = {arXiv:1906.08253 [cs, stat]},
	author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
	month = nov,
	year = {2019},
	note = {arXiv: 1906.08253},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/F7JKFJL8/Janner et al. - 2019 - When to Trust Your Model Model-Based Policy Optim.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8DQEEP2X/1906.html:text/html}
}

@article{sekar_planning_2020,
	title = {Planning to {Explore} via {Self}-{Supervised} {World} {Models}},
	url = {http://arxiv.org/abs/2005.05960},
	abstract = {Reinforcement learning allows solving complex tasks, however, the learning tends to be task-specific and the sample efficiency remains a challenge. We present Plan2Explore, a self-supervised reinforcement learning agent that tackles both these challenges through a new approach to self-supervised exploration and fast adaptation to new tasks, which need not be known during exploration. During exploration, unlike prior methods which retrospectively compute the novelty of observations after the agent has already reached them, our agent acts efficiently by leveraging planning to seek out expected future novelty. After exploration, the agent quickly adapts to multiple downstream tasks in a zero or a few-shot manner. We evaluate on challenging control tasks from high-dimensional image inputs. Without any training supervision or task-specific interaction, Plan2Explore outperforms prior self-supervised exploration methods, and in fact, almost matches the performances oracle which has access to rewards. Videos and code at https://ramanans1.github.io/plan2explore/},
	urldate = {2020-06-16},
	journal = {arXiv:2005.05960 [cs, stat]},
	author = {Sekar, Ramanan and Rybkin, Oleh and Daniilidis, Kostas and Abbeel, Pieter and Hafner, Danijar and Pathak, Deepak},
	month = may,
	year = {2020},
	note = {arXiv: 2005.05960},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/DY9ETJYR/Sekar et al. - 2020 - Planning to Explore via Self-Supervised World Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/95PXMIAX/2005.html:text/html}
}

@article{okyere_deterministic_2019,
	title = {Deterministic {Epidemic} {Models} {For} {Ebola} {Infection} {With} {Time}-dependent {Controls}},
	url = {http://arxiv.org/abs/1908.07974},
	abstract = {In this paper, we have studied epidemiological models for Ebola infection using nonlinear ordinary differential equations and optimal control theory. We considered optimal control analysis of SIR and SEIR models for the deadly Ebola infection using vaccination, treatment and educational campaign as time-dependent controls functions. We have applied indirect methods to study existing deterministic optimal control epidemic models for Ebola virus disease. These methods in optimal control are based on Hamiltonian function and the Pontryagin's maximum principle to construct adjoint equations and optimality systems. The forward-backward sweep numerical scheme with fourth-order Runge-Kutta method is used to solve the optimality system for the various control strategies. From our numerical illustrations, we can conclude that, effective educational campaigns and vaccination of susceptible individuals as were as effective treatments of infected individuals can help reduce the disease transmission.},
	urldate = {2020-06-17},
	journal = {arXiv:1908.07974 [math, q-bio]},
	author = {Okyere, E. and Ankamah, J. D. and Hunkpe, A. K. and Mensah, D.},
	month = aug,
	year = {2019},
	note = {arXiv: 1908.07974},
	keywords = {Mathematics - Dynamical Systems, Mathematics - Optimization and Control, Quantitative Biology - Populations and Evolution},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/LJ94L3JI/Okyere et al. - 2019 - Deterministic Epidemic Models For Ebola Infection .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/34E8KQBJ/1908.html:text/html}
}

@article{mattheakis_hamiltonian_2020,
	title = {Hamiltonian {Neural} {Networks} for solving differential equations},
	url = {http://arxiv.org/abs/2001.11107},
	abstract = {There has been a wave of interest in applying machine learning to study dynamical systems. In particular, neural networks have been applied to solve the equations of motion, and therefore, track the evolution of a system. In contrast to other applications of neural networks and machine learning, dynamical systems -- depending on their underlying symmetries -- possess invariants such as energy, momentum, and angular momentum. Traditional numerical iteration methods usually violate these conservation laws, propagating errors in time, and reducing the predictability of the method. We present a Hamiltonian neural network that solves the differential equations that govern dynamical systems. This unsupervised model is learning solutions that satisfy identically, up to an arbitrarily small error, Hamilton's equations and, therefore, conserve the Hamiltonian invariants. Once it is optimized, the proposed architecture is considered a symplectic unit due to the introduction of an efficient parametric form of solutions. In addition, by sharing the network parameters and the choice of an appropriate activation function drastically improve the predictability of the network. An error analysis is derived and states that the numerical errors depend on the overall network performance. The symplectic architecture is then employed to solve the equations for the nonlinear oscillator and the chaotic Henon-Heiles dynamical system. In both systems, the symplectic Euler integrator requires two orders more evaluation points than the Hamiltonian network in order to achieve the same order of the numerical error in the predicted phase space trajectories.},
	urldate = {2020-06-17},
	journal = {arXiv:2001.11107 [physics]},
	author = {Mattheakis, Marios and Sondak, David and Dogra, Akshunna S. and Protopapas, Pavlos},
	month = feb,
	year = {2020},
	note = {arXiv: 2001.11107},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/M4JXT5QT/Mattheakis et al. - 2020 - Hamiltonian Neural Networks for solving differenti.pdf:application/pdf}
}

@article{mattheakis_hamiltonian_2020-1,
	title = {Hamiltonian {Neural} {Networks} for solving differential equations},
	url = {http://arxiv.org/abs/2001.11107},
	abstract = {There has been a wave of interest in applying machine learning to study dynamical systems. In particular, neural networks have been applied to solve the equations of motion, and therefore, track the evolution of a system. In contrast to other applications of neural networks and machine learning, dynamical systems -- depending on their underlying symmetries -- possess invariants such as energy, momentum, and angular momentum. Traditional numerical iteration methods usually violate these conservation laws, propagating errors in time, and reducing the predictability of the method. We present a Hamiltonian neural network that solves the differential equations that govern dynamical systems. This unsupervised model is learning solutions that satisfy identically, up to an arbitrarily small error, Hamilton's equations and, therefore, conserve the Hamiltonian invariants. Once it is optimized, the proposed architecture is considered a symplectic unit due to the introduction of an efficient parametric form of solutions. In addition, by sharing the network parameters and the choice of an appropriate activation function drastically improve the predictability of the network. An error analysis is derived and states that the numerical errors depend on the overall network performance. The symplectic architecture is then employed to solve the equations for the nonlinear oscillator and the chaotic Henon-Heiles dynamical system. In both systems, the symplectic Euler integrator requires two orders more evaluation points than the Hamiltonian network in order to achieve the same order of the numerical error in the predicted phase space trajectories.},
	urldate = {2020-06-17},
	journal = {arXiv:2001.11107 [physics]},
	author = {Mattheakis, Marios and Sondak, David and Dogra, Akshunna S. and Protopapas, Pavlos},
	month = feb,
	year = {2020},
	note = {arXiv: 2001.11107},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/RGXZG92K/Mattheakis et al. - 2020 - Hamiltonian Neural Networks for solving differenti.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/974BMW8R/2001.html:text/html}
}

@article{rabault_deep_2020,
	title = {Deep {Reinforcement} {Learning} in {Fluid} {Mechanics}: a promising method for both {Active} {Flow} {Control} and {Shape} {Optimization}},
	shorttitle = {Deep {Reinforcement} {Learning} in {Fluid} {Mechanics}},
	url = {http://arxiv.org/abs/2001.02464},
	abstract = {In recent years, Artificial Neural Networks (ANNs) and Deep Learning have become increasingly popular across a wide range of scientific and technical fields, including Fluid Mechanics. While it will take time to fully grasp the potentialities as well as the limitations of these methods, evidence is starting to accumulate that point to their potential in helping solve problems for which no theoretically optimal solution method is known. This is particularly true in Fluid Mechanics, where problems involving optimal control and optimal design are involved. Indeed, such problems are famously difficult to solve effectively with traditional methods due to the combination of non linearity, non convexity, and high dimensionality they involve. By contrast, Deep Reinforcement Learning (DRL), a method of optimization based on teaching empirical strategies to an ANN through trial and error, is well adapted to solving such problems. In this short review, we offer an insight into the current state of the art of the use of DRL within fluid mechanics, focusing on control and optimal design problems.},
	urldate = {2020-06-17},
	journal = {arXiv:2001.02464 [physics]},
	author = {Rabault, Jean and Ren, Feng and Zhang, Wei and Tang, Hui and Xu, Hui},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.02464},
	keywords = {Physics - Computational Physics, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/JQY4FAB3/Rabault et al. - 2020 - Deep Reinforcement Learning in Fluid Mechanics a .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/NEYH88M4/2001.html:text/html}
}

@article{sprangers_reinforcement_2015,
	title = {Reinforcement learning for port-{Hamiltonian} systems},
	volume = {45},
	issn = {2168-2267, 2168-2275},
	url = {http://arxiv.org/abs/1212.5524},
	doi = {10.1109/TCYB.2014.2343194},
	abstract = {Passivity-based control (PBC) for port-Hamiltonian systems provides an intuitive way of achieving stabilization by rendering a system passive with respect to a desired storage function. However, in most instances the control law is obtained without any performance considerations and it has to be calculated by solving a complex partial differential equation (PDE). In order to address these issues we introduce a reinforcement learning approach into the energy-balancing passivity-based control (EB-PBC) method, which is a form of PBC in which the closed-loop energy is equal to the difference between the stored and supplied energies. We propose a technique to parameterize EB-PBC that preserves the systems's PDE matching conditions, does not require the specification of a global desired Hamiltonian, includes performance criteria, and is robust to extra non-linearities such as control input saturation. The parameters of the control law are found using actor-critic reinforcement learning, enabling learning near-optimal control policies satisfying a desired closed-loop energy landscape. The advantages are that near-optimal controllers can be generated using standard energy shaping techniques and that the solutions learned can be interpreted in terms of energy shaping and damping injection, which makes it possible to numerically assess stability using passivity theory. From the reinforcement learning perspective, our proposal allows for the class of port-Hamiltonian systems to be incorporated in the actor-critic framework, speeding up the learning thanks to the resulting parameterization of the policy. The method has been successfully applied to the pendulum swing-up problem in simulations and real-life experiments.},
	number = {5},
	urldate = {2020-06-18},
	journal = {IEEE Transactions on Cybernetics},
	author = {Sprangers, Olivier and Lopes, Gabriel A. D. and Babuska, Robert},
	month = may,
	year = {2015},
	note = {arXiv: 1212.5524},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	pages = {1017--1027},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/7MU3N2BD/Sprangers et al. - 2015 - Reinforcement learning for port-Hamiltonian system.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/GPALL2R5/1212.html:text/html}
}

@misc{hill_alsnhllseir_covid19_2020,
	title = {alsnhll/{SEIR}\_COVID19},
	url = {https://github.com/alsnhll/SEIR_COVID19},
	abstract = {SEIR model for COVID-19 infection, including different clinical trajectories of infection},
	urldate = {2020-06-18},
	author = {Hill, Alison},
	month = jun,
	year = {2020},
	note = {original-date: 2020-03-09T06:28:22Z}
}

@article{roehrl_modeling_2020,
	title = {Modeling {System} {Dynamics} with {Physics}-{Informed} {Neural} {Networks} {Based} on {Lagrangian} {Mechanics}},
	url = {http://arxiv.org/abs/2005.14617},
	abstract = {Identifying accurate dynamic models is required for the simulation and control of various technical systems. In many important real-world applications, however, the two main modeling approaches often fail to meet requirements: first principles methods suffer from high bias, whereas data-driven modeling tends to have high variance. Additionally, purely data-based models often require large amounts of data and are often difficult to interpret. In this paper, we present physics-informed neural ordinary differential equations (PINODE), a hybrid model that combines the two modeling techniques to overcome the aforementioned problems. This new approach directly incorporates the equations of motion originating from the Lagrange Mechanics into a deep neural network structure. Thus, we can integrate prior physics knowledge where it is available and use function approximation--e. g., neural networks--where it is not. The method is tested with a forward model of a real-world physical system with large uncertainties. The resulting model is accurate and data-efficient while ensuring physical plausibility. With this, we demonstrate a method that beneficially merges physical insight with real data. Our findings are of interest for model-based control and system identification of mechanical systems.},
	urldate = {2020-06-19},
	journal = {arXiv:2005.14617 [cs, stat]},
	author = {Roehrl, Manuel A. and Runkler, Thomas A. and Brandtstetter, Veronika and Tokic, Michel and Obermayer, Stefan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.14617},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/PGEDVV2Q/Roehrl et al. - 2020 - Modeling System Dynamics with Physics-Informed Neu.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/ISIK7K2C/2005.html:text/html}
}

@article{laskin_reinforcement_2020,
	title = {Reinforcement {Learning} with {Augmented} {Data}},
	url = {http://arxiv.org/abs/2004.14990},
	abstract = {Learning from visual observations is a fundamental yet challenging problem in reinforcement learning (RL). Although algorithmic advancements combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) sample efficiency of learning and (b) generalization to new environments. To this end, we present RAD: Reinforcement Learning with Augmented Data, a simple plug-and-play module that can enhance any RL algorithm. We show that data augmentations such as random crop, color jitter, patch cutout, and random convolutions can enable simple RL algorithms to match and even outperform complex state-of-the-art methods across common benchmarks in terms of data-efficiency, generalization, and wall-clock speed. We find that data diversity alone can make agents focus on meaningful information from high-dimensional observations without any changes to the reinforcement learning method. On the DeepMind Control Suite, we show that RAD is state-of-the-art in terms of data-efficiency and performance across 15 environments. We further demonstrate that RAD can significantly improve the test-time generalization on several OpenAI ProcGen benchmarks. Finally, our customized data augmentation modules enable faster wall-clock speed compared to competing RL techniques. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
	urldate = {2020-06-19},
	journal = {arXiv:2004.14990 [cs, stat]},
	author = {Laskin, Michael and Lee, Kimin and Stooke, Adam and Pinto, Lerrel and Abbeel, Pieter and Srinivas, Aravind},
	month = may,
	year = {2020},
	note = {arXiv: 2004.14990},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/B72WWVKT/Laskin et al. - 2020 - Reinforcement Learning with Augmented Data.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/FAE726LU/2004.html:text/html}
}

@article{orbanz_lecture_nodate,
	title = {Lecture {Notes} on {Bayesian} {Nonparametrics}},
	language = {en},
	author = {Orbanz, Peter},
	pages = {108},
	file = {Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:/Users/shaan/Zotero/storage/56A5QEEE/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:application/pdf}
}

@article{orbanz_lecture_nodate-1,
	title = {Lecture {Notes} on {Bayesian} {Nonparametrics}},
	language = {en},
	author = {Orbanz, Peter},
	pages = {108},
	file = {Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:/Users/shaan/Zotero/storage/W29DHS2L/Orbanz - Lecture Notes on Bayesian Nonparametrics.pdf:application/pdf}
}

@inproceedings{toussaint_differentiable_2018,
	title = {Differentiable {Physics} and {Stable} {Modes} for {Tool}-{Use} and {Manipulation} {Planning}},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p44.pdf},
	doi = {10.15607/RSS.2018.XIV.044},
	abstract = {We consider the problem of sequential manipulation and tool-use planning in domains that include physical interactions such as hitting and throwing. The approach integrates a Task And Motion Planning formulation with primitives that either impose stable kinematic constraints or differentiable dynamical and impulse exchange constraints at the path optimization level. We demonstrate our approach on a variety of physical puzzles that involve tool use and dynamic interactions. We then compare manipulation sequences generated by our approach to human actions on analogous tasks, suggesting future directions and illuminating current limitations.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Robotics: {Science} and {Systems} {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Toussaint, Marc and Allen, Kelsey and Smith, Kevin and Tenenbaum, Joshua},
	month = jun,
	year = {2018},
	file = {Toussaint et al. - 2018 - Differentiable Physics and Stable Modes for Tool-U.pdf:/Users/shaan/Zotero/storage/L626JPKU/Toussaint et al. - 2018 - Differentiable Physics and Stable Modes for Tool-U.pdf:application/pdf}
}

@inproceedings{toussaint_differentiable_2018-1,
	title = {Differentiable {Physics} and {Stable} {Modes} for {Tool}-{Use} and {Manipulation} {Planning}},
	isbn = {978-0-9923747-4-7},
	url = {http://www.roboticsproceedings.org/rss14/p44.pdf},
	doi = {10.15607/RSS.2018.XIV.044},
	abstract = {We consider the problem of sequential manipulation and tool-use planning in domains that include physical interactions such as hitting and throwing. The approach integrates a Task And Motion Planning formulation with primitives that either impose stable kinematic constraints or differentiable dynamical and impulse exchange constraints at the path optimization level. We demonstrate our approach on a variety of physical puzzles that involve tool use and dynamic interactions. We then compare manipulation sequences generated by our approach to human actions on analogous tasks, suggesting future directions and illuminating current limitations.},
	language = {en},
	urldate = {2020-06-23},
	booktitle = {Robotics: {Science} and {Systems} {XIV}},
	publisher = {Robotics: Science and Systems Foundation},
	author = {Toussaint, Marc and Allen, Kelsey and Smith, Kevin and Tenenbaum, Joshua},
	month = jun,
	year = {2018},
	file = {Toussaint et al. - 2018 - Differentiable Physics and Stable Modes for Tool-U.pdf:/Users/shaan/Zotero/storage/Z2JIFR9E/Toussaint et al. - 2018 - Differentiable Physics and Stable Modes for Tool-U.pdf:application/pdf}
}

@misc{jepsen_how_2019,
	title = {How to do {Deep} {Learning} on {Graphs} with {Graph} {Convolutional} {Networks}},
	url = {https://towardsdatascience.com/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780},
	abstract = {Part 1: A High-Level Introduction to Graph Convolutional Networks},
	language = {en},
	urldate = {2020-06-25},
	journal = {Medium},
	author = {Jepsen, Tobias Skovgaard},
	month = may,
	year = {2019},
	note = {Library Catalog: towardsdatascience.com},
	file = {Snapshot:/Users/shaan/Zotero/storage/F7CPDR2P/how-to-do-deep-learning-on-graphs-with-graph-convolutional-networks-7d2250723780.html:text/html}
}

@article{udrescu_symbolic_2020,
	title = {Symbolic {Pregression}: {Discovering} {Physical} {Laws} from {Raw} {Distorted} {Video}},
	shorttitle = {Symbolic {Pregression}},
	url = {http://arxiv.org/abs/2005.11212},
	abstract = {We present a method for unsupervised learning of equations of motion for objects in raw and optionally distorted unlabeled video. We first train an autoencoder that maps each video frame into a low-dimensional latent space where the laws of motion are as simple as possible, by minimizing a combination of non-linearity, acceleration and prediction error. Differential equations describing the motion are then discovered using Pareto-optimal symbolic regression. We find that our pre-regression ("pregression") step is able to rediscover Cartesian coordinates of unlabeled moving objects even when the video is distorted by a generalized lens. Using intuition from multidimensional knot-theory, we find that the pregression step is facilitated by first adding extra latent space dimensions to avoid topological problems during training and then removing these extra dimensions via principal component analysis.},
	urldate = {2020-06-26},
	journal = {arXiv:2005.11212 [physics, stat]},
	author = {Udrescu, Silviu-Marian and Tegmark, Max},
	month = may,
	year = {2020},
	note = {arXiv: 2005.11212},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6J8VGTDU/Udrescu and Tegmark - 2020 - Symbolic Pregression Discovering Physical Laws fr.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/NCFFWSHS/2005.html:text/html}
}

@misc{noauthor_rl_2015,
	title = {{RL} {Course} by {David} {Silver} - {Lecture} 8: {Integrating} {Learning} and {Planning}},
	shorttitle = {{RL} {Course} by {David} {Silver} - {Lecture} 8},
	url = {https://www.youtube.com/watch?v=ItMutbeOHtc&index=8&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ},
	abstract = {\#Reinforcement Learning Course by David Silver\# Lecture 8:    Integrating Learning and Planning

\#Slides and more info about the course: http://goo.gl/vUiyjq},
	urldate = {2020-06-26},
	month = may,
	year = {2015}
}

@misc{wang_wilsonwangthumbbl_2020,
	title = {{WilsonWangTHU}/mbbl},
	url = {https://github.com/WilsonWangTHU/mbbl},
	abstract = {Contribute to WilsonWangTHU/mbbl development by creating an account on GitHub.},
	urldate = {2020-06-26},
	author = {Wang, Tingwu},
	month = jun,
	year = {2020},
	note = {original-date: 2019-07-02T18:00:23Z}
}

@article{noauthor_logical_nodate,
	title = {A logical calculus of the ideas immanent in nervous activity},
	language = {en},
	pages = {19},
	file = {A logical calculus of the ideas immanent in nervou.pdf:/Users/shaan/Zotero/storage/9ZQJR53C/A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@article{mcculloch_logical_1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	volume = {5},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02478259},
	doi = {10.1007/BF02478259},
	abstract = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
	language = {en},
	number = {4},
	urldate = {2020-06-30},
	journal = {The bulletin of mathematical biophysics},
	author = {McCulloch, Warren S. and Pitts, Walter},
	month = dec,
	year = {1943},
	pages = {115--133},
	file = {Springer Full Text PDF:/Users/shaan/Zotero/storage/3LAYLVCD/McCulloch and Pitts - 1943 - A logical calculus of the ideas immanent in nervou.pdf:application/pdf}
}

@incollection{cowan_neural_1990,
	title = {Neural {Networks}: {The} {Early} {Days}},
	shorttitle = {Neural {Networks}},
	url = {http://papers.nips.cc/paper/198-neural-networks-the-early-days.pdf},
	urldate = {2020-06-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 2},
	publisher = {Morgan-Kaufmann},
	author = {Cowan, Jack D.},
	editor = {Touretzky, D. S.},
	year = {1990},
	pages = {828--842},
	file = {NIPS Full Text PDF:/Users/shaan/Zotero/storage/ULWK8HHB/Cowan - 1990 - Neural Networks The Early Days.pdf:application/pdf;NIPS Snapshot:/Users/shaan/Zotero/storage/X7FW8BBW/198-neural-networks-the-early-days.html:text/html}
}

@book{brain_perceptron_nodate,
	title = {The {Perceptron}: {A} {Probabilistic} {Model} for {Information} {Storage} and {Organization}},
	shorttitle = {The {Perceptron}},
	abstract = {If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the province of sensory physiology, and is the only one for which appreciable understanding has been achieved. This article will be concerned primarily with the second and third questions, which are still subject to a vast amount of speculation, and where the few relevant facts currently supplied by neurophysiology have not yet been integrated into an acceptable theory. With regard to the second question, two alternative positions have been maintained. The first suggests that storage of sensory information is in the form of coded representations or images, with some sort of one-to-one mapping between the sensory stimulus 1 The development of this theory has been carried out at the Cornell Aeronautical Laboratory, Inc., under the sponsorship of the Office of Naval Research, Contract Nonr-2381(00). This article is primarily'an adaptation of material reported in Ref. IS, which constitutes the first full report on the program.},
	author = {Brain, In The and Rosenblatt, F.},
	file = {Citeseer - Full Text PDF:/Users/shaan/Zotero/storage/Q9KUTN8J/Brain and Rosenblatt - The Perceptron A Probabilistic Model for Informat.pdf:application/pdf;Citeseer - Snapshot:/Users/shaan/Zotero/storage/ASH2QH2Z/summary.html:text/html}
}

@misc{noauthor_roots_nodate,
	title = {The {Roots} of {Backpropagation}: {From} {Ordered} {Derivatives} to {Neural} {Networks} and {Political} {Forecasting} {\textbar} {Wiley}},
	shorttitle = {The {Roots} of {Backpropagation}},
	url = {https://www.wiley.com/en-zm/The+Roots+of+Backpropagation%3A+From+Ordered+Derivatives+to+Neural+Networks+and+Political+Forecasting+-p-9780471598978},
	abstract = {Now, for the first time, publication of the landmark work inbackpropagation! Scientists, engineers, statisticians, operationsresearchers, and other investigators involved in neural networkshave long sought direct access to Paul Werboss groundbreaking,much-cited 1974 Harvard doctoral thesis, The Roots ofBackpropagation, which laid the foundation of backpropagation. Now,with the publication of its full text, these practitioners can gostraight to the original material and gain a deeper, practicalunderstanding of this unique mathematical approach to socialstudies and related fields. In addition, Werbos has provided threemore recent research papers, which were inspired by his originalwork, and a new guide to the field. Originally written for readerswho lacked any knowledge of neural nets, The Roots ofBackpropagation firmly established both its historical andcontinuing significance as it: * Demonstrates the ongoing value and new potential ofbackpropagation * Creates a wealth of sound mathematical tools useful acrossdisciplines * Sets the stage for the emerging area of fast automaticdifferentiation * Describes new designs for forecasting and control which exploitbackpropagation * Unifies concepts from Freud, Jung, biologists, and others into anew mathematical picture of the human mind and how it works * Certifies the viability of Deutschs model of nationalism as apredictive tool--as well as the utility of extensions of thiscentral paradigm What a delight it was to see Paul Werbos rediscover Freudsversion of back-propagation. Freud was adamant (in The Projectfor a Scientific Psychology) that selective learning could onlytake place if the presynaptic neuron was as influenced as is thepostsynaptic neuron during excitation. Such activation of bothsides of the contact barrier (Freuds name for the synapse) wasaccomplished by reducing synaptic resistance by the absorption ofenergy at the synaptic membranes. Not bad for 1895! But Werbos1993 is even better. --Karl H. Pribram Professor Emeritus,Stanford University},
	language = {en-zm},
	urldate = {2020-06-30},
	journal = {Wiley.com},
	note = {Library Catalog: www.wiley.com},
	file = {Snapshot:/Users/shaan/Zotero/storage/RZWUZVTS/The+Roots+of+Backpropagation+From+Ordered+Derivatives+to+Neural+Networks+and+Political+Forecast.html:text/html}
}

@article{verdon_quantum_2019,
	title = {Quantum {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1909.12264},
	abstract = {We introduce Quantum Graph Neural Networks (QGNN), a new class of quantum neural network ansatze which are tailored to represent quantum processes which have a graph structure, and are particularly suitable to be executed on distributed quantum systems over a quantum network. Along with this general class of ansatze, we introduce further specialized architectures, namely, Quantum Graph Recurrent Neural Networks (QGRNN) and Quantum Graph Convolutional Neural Networks (QGCNN). We provide four example applications of QGNNs: learning Hamiltonian dynamics of quantum systems, learning how to create multipartite entanglement in a quantum network, unsupervised learning for spectral clustering, and supervised learning for graph isomorphism classification.},
	urldate = {2020-06-30},
	journal = {arXiv:1909.12264 [quant-ph]},
	author = {Verdon, Guillaume and McCourt, Trevor and Luzhnica, Enxhell and Singh, Vikash and Leichenauer, Stefan and Hidary, Jack},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.12264},
	keywords = {Computer Science - Machine Learning, Quantum Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/YMIN7IKS/Verdon et al. - 2019 - Quantum Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/VBBDRDJL/1909.html:text/html}
}

@incollection{howse_gradient_1996,
	title = {Gradient and {Hamiltonian} {Dynamics} {Applied} to {Learning} in {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.pdf},
	urldate = {2020-07-01},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 8},
	publisher = {MIT Press},
	author = {Howse, James W. and Abdallah, Chaouki T. and Heileman, Gregory L.},
	editor = {Touretzky, D. S. and Mozer, M. C. and Hasselmo, M. E.},
	year = {1996},
	pages = {274--280},
	file = {NIPS Full Text PDF:/Users/shaan/Zotero/storage/EZ8MEP55/Howse et al. - 1996 - Gradient and Hamiltonian Dynamics Applied to Learn.pdf:application/pdf;NIPS Snapshot:/Users/shaan/Zotero/storage/88CBPNR6/1033-gradient-and-hamiltonian-dynamics-applied-to-learning-in-neural-networks.html:text/html}
}

@article{wiewel_latent_2020,
	title = {Latent {Space} {Subdivision}: {Stable} and {Controllable} {Time} {Predictions} for {Fluid} {Flow}},
	shorttitle = {Latent {Space} {Subdivision}},
	url = {http://arxiv.org/abs/2003.08723},
	abstract = {We propose an end-to-end trained neural networkarchitecture to robustly predict the complex dynamics of fluid flows with high temporal stability. We focus on single-phase smoke simulations in 2D and 3D based on the incompressible Navier-Stokes (NS) equations, which are relevant for a wide range of practical problems. To achieve stable predictions for long-term flow sequences, a convolutional neural network (CNN) is trained for spatial compression in combination with a temporal prediction network that consists of stacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel latent space subdivision (LSS) to separate the respective input quantities into individual parts of the encoded latent space domain. This allows to distinctively alter the encoded quantities without interfering with the remaining latent space values and hence maximizes external control. By selectively overwriting parts of the predicted latent space points, our proposed method is capable to robustly predict long-term sequences of complex physics problems. In addition, we highlight the benefits of a recurrent training on the latent space creation, which is performed by the spatial compression network.},
	urldate = {2020-07-01},
	journal = {arXiv:2003.08723 [cs, stat]},
	author = {Wiewel, Steffen and Kim, Byungsoo and Azevedo, Vinicius C. and Solenthaler, Barbara and Thuerey, Nils},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.08723},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/NU46PEL7/Wiewel et al. - 2020 - Latent Space Subdivision Stable and Controllable .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/2DLBGS4R/2003.html:text/html}
}

@article{peng_deepmimic_2018,
	title = {{DeepMimic}: {Example}-{Guided} {Deep} {Reinforcement} {Learning} of {Physics}-{Based} {Character} {Skills}},
	volume = {37},
	issn = {0730-0301, 1557-7368},
	shorttitle = {{DeepMimic}},
	url = {http://arxiv.org/abs/1804.02717},
	doi = {10.1145/3197517.3201311},
	abstract = {A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
	number = {4},
	urldate = {2020-07-01},
	journal = {ACM Transactions on Graphics},
	author = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
	month = aug,
	year = {2018},
	note = {arXiv: 1804.02717},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	pages = {1--14},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/NBZ58Y49/Peng et al. - 2018 - DeepMimic Example-Guided Deep Reinforcement Learn.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/PRH8RP2H/1804.html:text/html}
}

@article{e_integrating_2020,
	title = {Integrating {Machine} {Learning} with {Physics}-{Based} {Modeling}},
	url = {http://arxiv.org/abs/2006.02619},
	abstract = {Machine learning is poised as a very powerful tool that can drastically improve our ability to carry out scientific research. However, many issues need to be addressed before this becomes a reality. This article focuses on one particular issue of broad interest: How can we integrate machine learning with physics-based modeling to develop new interpretable and truly reliable physical models? After introducing the general guidelines, we discuss the two most important issues for developing machine learning-based physical models: Imposing physical constraints and obtaining optimal datasets. We also provide a simple and intuitive explanation for the fundamental reasons behind the success of modern machine learning, as well as an introduction to the concurrent machine learning framework needed for integrating machine learning with physics-based modeling. Molecular dynamics and moment closure of kinetic equations are used as examples to illustrate the main issues discussed. We end with a general discussion on where this integration will lead us to, and where the new frontier will be after machine learning is successfully integrated into scientific modeling.},
	urldate = {2020-07-01},
	journal = {arXiv:2006.02619 [physics]},
	author = {E, Weinan and Han, Jiequn and Zhang, Linfeng},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.02619},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6BNGHVYL/E et al. - 2020 - Integrating Machine Learning with Physics-Based Mo.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/UVJIQF9T/2006.html:text/html}
}

@article{wiewel_latent_2020-1,
	title = {Latent {Space} {Subdivision}: {Stable} and {Controllable} {Time} {Predictions} for {Fluid} {Flow}},
	shorttitle = {Latent {Space} {Subdivision}},
	url = {http://arxiv.org/abs/2003.08723},
	abstract = {We propose an end-to-end trained neural networkarchitecture to robustly predict the complex dynamics of fluid flows with high temporal stability. We focus on single-phase smoke simulations in 2D and 3D based on the incompressible Navier-Stokes (NS) equations, which are relevant for a wide range of practical problems. To achieve stable predictions for long-term flow sequences, a convolutional neural network (CNN) is trained for spatial compression in combination with a temporal prediction network that consists of stacked Long Short-Term Memory (LSTM) layers. Our core contribution is a novel latent space subdivision (LSS) to separate the respective input quantities into individual parts of the encoded latent space domain. This allows to distinctively alter the encoded quantities without interfering with the remaining latent space values and hence maximizes external control. By selectively overwriting parts of the predicted latent space points, our proposed method is capable to robustly predict long-term sequences of complex physics problems. In addition, we highlight the benefits of a recurrent training on the latent space creation, which is performed by the spatial compression network.},
	urldate = {2020-07-01},
	journal = {arXiv:2003.08723 [cs, stat]},
	author = {Wiewel, Steffen and Kim, Byungsoo and Azevedo, Vinicius C. and Solenthaler, Barbara and Thuerey, Nils},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.08723},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ZL5VCATV/Wiewel et al. - 2020 - Latent Space Subdivision Stable and Controllable .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/LCV7ZENK/2003.html:text/html}
}

@article{cherifi_overview_2020,
	title = {An overview on recent machine learning techniques for {Port} {Hamiltonian} systems},
	volume = {411},
	issn = {0167-2789},
	url = {http://www.sciencedirect.com/science/article/pii/S0167278919307705},
	doi = {10.1016/j.physd.2020.132620},
	abstract = {Port Hamiltonian systems have grown in interest in recent years due to their modular property, close relation with physical modelling and the interesting properties arising from that. In this paper, we aim at providing an overview of the application of machine learning for port Hamiltonian systems in terms of modelling and control. After an introduction to Port Hamiltonian systems framework, recent results on Hamiltonian systems modelling are presented. Some results on minimal realization and model reduction are then overviewed. Finally, the most important results on the control of Port Hamiltonian systems based machine learning are discussed including adaptive control, iterative control and reinforcement learning. The results presented in this paper are a motivation for the potential of applying machine learning methods to dynamical systems in general and port Hamiltonian systems in particular.},
	language = {en},
	urldate = {2020-07-01},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Cherifi, Karim},
	month = oct,
	year = {2020},
	keywords = {Control theory, Dynamical systems, Machine learning, Physical modelling, Port Hamiltonian systems},
	pages = {132620},
	file = {ScienceDirect Full Text PDF:/Users/shaan/Zotero/storage/AKLTPWJA/Cherifi - 2020 - An overview on recent machine learning techniques .pdf:application/pdf;ScienceDirect Snapshot:/Users/shaan/Zotero/storage/JWNSB52R/S0167278919307705.html:text/html}
}

@article{sehanobish_learning_2020,
	title = {Learning {Potentials} of {Quantum} {Systems} using {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.13297},
	abstract = {Machine Learning has wide applications in a broad range of subjects, including physics. Recent works have shown that neural networks can learn classical Hamiltonian mechanics. The results of these works motivate the following question: Can we endow neural networks with inductive biases coming from quantum mechanics and provide insights for quantum phenomena? In this work, we try answering these questions by investigating possible approximations for reconstructing the Hamiltonian of a quantum system given one of its wave--functions. Instead of handcrafting the Hamiltonian and a solution of the Schr{\textbackslash}"odinger equation, we design neural networks that aim to learn it directly from our observations. We show that our method, termed Quantum Potential Neural Networks (QPNN), can learn potentials in an unsupervised manner with remarkable accuracy for a wide range of quantum systems, such as the quantum harmonic oscillator, particle in a box perturbed by an external potential, hydrogen atom, P{\textbackslash}"oschl--Teller potential, and a solitary wave system. Furthermore, in the case of a particle perturbed by an external force, we also learn the perturbed wave function in a joint end-to-end manner.},
	urldate = {2020-07-01},
	journal = {arXiv:2006.13297 [physics, physics:quant-ph, stat]},
	author = {Sehanobish, Arijit and Corzo, Hector H. and Kara, Onur and van Dijk, David},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.13297},
	keywords = {Computer Science - Machine Learning, Quantum Physics, Statistics - Machine Learning, Physics - Computational Physics, I.2.6, J.2},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ZD72XSYK/Sehanobish et al. - 2020 - Learning Potentials of Quantum Systems using Deep .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/4PFRY6UU/2006.html:text/html}
}

@article{yang_learning_2020,
	title = {Learning {Physical} {Constraints} with {Neural} {Projections}},
	url = {http://arxiv.org/abs/2006.12745},
	abstract = {We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator liesat the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.},
	urldate = {2020-07-01},
	journal = {arXiv:2006.12745 [cs]},
	author = {Yang, Shuqi and He, Xingzhe and Zhu, Bo},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12745},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/G3ZPRYHA/Yang et al. - 2020 - Learning Physical Constraints with Neural Projecti.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HXK5FLZU/2006.html:text/html}
}

@article{dipietro_sparse_2020,
	title = {Sparse {Symplectically} {Integrated} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.12972},
	abstract = {We introduce Sparse Symplectically Integrated Neural Networks (SSINNs), a novel model for learning Hamiltonian dynamical systems from data. SSINNs combine fourth-order symplectic integration with a learned parameterization of the Hamiltonian obtained using sparse regression through a mathematically elegant function space. This allows for interpretable models that incorporate symplectic inductive biases and have low memory requirements. We evaluate SSINNs on four classical Hamiltonian dynamical problems: the H{\textbackslash}'enon-Heiles system, nonlinearly coupled oscillators, a multi-particle mass-spring system, and a pendulum system. Our results demonstrate promise in both system prediction and conservation of energy, outperforming the current state-of-the-art black-box prediction techniques by an order of magnitude. Further, SSINNs successfully converge to true governing equations from highly limited and noisy data, demonstrating potential applicability in the discovery of new physical governing equations.},
	urldate = {2020-07-01},
	journal = {arXiv:2006.12972 [physics, stat]},
	author = {DiPietro, Daniel M. and Xiong, Shiying and Zhu, Bo},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12972},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/N7CB3VV6/DiPietro et al. - 2020 - Sparse Symplectically Integrated Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8CJGZMMT/2006.html:text/html}
}

@article{santesteban_learningbased_2019,
	title = {Learning‐{Based} {Animation} of {Clothing} for {Virtual} {Try}‐{On}},
	volume = {38},
	issn = {0167-7055, 1467-8659},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.13643},
	doi = {10.1111/cgf.13643},
	abstract = {This paper presents a learning-based clothing animation method for highly efﬁcient virtual try-on simulation. Given a garment, we preprocess a rich database of physically-based dressed character simulations, for multiple body shapes and animations. Then, using this database, we train a learning-based model of cloth drape and wrinkles, as a function of body shape and dynamics. We propose a model that separates global garment ﬁt, due to body shape, from local garment wrinkles, due to both pose dynamics and body shape. We use a recurrent neural network to regress garment wrinkles, and we achieve highly plausible nonlinear effects, in contrast to the blending artifacts suffered by previous methods. At runtime, dynamic virtual try-on animations are produced in just a few milliseconds for garments with thousands of triangles. We show qualitative and quantitative analysis of results.},
	language = {en},
	number = {2},
	urldate = {2020-07-01},
	journal = {Computer Graphics Forum},
	author = {Santesteban, Igor and Otaduy, Miguel A. and Casas, Dan},
	month = may,
	year = {2019},
	pages = {355--366},
	file = {Santesteban et al. - 2019 - Learning‐Based Animation of Clothing for Virtual T.pdf:/Users/shaan/Zotero/storage/856969XC/Santesteban et al. - 2019 - Learning‐Based Animation of Clothing for Virtual T.pdf:application/pdf}
}

@article{ummenhofer_lagrangian_2020,
	title = {{LAGRANGIAN} {FLUID} {SIMULATION} {WITH} {CONTINUOUS} {CONVOLUTIONS}},
	abstract = {We present an approach to Lagrangian ﬂuid simulation with a new type of convolutional network. Our networks process sets of moving particles, which describe ﬂuids in space and time. Unlike previous approaches, we do not build an explicit graph structure to connect the particles but use spatial convolutions as the main differentiable operation that relates particles to their neighbors. To this end we present a simple, novel, and effective extension of N-D convolutions to the continuous domain. We show that our network architecture can simulate different materials, generalizes to arbitrary collision geometries, and can be used for inverse problems. In addition, we demonstrate that our continuous convolutions outperform prior formulations in terms of accuracy and speed.},
	language = {en},
	author = {Ummenhofer, Benjamin and Prantl, Lukas and Thuerey, Nils and Koltun, Vladlen},
	year = {2020},
	pages = {15},
	file = {Ummenhofer et al. - 2020 - LAGRANGIAN FLUID SIMULATION WITH CONTINUOUS CONVOL.pdf:/Users/shaan/Zotero/storage/STSWGCBV/Ummenhofer et al. - 2020 - LAGRANGIAN FLUID SIMULATION WITH CONTINUOUS CONVOL.pdf:application/pdf}
}

@article{jaques_newtonianvae_2020,
	title = {{NewtonianVAE}: {Proportional} {Control} and {Goal} {Identification} from {Pixels} via {Physical} {Latent} {Spaces}},
	shorttitle = {{NewtonianVAE}},
	url = {http://arxiv.org/abs/2006.01959},
	abstract = {Learning low-dimensional latent state space dynamics models has been a powerful paradigm for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of much simpler controllers than prior work. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration.},
	urldate = {2020-07-01},
	journal = {arXiv:2006.01959 [cs, stat]},
	author = {Jaques, Miguel and Burke, Michael and Hospedales, Timothy},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.01959},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/SKBLS7LT/Jaques et al. - 2020 - NewtonianVAE Proportional Control and Goal Identi.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/6JI65977/2006.html:text/html}
}

@article{drgona_constrained_2020,
	title = {Constrained {Physics}-{Informed} {Deep} {Learning} for {Stable} {System} {Identification} and {Control} of {Unknown} {Linear} {Systems}},
	url = {http://arxiv.org/abs/2004.11184},
	abstract = {This paper presents a novel data-driven method for learning deep constrained continuous control policies and dynamical models of linear systems. By leveraging partial knowledge of system dynamics and constraint enforcing multi-objective loss functions, the method can learn from small and static datasets, handle time-varying state and input constraints and enforce the stability properties of the controlled system. We use a continuous control design example to demonstrate the performance of the method on three distinct tasks: system identification, control policy learning, and simultaneous system identification and policy learning. We assess the system identification performance by comparing open-loop simulations of the true system and the learned models. We demonstrate the performance of the policy learning methodology in closed-loop simulations using the system model affected by varying levels of parametric and additive uncertainties. We report superior performance in terms of reference tracking, robustness, and online computational and memory footprints compared with classical control approaches, namely LQR and LQI controllers, and with three variants of model predictive control (MPC) formulations and two traditional MPC solution approaches. We then evaluate the potential of simultaneously learning the system model and control policy. Our empirical results demonstrate the effectiveness of our unifying framework for constrained optimal control of linear systems to provide stability guarantees of the learned dynamics, robustness to uncertainty, and high sampling efficiency.},
	urldate = {2020-07-01},
	journal = {arXiv:2004.11184 [cs, eess]},
	author = {Drgona, Jan and Tuor, Aaron and Vrabie, Draguna},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11184},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ISUJF8YE/Drgona et al. - 2020 - Constrained Physics-Informed Deep Learning for Sta.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/VGE2LPNR/2004.html:text/html}
}

@article{geist_learning_2020,
	title = {Learning {Constrained} {Dynamics} with {Gauss} {Principle} adhering {Gaussian} {Processes}},
	url = {http://arxiv.org/abs/2004.11238},
	abstract = {The identification of the constrained dynamics of mechanical systems is often challenging. Learning methods promise to ease an analytical analysis, but require considerable amounts of data for training. We propose to combine insights from analytical mechanics with Gaussian process regression to improve the model's data efficiency and constraint integrity. The result is a Gaussian process model that incorporates a priori constraint knowledge such that its predictions adhere to Gauss' principle of least constraint. In return, predictions of the system's acceleration naturally respect potentially non-ideal (non-)holonomic equality constraints. As corollary results, our model enables to infer the acceleration of the unconstrained system from data of the constrained system and enables knowledge transfer between differing constraint configurations.},
	urldate = {2020-07-01},
	journal = {arXiv:2004.11238 [cs, eess, stat]},
	author = {Geist, A. Rene and Trimpe, Sebastian},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.11238},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/SY7S3T5Z/Geist and Trimpe - 2020 - Learning Constrained Dynamics with Gauss Principle.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/GHCEUDN7/2004.html:text/html}
}

@article{galioto_bayesian_nodate,
	title = {Bayesian {Identiﬁcation} of {Hamiltonian} {Dynamics} from {Symplectic} {Data}},
	abstract = {We propose a Bayesian probabilistic formulation for system identiﬁcation of Hamiltonian systems. This approach uses an approximate marginal Markov Chain Monte algorithm to directly discover the system’s Hamiltonian. Our approach improves upon existing methods in two ways: ﬁrst we encode the fact that the data generating process is symplectic directly into our learning objective, and second we utilize a learning objective that simultaneously accounts for unknown parameters, model form, and measurement noise. This objective is the log marginal posterior of a probabilistic model that embeds an uncertain dynamical system within a symplectic and reversible integrator. We demonstrate that the resulting learning problem yields dynamical systems that have improved accuracy and reduced predictive uncertainty compared to existing stateof-the-art approaches. Simulation results are shown on two canonical Hamiltonian systems.},
	language = {en},
	author = {Galioto, Nicholas and Gorodetsky, Alex A},
	pages = {8},
	file = {Galioto and Gorodetsky - Bayesian Identiﬁcation of Hamiltonian Dynamics fro.pdf:/Users/shaan/Zotero/storage/YNCAN2F6/Galioto and Gorodetsky - Bayesian Identiﬁcation of Hamiltonian Dynamics fro.pdf:application/pdf}
}

@article{schmidt_distilling_2009-1,
	title = {Distilling {Free}-{Form} {Natural} {Laws} from {Experimental} {Data}},
	volume = {324},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/324/5923/81},
	doi = {10.1126/science.1165893},
	abstract = {For centuries, scientists have attempted to identify and document analytical laws that underlie physical phenomena in nature. Despite the prevalence of computing power, the process of finding natural laws and their corresponding equations has resisted automation. A key challenge to finding analytic relations automatically is defining algorithmically what makes a correlation in observed data important and insightful. We propose a principle for the identification of nontriviality. We demonstrated this approach by automatically searching motion-tracking data captured from various physical systems, ranging from simple harmonic oscillators to chaotic double-pendula. Without any prior knowledge about physics, kinematics, or geometry, the algorithm discovered Hamiltonians, Lagrangians, and other laws of geometric and momentum conservation. The discovery rate accelerated as laws found for simpler systems were used to bootstrap explanations for more complex systems, gradually uncovering the “alphabet” used to describe those systems.
An algorithm has been developed to search for natural laws of physics in large data sets.
An algorithm has been developed to search for natural laws of physics in large data sets.},
	language = {en},
	number = {5923},
	urldate = {2020-07-01},
	journal = {Science},
	author = {Schmidt, Michael and Lipson, Hod},
	month = apr,
	year = {2009},
	pmid = {19342586},
	note = {Publisher: American Association for the Advancement of Science
Section: Report},
	pages = {81--85},
	file = {Snapshot:/Users/shaan/Zotero/storage/TJLKGQVH/81.html:text/html}
}

@article{iten_discovering_2020-1,
	title = {Discovering physical concepts with neural networks},
	volume = {124},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1807.10300},
	doi = {10.1103/PhysRevLett.124.010508},
	abstract = {Despite the success of neural networks at solving concrete physics problems, their use as a general-purpose tool for scientific discovery is still in its infancy. Here, we approach this problem by modelling a neural network architecture after the human physical reasoning process, which has similarities to representation learning. This allows us to make progress towards the long-term goal of machine-assisted scientific discovery from experimental data without making prior assumptions about the system. We apply this method to toy examples and show that the network finds the physically relevant parameters, exploits conservation laws to make predictions, and can help to gain conceptual insights, e.g. Copernicus' conclusion that the solar system is heliocentric.},
	number = {1},
	urldate = {2020-07-01},
	journal = {Physical Review Letters},
	author = {Iten, Raban and Metger, Tony and Wilming, Henrik and del Rio, Lidia and Renner, Renato},
	month = jan,
	year = {2020},
	note = {arXiv: 1807.10300},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Quantum Physics},
	pages = {010508},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XZW9QES9/Iten et al. - 2020 - Discovering physical concepts with neural networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/AK9BI9EP/1807.html:text/html}
}

@article{bondesan_learning_2019-1,
	title = {Learning {Symmetries} of {Classical} {Integrable} {Systems}},
	url = {http://arxiv.org/abs/1906.04645},
	abstract = {The solution of problems in physics is often facilitated by a change of variables. In this work we present neural transformations to learn symmetries of Hamiltonian mechanical systems. Maintaining the Hamiltonian structure requires novel network architectures that parametrize symplectic transformations. We demonstrate the utility of these architectures by learning the structure of integrable models. Our work exemplifies the adaptation of neural transformations to a family constrained by more than the condition of invertibility, which we expect to be a common feature of applications of these methods.},
	urldate = {2020-07-01},
	journal = {arXiv:1906.04645 [physics]},
	author = {Bondesan, Roberto and Lamacraft, Austen},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04645},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XXEJXNVS/Bondesan and Lamacraft - 2019 - Learning Symmetries of Classical Integrable System.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/Y8UBY546/1906.html:text/html}
}

@article{rupp_fast_2012-1,
	title = {Fast and {Accurate} {Modeling} of {Molecular} {Atomization} {Energies} with {Machine} {Learning}},
	volume = {108},
	issn = {0031-9007, 1079-7114},
	url = {http://arxiv.org/abs/1109.2618},
	doi = {10.1103/PhysRevLett.108.058301},
	abstract = {We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schr{\textbackslash}"odinger equation is mapped onto a non-linear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross-validation over more than seven thousand small organic molecules yields a mean absolute error of {\textasciitilde}10 kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.},
	number = {5},
	urldate = {2020-07-01},
	journal = {Physical Review Letters},
	author = {Rupp, Matthias and Tkatchenko, Alexandre and Müller, Klaus-Robert and von Lilienfeld, O. Anatole},
	month = jan,
	year = {2012},
	note = {arXiv: 1109.2618},
	keywords = {Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Materials Science, Physics - Chemical Physics},
	pages = {058301},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/RTDLX5X3/Rupp et al. - 2012 - Fast and Accurate Modeling of Molecular Atomizatio.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/D9PE5K7A/1109.html:text/html}
}

@article{smith_ani-1_2017-1,
	title = {{ANI}-1: an extensible neural network potential with {DFT} accuracy at force field computational cost},
	volume = {8},
	issn = {2041-6539},
	shorttitle = {{ANI}-1},
	url = {https://pubs.rsc.org/en/content/articlelanding/2017/sc/c6sc05720a},
	doi = {10.1039/C6SC05720A},
	abstract = {Deep learning is revolutionizing many areas of science and technology, especially image, text, and speech recognition. In this paper, we demonstrate how a deep neural network (NN) trained on quantum mechanical (QM) DFT calculations can learn an accurate and transferable potential for organic molecules. We introduce ANAKIN-ME (Accurate NeurAl networK engINe for Molecular Energies) or ANI for short. ANI is a new method designed with the intent of developing transferable neural network potentials that utilize a highly-modified version of the Behler and Parrinello symmetry functions to build single-atom atomic environment vectors (AEV) as a molecular representation. AEVs provide the ability to train neural networks to data that spans both configurational and conformational space, a feat not previously accomplished on this scale. We utilized ANI to build a potential called ANI-1, which was trained on a subset of the GDB databases with up to 8 heavy atoms in order to predict total energies for organic molecules containing four atom types: H, C, N, and O. To obtain an accelerated but physically relevant sampling of molecular potential surfaces, we also proposed a Normal Mode Sampling (NMS) method for generating molecular conformations. Through a series of case studies, we show that ANI-1 is chemically accurate compared to reference DFT calculations on much larger molecular systems (up to 54 atoms) than those included in the training data set.},
	language = {en},
	number = {4},
	urldate = {2020-07-01},
	journal = {Chemical Science},
	author = {Smith, J. S. and Isayev, O. and Roitberg, A. E.},
	month = mar,
	year = {2017},
	note = {Publisher: The Royal Society of Chemistry},
	pages = {3192--3203},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/2MG8P8FM/Smith et al. - 2017 - ANI-1 an extensible neural network potential with.pdf:application/pdf;Snapshot:/Users/shaan/Zotero/storage/SC3MWLRC/c6sc05720a.html:text/html}
}

@article{chmiela_machine_2017-1,
	title = {Machine learning of accurate energy-conserving molecular force fields},
	volume = {3},
	issn = {2375-2548},
	doi = {10.1126/sciadv.1603015},
	abstract = {Using conservation of energy-a fundamental property of closed classical and quantum mechanical systems-we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol-1 for energies and 1 kcal mol-1 Å̊-1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.},
	language = {eng},
	number = {5},
	journal = {Science Advances},
	author = {Chmiela, Stefan and Tkatchenko, Alexandre and Sauceda, Huziel E. and Poltavsky, Igor and Schütt, Kristof T. and Müller, Klaus-Robert},
	month = may,
	year = {2017},
	pmid = {28508076},
	pmcid = {PMC5419702},
	keywords = {machine learning, energy conservation, force field, gradient field, kernel regression, molecular dynamics, path integrals, potential-energy surface},
	pages = {e1603015},
	file = {Full Text:/Users/shaan/Zotero/storage/ADK6M4YN/Chmiela et al. - 2017 - Machine learning of accurate energy-conserving mol.pdf:application/pdf}
}

@article{pukrittayakamee_simultaneous_2009-1,
	title = {Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks},
	volume = {130},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.3095491},
	doi = {10.1063/1.3095491},
	language = {en},
	number = {13},
	urldate = {2020-07-01},
	journal = {The Journal of Chemical Physics},
	author = {Pukrittayakamee, A. and Malshe, M. and Hagan, M. and Raff, L. M. and Narulkar, R. and Bukkapatnum, S. and Komanduri, R.},
	month = apr,
	year = {2009},
	pages = {134101},
	file = {Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:/Users/shaan/Zotero/storage/AJCD6W5D/Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:application/pdf}
}

@article{pukrittayakamee_simultaneous_2009-2,
	title = {Simultaneous fitting of a potential-energy surface and its corresponding force fields using feedforward neural networks},
	volume = {130},
	issn = {0021-9606, 1089-7690},
	url = {http://aip.scitation.org/doi/10.1063/1.3095491},
	doi = {10.1063/1.3095491},
	language = {en},
	number = {13},
	urldate = {2020-07-01},
	journal = {The Journal of Chemical Physics},
	author = {Pukrittayakamee, A. and Malshe, M. and Hagan, M. and Raff, L. M. and Narulkar, R. and Bukkapatnum, S. and Komanduri, R.},
	month = apr,
	year = {2009},
	pages = {134101},
	file = {Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:/Users/shaan/Zotero/storage/KD2MIYYX/Pukrittayakamee et al. - 2009 - Simultaneous fitting of a potential-energy surface.pdf:application/pdf}
}

@article{schutt_quantum-chemical_2017,
	title = {Quantum-{Chemical} {Insights} from {Deep} {Tensor} {Neural} {Networks}},
	volume = {8},
	issn = {2041-1723},
	url = {http://arxiv.org/abs/1609.08259},
	doi = {10.1038/ncomms13890},
	abstract = {Learning from data has led to paradigm shifts in a multitude of disciplines, including web, text, and image search, speech recognition, as well as bioinformatics. Can machine learning enable similar breakthroughs in understanding quantum many-body systems? Here we develop an efficient deep learning approach that enables spatially and chemically resolved insights into quantum-mechanical observables of molecular systems. We unify concepts from many-body Hamiltonians with purpose-designed deep tensor neural networks (DTNN), which leads to size-extensive and uniformly accurate (1 kcal/mol) predictions in compositional and configurational chemical space for molecules of intermediate size. As an example of chemical relevance, the DTNN model reveals a classification of aromatic rings with respect to their stability -- a useful property that is not contained as such in the training dataset. Further applications of DTNN for predicting atomic energies and local chemical potentials in molecules, reliable isomer energies, and molecules with peculiar electronic structure demonstrate the high potential of machine learning for revealing novel insights into complex quantum-chemical systems.},
	number = {1},
	urldate = {2020-07-01},
	journal = {Nature Communications},
	author = {Schütt, Kristof T. and Arbabzadah, Farhad and Chmiela, Stefan and Müller, Klaus R. and Tkatchenko, Alexandre},
	month = apr,
	year = {2017},
	note = {arXiv: 1609.08259},
	keywords = {Physics - Chemical Physics},
	pages = {13890},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/FQSPP29D/Schütt et al. - 2017 - Quantum-Chemical Insights from Deep Tensor Neural .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/39RVTYC5/1609.html:text/html}
}

@article{lutter_deep_2019,
	title = {Deep {Lagrangian} {Networks}: {Using} {Physics} as {Model} {Prior} for {Deep} {Learning}},
	shorttitle = {Deep {Lagrangian} {Networks}},
	url = {http://arxiv.org/abs/1907.04490},
	abstract = {Deep learning has achieved astonishing results on many tasks with large amounts of data and generalization within the proximity of training data. For many important real-world applications, these requirements are unfeasible and additional prior knowledge on the task domain is required to overcome the resulting problems. In particular, learning physics models for model-based control requires robust extrapolation from fewer samples - often collected online in real-time - and model errors may lead to drastic damages of the system. Directly incorporating physical insight has enabled us to obtain a novel deep model learning approach that extrapolates well while requiring fewer samples. As a first example, we propose Deep Lagrangian Networks (DeLaN) as a deep network structure upon which Lagrangian Mechanics have been imposed. DeLaN can learn the equations of motion of a mechanical system (i.e., system dynamics) with a deep network efficiently while ensuring physical plausibility. The resulting DeLaN network performs very well at robot tracking control. The proposed method did not only outperform previous model learning approaches at learning speed but exhibits substantially improved and more robust extrapolation to novel trajectories and learns online in real-time},
	urldate = {2020-07-01},
	journal = {arXiv:1907.04490 [cs, eess, stat]},
	author = {Lutter, Michael and Ritter, Christian and Peters, Jan},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.04490},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ZDRDT99Z/Lutter et al. - 2019 - Deep Lagrangian Networks Using Physics as Model P.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/PUDJ5TIF/1907.html:text/html}
}

@article{dupont_augmented_2019,
	title = {Augmented {Neural} {ODEs}},
	url = {http://arxiv.org/abs/1904.01681},
	abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
	urldate = {2020-07-01},
	journal = {arXiv:1904.01681 [cs, stat]},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	month = oct,
	year = {2019},
	note = {arXiv: 1904.01681},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/AIRFS9JZ/Dupont et al. - 2019 - Augmented Neural ODEs.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/YZI7VPI6/1904.html:text/html}
}

@article{wang_graphgan_2017,
	title = {{GraphGAN}: {Graph} {Representation} {Learning} with {Generative} {Adversarial} {Nets}},
	shorttitle = {{GraphGAN}},
	url = {http://arxiv.org/abs/1711.08267},
	abstract = {The goal of graph representation learning is to embed each vertex in a graph into a low-dimensional vector space. Existing graph representation learning methods can be classified into two categories: generative models that learn the underlying connectivity distribution in the graph, and discriminative models that predict the probability of edge existence between a pair of vertices. In this paper, we propose GraphGAN, an innovative graph representation learning framework unifying above two classes of methods, in which the generative model and discriminative model play a game-theoretical minimax game. Specifically, for a given vertex, the generative model tries to fit its underlying true connectivity distribution over all other vertices and produces "fake" samples to fool the discriminative model, while the discriminative model tries to detect whether the sampled vertex is from ground truth or generated by the generative model. With the competition between these two models, both of them can alternately and iteratively boost their performance. Moreover, when considering the implementation of generative model, we propose a novel graph softmax to overcome the limitations of traditional softmax function, which can be proven satisfying desirable properties of normalization, graph structure awareness, and computational efficiency. Through extensive experiments on real-world datasets, we demonstrate that GraphGAN achieves substantial gains in a variety of applications, including link prediction, node classification, and recommendation, over state-of-the-art baselines.},
	urldate = {2020-07-03},
	journal = {arXiv:1711.08267 [cs, stat]},
	author = {Wang, Hongwei and Wang, Jia and Wang, Jialin and Zhao, Miao and Zhang, Weinan and Zhang, Fuzheng and Xie, Xing and Guo, Minyi},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.08267},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/B2USK45E/Wang et al. - 2017 - GraphGAN Graph Representation Learning with Gener.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/6LZBMYBF/1711.html:text/html}
}

@article{yu_neural_nodate,
	title = {Neural {Control} for {Nonlinear} {Dynamic} {Systems}},
	abstract = {A neural network based approach is presented for controlling two distinct types of nonlinear systems. The first corresponds to nonlinear systems with parametric uncertainties where the parameters occur nonlinearly. The second corresponds to systems for which stabilizing control structures cannot be determined. The proposed neural controllers are shown to result in closed-loop system stability under certain conditions.},
	language = {en},
	author = {Yu, Ssu-Hsin and Annaswamy, Anuradha M},
	pages = {7},
	file = {Yu and Annaswamy - Neural Control for Nonlinear Dynamic Systems.pdf:/Users/shaan/Zotero/storage/W9GPEY5Y/Yu and Annaswamy - Neural Control for Nonlinear Dynamic Systems.pdf:application/pdf}
}

@article{celledoni_structure_2020,
	title = {Structure preserving deep learning},
	url = {http://arxiv.org/abs/2006.03364},
	abstract = {Over the past few years, deep learning has risen to the foreground as a topic of massive interest, mainly as a result of successes obtained in solving large-scale image processing tasks. There are multiple challenging mathematical problems involved in applying deep learning: most deep learning methods require the solution of hard optimisation problems, and a good understanding of the tradeoff between computational effort, amount of data and model complexity is required to successfully design a deep learning approach for a given problem. A large amount of progress made in deep learning has been based on heuristic explorations, but there is a growing effort to mathematically understand the structure in existing deep learning methods and to systematically design new deep learning methods to preserve certain types of structure in deep learning. In this article, we review a number of these directions: some deep neural networks can be understood as discretisations of dynamical systems, neural networks can be designed to have desirable properties such as invertibility or group equivariance, and new algorithmic frameworks based on conformal Hamiltonian systems and Riemannian manifolds to solve the optimisation problems have been proposed. We conclude our review of each of these topics by discussing some open problems that we consider to be interesting directions for future research.},
	urldate = {2020-07-24},
	journal = {arXiv:2006.03364 [cs, math, stat]},
	author = {Celledoni, Elena and Ehrhardt, Matthias J. and Etmann, Christian and McLachlan, Robert I. and Owren, Brynjulf and Schönlieb, Carola-Bibiane and Sherry, Ferdia},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.03364},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/J5Y627QL/Celledoni et al. - 2020 - Structure preserving deep learning.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/VIH3NQUI/2006.html:text/html}
}

@article{celledoni_energy-preserving_2016,
	title = {Energy-{Preserving} {Integrators} {Applied} to {Nonholonomic} {Systems}},
	url = {http://arxiv.org/abs/1605.02845},
	abstract = {We introduce energy-preserving integrators for nonholonomic mechanical systems. We will see that the nonholonomic dynamics is completely determined by a triple \$(\{{\textbackslash}mathcal D\}{\textasciicircum}*, {\textbackslash}Pi, {\textbackslash}mathcal\{H\})\$, where \$\{{\textbackslash}mathcal D\}{\textasciicircum}*\$ is the dual of the vector bundle determined by the nonholonomic constraints, \${\textbackslash}Pi\$ is an almost-Poisson bracket (the nonholonomic bracket) and \${\textbackslash}mathcal\{H\}:\{{\textbackslash}mathcal D\}{\textasciicircum}\{*\}{\textbackslash}rightarrow {\textbackslash}mathbb\{R\}\$ is a Hamiltonian function. For this triple, we can apply energy-preserving integrators, in particular, we show that discrete gradients can be used in the numerical integration of nonholonomic dynamics. By construction, we achieve preservation of the constraints and of the energy of the nonholonomic system. Moreover, to facilitate their applicability to complex systems which cannot be easily transformed into the aforementioned almost-Poisson form, we rewrite our integrators using just the initial information of the nonholonomic system. The derived procedures are tested on several examples: A chaotic quartic nonholonomic mechanical system, the Chaplygin sleigh system, the Suslov problem and a continuous gearbox driven by an asymmetric pendulum. Their performace is compared with other standard methods in nonholonomic dynamics, and their merits verified in practice.},
	urldate = {2020-07-24},
	journal = {arXiv:1605.02845 [math-ph]},
	author = {Celledoni, Elena and Puiggalí, Marta Farré and Høiseth, Eirik Hoel and de Diego, David Martín},
	month = may,
	year = {2016},
	note = {arXiv: 1605.02845},
	keywords = {Mathematics - Numerical Analysis, Mathematical Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/G9C8T8VF/Celledoni et al. - 2016 - Energy-Preserving Integrators Applied to Nonholono.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/WD7VUIEA/1605.html:text/html}
}

@article{chen_symplectic_2020,
	title = {Symplectic {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1909.13334},
	abstract = {We propose Symplectic Recurrent Neural Networks (SRNNs) as learning algorithms that capture the dynamics of physical systems from observed trajectories. An SRNN models the Hamiltonian function of the system by a neural network and furthermore leverages symplectic integration, multiple-step training and initial state optimization to address the challenging numerical issues associated with Hamiltonian systems. We show that SRNNs succeed reliably on complex and noisy Hamiltonian systems. We also show how to augment the SRNN integration scheme in order to handle stiff dynamical systems such as bouncing billiards.},
	urldate = {2020-09-16},
	journal = {arXiv:1909.13334 [cs, stat]},
	author = {Chen, Zhengdao and Zhang, Jianyu and Arjovsky, Martin and Bottou, Léon},
	month = apr,
	year = {2020},
	note = {arXiv: 1909.13334},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/SKDNHQZ6/Chen et al. - 2020 - Symplectic Recurrent Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9HTDCCSW/1909.html:text/html}
}

@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	shorttitle = {Physics-informed neural networks},
	url = {http://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	language = {en},
	urldate = {2020-09-16},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	month = feb,
	year = {2019},
	keywords = {Machine learning, Data-driven scientific computing, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
	file = {ScienceDirect Snapshot:/Users/shaan/Zotero/storage/BMVUERMI/S0021999118307125.html:text/html}
}

@article{zhu_deep_2020-1,
	title = {Deep {Hamiltonian} networks based on symplectic integrators},
	url = {http://arxiv.org/abs/2004.13830},
	abstract = {HNets is a class of neural networks on grounds of physical prior for learning Hamiltonian systems. This paper explains the influences of different integrators as hyper-parameters on the HNets through error analysis. If we define the network target as the map with zero empirical loss on arbitrary training data, then the non-symplectic integrators cannot guarantee the existence of the network targets of HNets. We introduce the inverse modified equations for HNets and prove that the HNets based on symplectic integrators possess network targets and the differences between the network targets and the original Hamiltonians depend on the accuracy orders of the integrators. Our numerical experiments show that the phase flows of the Hamiltonian systems obtained by symplectic HNets do not exactly preserve the original Hamiltonians, but preserve the network targets calculated; the loss of the network target for the training data and the test data is much less than the loss of the original Hamiltonian; the symplectic HNets have more powerful generalization ability and higher accuracy than the non-symplectic HNets in addressing predicting issues. Thus, the symplectic integrators are of critical importance for HNets.},
	urldate = {2020-09-16},
	journal = {arXiv:2004.13830 [cs, math]},
	author = {Zhu, Aiqing and Jin, Pengzhan and Tang, Yifa},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13830},
	keywords = {Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/8PWE3DIS/Zhu et al. - 2020 - Deep Hamiltonian networks based on symplectic inte.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/77G6GZ2B/2004.html:text/html}
}

@article{yang_learning_2020-1,
	title = {Learning {Physical} {Constraints} with {Neural} {Projections}},
	url = {http://arxiv.org/abs/2006.12745},
	abstract = {We propose a new family of neural networks to predict the behaviors of physical systems by learning their underpinning constraints. A neural projection operator liesat the heart of our approach, composed of a lightweight network with an embedded recursive architecture that interactively enforces learned underpinning constraints and predicts the various governed behaviors of different physical systems. Our neural projection operator is motivated by the position-based dynamics model that has been used widely in game and visual effects industries to unify the various fast physics simulators. Our method can automatically and effectively uncover a broad range of constraints from observation point data, such as length, angle, bending, collision, boundary effects, and their arbitrary combinations, without any connectivity priors. We provide a multi-group point representation in conjunction with a configurable network connection mechanism to incorporate prior inputs for processing complex physical systems. We demonstrated the efficacy of our approach by learning a set of challenging physical systems all in a unified and simple fashion including: rigid bodies with complex geometries, ropes with varying length and bending, articulated soft and rigid bodies, and multi-object collisions with complex boundaries.},
	urldate = {2020-09-16},
	journal = {arXiv:2006.12745 [cs]},
	author = {Yang, Shuqi and He, Xingzhe and Zhu, Bo},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12745},
	keywords = {Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/834GQ2UD/Yang et al. - 2020 - Learning Physical Constraints with Neural Projecti.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/MLHYE43N/2006.html:text/html}
}

@article{um_solver---loop_2020,
	title = {Solver-in-the-{Loop}: {Learning} from {Differentiable} {Physics} to {Interact} with {Iterative} {PDE}-{Solvers}},
	shorttitle = {Solver-in-the-{Loop}},
	url = {http://arxiv.org/abs/2007.00016},
	abstract = {Finding accurate solutions to partial differential equations (PDEs) is a crucial task in all scientific and engineering disciplines. It has recently been shown that machine learning methods can improve the solution accuracy by correcting for effects not captured by the discretized PDE. We target the problem of reducing numerical errors of iterative PDE solvers and compare different learning approaches for finding complex correction functions. We find that previously used learning approaches are significantly outperformed by methods that integrate the solver into the training loop and thereby allow the model to interact with the PDE during training. This provides the model with realistic input distributions that take previous corrections into account, yielding improvements in accuracy with stable rollouts of several hundred recurrent evaluation steps and surpassing even tailored supervised variants. We highlight the performance of the differentiable physics networks for a wide variety of PDEs, from non-linear advection-diffusion systems to three-dimensional Navier-Stokes flows.},
	urldate = {2020-09-16},
	journal = {arXiv:2007.00016 [physics]},
	author = {Um, Kiwon and Raymond and Fei and Holl, Philipp and Brand, Robert and Thuerey, Nils},
	month = jun,
	year = {2020},
	note = {arXiv: 2007.00016},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/52FN5EH2/Um et al. - 2020 - Solver-in-the-Loop Learning from Differentiable P.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/M66594LP/2007.html:text/html}
}

@article{arik_interpretable_2020,
	title = {Interpretable {Sequence} {Learning} for {COVID}-19 {Forecasting}},
	url = {http://arxiv.org/abs/2008.00646},
	abstract = {We propose a novel approach that integrates machine learning into compartmental disease modeling to predict the progression of COVID-19. Our model is explainable by design as it explicitly shows how different compartments evolve and it uses interpretable encoders to incorporate covariates and improve performance. Explainability is valuable to ensure that the model's forecasts are credible to epidemiologists and to instill confidence in end-users such as policy makers and healthcare institutions. Our model can be applied at different geographic resolutions, and here we demonstrate it for states and counties in the United States. We show that our model provides more accurate forecasts, in metrics averaged across the entire US, than state-of-the-art alternatives, and that it provides qualitatively meaningful explanatory insights. Lastly, we analyze the performance of our model for different subgroups based on the subgroup distributions within the counties.},
	urldate = {2020-09-16},
	journal = {arXiv:2008.00646 [cs, stat]},
	author = {Arik, Sercan O. and Li, Chun-Liang and Yoon, Jinsung and Sinha, Rajarishi and Epshteyn, Arkady and Le, Long T. and Menon, Vikas and Singh, Shashank and Zhang, Leyou and Yoder, Nate and Nikoltchev, Martin and Sonthalia, Yash and Nakhost, Hootan and Kanal, Elli and Pfister, Tomas},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.00646},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/MEKZSKEQ/Arik et al. - 2020 - Interpretable Sequence Learning for COVID-19 Forec.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/YA2J7IBP/2008.html:text/html}
}

@article{seo_physics-aware_2020-1,
	title = {Physics-aware {Spatiotemporal} {Modules} with {Auxiliary} {Tasks} for {Meta}-{Learning}},
	url = {http://arxiv.org/abs/2006.08831},
	abstract = {Modeling the dynamics of real-world physical systems is critical for spatiotemporal prediction tasks, but challenging when data is limited. The scarcity of real-world data and the difficulty in reproducing the data distribution hinder directly applying meta-learning techniques. Although the knowledge of governing partial differential equations (PDEs) of the data can be helpful for the fast adaptation to few observations, it is difficult to generalize to different or unknown dynamics. In this paper, we propose a framework, physics-aware modular meta-learning with auxiliary tasks (PiMetaL) whose spatial modules incorporate PDE-independent knowledge and temporal modules are rapidly adaptable to the limited data, respectively. The framework does not require the exact form of governing equations to model the observed spatiotemporal data. Furthermore, it mitigates the need for a large number of real-world tasks for meta-learning by leveraging simulated data. We apply the proposed framework to both synthetic and real-world spatiotemporal prediction tasks and demonstrate its superior performance with limited observations.},
	urldate = {2020-09-16},
	journal = {arXiv:2006.08831 [cs, stat]},
	author = {Seo, Sungyong and Meng, Chuizheng and Rambhatla, Sirisha and Liu, Yan},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08831},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/L6LTDRNC/Seo et al. - 2020 - Physics-aware Spatiotemporal Modules with Auxiliar.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/6V5V8B4F/2006.html:text/html}
}

@article{zhong_unsupervised_2020,
	title = {Unsupervised {Learning} of {Lagrangian} {Dynamics} from {Images} for {Prediction} and {Control}},
	url = {http://arxiv.org/abs/2007.01926},
	abstract = {Recent approaches for modelling dynamics of physical systems with neural networks enforce Lagrangian or Hamiltonian structure to improve prediction and generalization. However, these approaches fail to handle the case when coordinates are embedded in high-dimensional data such as images. We introduce a new unsupervised neural network model that learns Lagrangian dynamics from images, with interpretability that benefits prediction and control. The model infers Lagrangian dynamics on generalized coordinates that are simultaneously learned with a coordinate-aware variational autoencoder (VAE). The VAE is designed to account for the geometry of physical systems composed of multiple rigid bodies in the plane. By inferring interpretable Lagrangian dynamics, the model learns physical system properties, such as kinetic and potential energy, which enables long-term prediction of dynamics in the image space and synthesis of energy-based controllers.},
	urldate = {2020-09-16},
	journal = {arXiv:2007.01926 [cs, eess, stat]},
	author = {Zhong, Yaofeng Desmond and Leonard, Naomi Ehrich},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01926},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/ET92MQTQ/Zhong and Leonard - 2020 - Unsupervised Learning of Lagrangian Dynamics from .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/34MGIA8A/2007.html:text/html}
}

@article{atkinson_data-driven_2019,
	title = {Data-driven discovery of free-form governing differential equations},
	url = {http://arxiv.org/abs/1910.05117},
	abstract = {We present a method of discovering governing differential equations from data without the need to specify a priori the terms to appear in the equation. The input to our method is a dataset (or ensemble of datasets) corresponding to a particular solution (or ensemble of particular solutions) of a differential equation. The output is a human-readable differential equation with parameters calibrated to the individual particular solutions provided. The key to our method is to learn differentiable models of the data that subsequently serve as inputs to a genetic programming algorithm in which graphs specify computation over arbitrary compositions of functions, parameters, and (potentially differential) operators on functions. Differential operators are composed and evaluated using recursive application of automatic differentiation, allowing our algorithm to explore arbitrary compositions of operators without the need for human intervention. We also demonstrate an active learning process to identify and remedy deficiencies in the proposed governing equations.},
	urldate = {2020-09-19},
	journal = {arXiv:1910.05117 [physics, stat]},
	author = {Atkinson, Steven and Subber, Waad and Wang, Liping and Khan, Genghis and Hawi, Philippe and Ghanem, Roger},
	month = nov,
	year = {2019},
	note = {arXiv: 1910.05117},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics, Computer Science - Computational Engineering, Finance, and Science},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/NP9TLY7X/Atkinson et al. - 2019 - Data-driven discovery of free-form governing diffe.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/GKBIWDGM/1910.html:text/html}
}

@article{guillemin_symplectic_nodate,
	title = {Symplectic techniques in physics},
	language = {en},
	author = {Guillemin, Victor and Sternberg, Shlomo},
	pages = {4},
	file = {Guillemin and Sternberg - Symplectic techniques in physics.pdf:/Users/shaan/Zotero/storage/78RQ4W8T/Guillemin and Sternberg - Symplectic techniques in physics.pdf:application/pdf}
}

@article{rath_symplectic_2020,
	title = {Symplectic {Gaussian} {Process} {Regression} of {Hamiltonian} {Flow} {Maps}},
	url = {http://arxiv.org/abs/2009.05569},
	abstract = {We present an approach to construct appropriate and efficient emulators for Hamiltonian flow maps. Intended future applications are long-term tracing of fast charged particles in accelerators and magnetic plasma confinement configurations. The method is based on multi-output Gaussian process regression on scattered training data. To obtain long-term stability the symplectic property is enforced via the choice of the matrix-valued covariance function. Based on earlier work on spline interpolation we observe derivatives of the generating function of a canonical transformation. A product kernel produces an accurate implicit method, whereas a sum kernel results in a fast explicit method from this approach. Both correspond to a symplectic Euler method in terms of numerical integration. These methods are applied to the pendulum and the H{\textbackslash}'enon-Heiles system and results compared to an symmetric regression with orthogonal polynomials. In the limit of small mapping times, the Hamiltonian function can be identified with a part of the generating function and thereby learned from observed time-series data of the system's evolution. Besides comparable performance of implicit kernel and spectral regression for symplectic maps, we demonstrate a substantial increase in performance for learning the Hamiltonian function compared to existing approaches.},
	urldate = {2020-09-19},
	journal = {arXiv:2009.05569 [physics, stat]},
	author = {Rath, Katharina and Albert, Christopher G. and Bischl, Bernd and von Toussaint, Udo},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.05569},
	keywords = {Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/327UNYEI/Rath et al. - 2020 - Symplectic Gaussian Process Regression of Hamilton.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/9S645P2D/2009.html:text/html}
}

@article{jin_sympnets_2020,
	title = {{SympNets}: {Intrinsic} structure-preserving symplectic networks for identifying {Hamiltonian} systems},
	volume = {132},
	issn = {0893-6080},
	shorttitle = {{SympNets}},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608020303063},
	doi = {10.1016/j.neunet.2020.08.017},
	abstract = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions. We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models, and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
	language = {en},
	urldate = {2020-09-19},
	journal = {Neural Networks},
	author = {Jin, Pengzhan and Zhang, Zhen and Zhu, Aiqing and Tang, Yifa and Karniadakis, George Em},
	month = dec,
	year = {2020},
	keywords = {Hamiltonian systems, Dynamical systems, Deep learning, Physics-informed, Symplectic integrators, Symplectic maps},
	pages = {166--179},
	file = {ScienceDirect Snapshot:/Users/shaan/Zotero/storage/XDJGJZZ9/S0893608020303063.html:text/html;Submitted Version:/Users/shaan/Zotero/storage/MBG2R94D/Jin et al. - 2020 - SympNets Intrinsic structure-preserving symplecti.pdf:application/pdf}
}

@article{miller_mastering_2020,
	title = {Mastering high-dimensional dynamics with {Hamiltonian} neural networks},
	url = {http://arxiv.org/abs/2008.04214},
	abstract = {We detail how incorporating physics into neural network design can significantly improve the learning and forecasting of dynamical systems, even nonlinear systems of many dimensions. A map building perspective elucidates the superiority of Hamiltonian neural networks over conventional neural networks. The results clarify the critical relation between data, dimension, and neural network learning performance.},
	urldate = {2020-09-20},
	journal = {arXiv:2008.04214 [nlin]},
	author = {Miller, Scott T. and Lindner, John F. and Choudhary, Anshul and Sinha, Sudeshna and Ditto, William L.},
	month = jul,
	year = {2020},
	note = {arXiv: 2008.04214},
	keywords = {Computer Science - Neural and Evolutionary Computing, Nonlinear Sciences - Chaotic Dynamics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XLPU23HB/Miller et al. - 2020 - Mastering high-dimensional dynamics with Hamiltoni.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/YALXGGQH/2008.html:text/html}
}

@article{atkinson_bayesian_2020-1,
	title = {Bayesian {Hidden} {Physics} {Models}: {Uncertainty} {Quantification} for {Discovery} of {Nonlinear} {Partial} {Differential} {Operators} from {Data}},
	shorttitle = {Bayesian {Hidden} {Physics} {Models}},
	url = {http://arxiv.org/abs/2006.04228},
	abstract = {What do data tell us about physics-and what don't they tell us? There has been a surge of interest in using machine learning models to discover governing physical laws such as differential equations from data, but current methods lack uncertainty quantification to communicate their credibility. This work addresses this shortcoming from a Bayesian perspective. We introduce a novel model comprising "leaf" modules that learn to represent distinct experiments' spatiotemporal functional data as neural networks and a single "root" module that expresses a nonparametric distribution over their governing nonlinear differential operator as a Gaussian process. Automatic differentiation is used to compute the required partial derivatives from the leaf functions as inputs to the root. Our approach quantifies the reliability of the learned physics in terms of a posterior distribution over operators and propagates this uncertainty to solutions of novel initial-boundary value problem instances. Numerical experiments demonstrate the method on several nonlinear PDEs.},
	urldate = {2020-09-20},
	journal = {arXiv:2006.04228 [cs, stat]},
	author = {Atkinson, Steven},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04228},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/Users/shaan/Zotero/storage/I8UNW3B6/2006.html:text/html}
}

@article{raissi_deep_nodate,
	title = {Deep {Hidden} {Physics} {Models}: {Deep} {Learning} of {Nonlinear} {Partial} {Diﬀerential} {Equations}},
	abstract = {We put forth a deep learning approach for discovering nonlinear partial diﬀerential equations from scattered and potentially noisy observations in space and time. Speciﬁcally, we approximate the unknown solution as well as the nonlinear dynamics by two deep neural networks. The ﬁrst network acts as a prior on the unknown solution and essentially enables us to avoid numerical diﬀerentiations which are inherently ill-conditioned and unstable. The second network represents the nonlinear dynamics and helps us distill the mechanisms that govern the evolution of a given spatiotemporal data-set. We test the eﬀectiveness of our approach for several benchmark problems spanning a number of scientiﬁc domains and demonstrate how the proposed framework can help us accurately learn the underlying dynamics and forecast future states of the system. In particular, we study the Burgers’, Korteweg-de Vries (KdV), Kuramoto-Sivashinsky, nonlinear Schr¨odinger, and Navier-Stokes equations.},
	language = {en},
	author = {Raissi, Maziar},
	pages = {24},
	file = {Raissi - Deep Hidden Physics Models Deep Learning of Nonli.pdf:/Users/shaan/Zotero/storage/PABLGZIX/Raissi - Deep Hidden Physics Models Deep Learning of Nonli.pdf:application/pdf}
}

@article{masi_thermodynamics-based_2020,
	title = {Thermodynamics-based {Artificial} {Neural} {Networks} for constitutive modeling},
	url = {http://arxiv.org/abs/2005.12183},
	abstract = {Machine Learning methods and, in particular, Artificial Neural Networks (ANNs) have demonstrated promising capabilities in material constitutive modeling. One of the main drawbacks of such approaches is the lack of a rigorous frame based on the laws of physics. This may render physically inconsistent the predictions of a trained network, which can be even dangerous for real applications. Here we propose a new class of data-driven, physics-based, neural networks for constitutive modeling of strain rate independent processes at the material point level, which we define as Thermodynamics-based Artificial Neural Networks (TANNs). The two basic principles of thermodynamics are encoded in the network's architecture by taking advantage of automatic differentiation to compute the numerical derivatives of a network with respect to its inputs. In this way, derivatives of the free-energy, the dissipation rate and their relation with the stress and internal state variables are hardwired in the network. Consequently, our network does not have to identify the underlying pattern of thermodynamic laws during training, reducing the need of large data-sets. Moreover the training is more efficient and robust, and the predictions more accurate. Finally and more important, the predictions remain thermodynamically consistent, even for unseen data. Based on these features, TANNs are a starting point for data-driven, physics-based constitutive modeling with neural networks. We demonstrate the wide applicability of TANNs for modeling elasto-plastic materials, with strain hardening and strain softening. Detailed comparisons show that the predictions of TANNs outperform those of standard ANNs. TANNs ' architecture is general, enabling applications to materials with different or more complex behavior, without any modification.},
	urldate = {2020-09-20},
	journal = {arXiv:2005.12183 [physics, stat]},
	author = {Masi, Filippo and Stefanou, Ioannis and Vannucci, Paolo and Maffi-Berthier, Victor},
	month = may,
	year = {2020},
	note = {arXiv: 2005.12183},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/FZ5RW3QV/Masi et al. - 2020 - Thermodynamics-based Artificial Neural Networks fo.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/MZ5IHJUF/2005.html:text/html}
}

@article{zheng_dags_2018,
	title = {{DAGs} with {NO} {TEARS}: {Continuous} {Optimization} for {Structure} {Learning}},
	shorttitle = {{DAGs} with {NO} {TEARS}},
	url = {http://arxiv.org/abs/1803.01422},
	abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely {\textbackslash}emph\{continuous\} optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.},
	urldate = {2020-09-20},
	journal = {arXiv:1803.01422 [cs, stat]},
	author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
	month = nov,
	year = {2018},
	note = {arXiv: 1803.01422},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/7YDNWPY8/Zheng et al. - 2018 - DAGs with NO TEARS Continuous Optimization for St.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/93YVU4HS/1803.html:text/html}
}

@article{pamfil_dynotears_2020,
	title = {{DYNOTEARS}: {Structure} {Learning} from {Time}-{Series} {Data}},
	shorttitle = {{DYNOTEARS}},
	url = {http://arxiv.org/abs/2002.00498},
	abstract = {We revisit the structure learning problem for dynamic Bayesian networks and propose a method that simultaneously estimates contemporaneous (intra-slice) and time-lagged (inter-slice) relationships between variables in a time-series. Our approach is score-based, and revolves around minimizing a penalized loss subject to an acyclicity constraint. To solve this problem, we leverage a recent algebraic result characterizing the acyclicity constraint as a smooth equality constraint. The resulting algorithm, which we call DYNOTEARS, outperforms other methods on simulated data, especially in high-dimensions as the number of variables increases. We also apply this algorithm on real datasets from two different domains, finance and molecular biology, and analyze the resulting output. Compared to state-of-the-art methods for learning dynamic Bayesian networks, our method is both scalable and accurate on real data. The simple formulation and competitive performance of our method make it suitable for a variety of problems where one seeks to learn connections between variables across time.},
	urldate = {2020-09-20},
	journal = {arXiv:2002.00498 [cs, stat]},
	author = {Pamfil, Roxana and Sriwattanaworachai, Nisara and Desai, Shaan and Pilgerstorfer, Philip and Beaumont, Paul and Georgatzis, Konstantinos and Aragam, Bryon},
	month = apr,
	year = {2020},
	note = {arXiv: 2002.00498},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/T7CI22CM/Pamfil et al. - 2020 - DYNOTEARS Structure Learning from Time-Series Dat.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/AH9SCXZX/2002.html:text/html}
}

@article{lachapelle_gradient-based_2020,
	title = {{GRADIENT}-{BASED} {NEURAL} {DAG} {LEARNING}},
	abstract = {We propose a novel score-based approach to learning a directed acyclic graph (DAG) from observational data. We adapt a recently proposed continuous constrained optimization formulation to allow for nonlinear relationships between variables using neural networks. This extension allows to model complex interactions while avoiding the combinatorial nature of the problem. In addition to comparing our method to existing continuous optimization methods, we provide missing empirical comparisons to nonlinear greedy search methods. On both synthetic and real-world data sets, this new method outperforms current continuous methods on most tasks, while being competitive with existing greedy search methods on important metrics for causal inference.},
	language = {en},
	author = {Lachapelle, Sebastien and Brouillard, Philippe and Deleu, Tristan and Lacoste-Julien, Simon},
	year = {2020},
	pages = {23},
	file = {Lachapelle et al. - 2020 - GRADIENT-BASED NEURAL DAG LEARNING.pdf:/Users/shaan/Zotero/storage/AKJVLKMY/Lachapelle et al. - 2020 - GRADIENT-BASED NEURAL DAG LEARNING.pdf:application/pdf}
}

@article{yu_dag-gnn_nodate,
	title = {{DAG}-{GNN}: {DAG} {Structure} {Learning} with {Graph} {Neural} {Networks}},
	abstract = {Learning a faithful directed acyclic graph (DAG) from samples of a joint distribution is a challenging combinatorial problem, owing to the intractable search space superexponential in the number of graph nodes. A recent breakthrough formulates the problem as a continuous optimization with a structural constraint that ensures acyclicity (Zheng et al., 2018). The authors apply the approach to the linear structural equation model (SEM) and the least-squares loss function that are statistically well justiﬁed but nevertheless limited. Motivated by the widespread success of deep learning that is capable of capturing complex nonlinear mappings, in this work we propose a deep generative model and apply a variant of the structural constraint to learn the DAG. At the heart of the generative model is a variational autoencoder parameterized by a novel graph neural network architecture, which we coin DAG-GNN. In addition to the richer capacity, an advantage of the proposed model is that it naturally handles discrete variables as well as vector-valued ones. We demonstrate that on synthetic data sets, the proposed method learns more accurate graphs for nonlinearly generated samples; and on benchmark data sets with discrete variables, the learned graphs are reasonably close to the global optima. The code is available at https:// github.com/fishmoon1234/DAG-GNN.},
	language = {en},
	author = {Yu, Yue and Chen, Jie and Gao, Tian and Yu, Mo},
	pages = {10},
	file = {Yu et al. - DAG-GNN DAG Structure Learning with Graph Neural .pdf:/Users/shaan/Zotero/storage/BTSZWGUF/Yu et al. - DAG-GNN DAG Structure Learning with Graph Neural .pdf:application/pdf}
}

@article{brunton_discovering_2016-1,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1517384113},
	doi = {10.1073/pnas.1517384113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	language = {en},
	number = {15},
	urldate = {2020-09-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
	file = {Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf:/Users/shaan/Zotero/storage/BP4DFZQI/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf:application/pdf}
}

@article{he_physics-informed_2020,
	title = {Physics-informed neural networks for multiphysics data assimilation with application to subsurface transport},
	volume = {141},
	issn = {0309-1708},
	url = {http://www.sciencedirect.com/science/article/pii/S0309170819311649},
	doi = {10.1016/j.advwatres.2020.103610},
	abstract = {Data assimilation for parameter and state estimation in subsurface transport problems remains a significant challenge because of the sparsity of measurements, the heterogeneity of porous media, and the high computational cost of forward numerical models. We present a multiphysics-informed deep neural network machine learning method for estimating space-dependent hydraulic conductivity, hydraulic head, and concentration fields from sparse measurements. In this approach, we employ individual deep neural networks (DNNs) to approximate the unknown parameters (e.g., hydraulic conductivity) and states (e.g., hydraulic head and concentration) of a physical system. Next, we jointly train these DNNs by minimizing the loss function that consists of the governing equations residuals in addition to the error with respect to measurement data. We apply this approach to assimilate conductivity, hydraulic head, and concentration measurements for the joint inversion of these parameter and states in a steady-state advection–dispersion problem. We study the accuracy of the proposed data assimilation approach with respect to the data size (i.e., the number of measured variables and the number of measurements of each variable), DNN size, and the complexity of the parameter field. We demonstrate that the physics-informed DNNs are significantly more accurate than the standard data-driven DNNs, especially when the training set consists of sparse data. We also show that the accuracy of parameter estimation increases as more different multiphysics variables are inverted jointly.},
	language = {en},
	urldate = {2020-09-21},
	journal = {Advances in Water Resources},
	author = {He, QiZhi and Barajas-Solano, David and Tartakovsky, Guzel and Tartakovsky, Alexandre M.},
	month = jul,
	year = {2020},
	keywords = {Data assimilation, Inverse problems, Parameter estimation, Physics-informed deep neural networks, Subsurface flow and transport},
	pages = {103610},
	file = {ScienceDirect Snapshot:/Users/shaan/Zotero/storage/3GPEUU9N/S0309170819311649.html:text/html;Submitted Version:/Users/shaan/Zotero/storage/GV3Q92ZK/He et al. - 2020 - Physics-informed neural networks for multiphysics .pdf:application/pdf}
}

@article{kadeethum_physics-informed_2020,
	title = {Physics-informed neural networks for solving nonlinear diffusivity and {Biot}’s equations},
	volume = {15},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0232683},
	doi = {10.1371/journal.pone.0232683},
	abstract = {This paper presents the potential of applying physics-informed neural networks for solving nonlinear multiphysics problems, which are essential to many fields such as biomedical engineering, earthquake prediction, and underground energy harvesting. Specifically, we investigate how to extend the methodology of physics-informed neural networks to solve both the forward and inverse problems in relation to the nonlinear diffusivity and Biot’s equations. We explore the accuracy of the physics-informed neural networks with different training example sizes and choices of hyperparameters. The impacts of the stochastic variations between various training realizations are also investigated. In the inverse case, we also study the effects of noisy measurements. Furthermore, we address the challenge of selecting the hyperparameters of the inverse model and illustrate how this challenge is linked to the hyperparameters selection performed for the forward one.},
	language = {en},
	number = {5},
	urldate = {2020-09-21},
	journal = {PLOS ONE},
	author = {Kadeethum, Teeratorn and Jørgensen, Thomas M. and Nick, Hamidreza M.},
	month = may,
	year = {2020},
	note = {Publisher: Public Library of Science},
	keywords = {Behavior, Deformation, Fluid flow, Momentum, Neural networks, Neurons, Optimization, Partial differential equations},
	pages = {e0232683},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/WDFS5EUU/Kadeethum et al. - 2020 - Physics-informed neural networks for solving nonli.pdf:application/pdf}
}

@article{sahli_costabal_physics-informed_2020,
	title = {Physics-{Informed} {Neural} {Networks} for {Cardiac} {Activation} {Mapping}},
	volume = {8},
	issn = {2296-424X},
	url = {https://www.frontiersin.org/articles/10.3389/fphy.2020.00042/full},
	doi = {10.3389/fphy.2020.00042},
	abstract = {A critical procedure in diagnosing atrial fibrillation is the creation of electro-anatomic activation maps. Current methods generate these mappings from interpolation using a few sparse data points recorded inside the atria; they neither include prior knowledge of the underlying physics nor uncertainty of these recordings. Here we propose a physics-informed neural network for cardiac activation mapping that accounts for the underlying wave propagation dynamics and we quantify the epistemic uncertainty associated with these predictions. These uncertainty estimates not only allow us to quantify the predictive error of the neural network, but also help to reduce it by judiciously selecting new informative measurement locations via active learning. We illustrate the potential of our approach using a synthetic benchmark problem and a personalized electrophysiology model of the left atrium. We show that our new method outperforms linear interpolation and Gaussian process regression for the benchmark problem and linear interpolation at clinical densities for the left atrium. In both cases, the active learning algorithm achieves lower error levels than random allocation. Our findings open the door towards physics-based electro-anatomic mapping with the ultimate goals to reduce procedural time and improve diagnostic predictability for patients affected by atrial fibrillation. Open source code is available at https://github.com/fsahli/EikonalNet.},
	language = {English},
	urldate = {2020-09-21},
	journal = {Frontiers in Physics},
	author = {Sahli Costabal, Francisco and Yang, Yibo and Perdikaris, Paris and Hurtado, Daniel E. and Kuhl, Ellen},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Active Learning, Atrial Fibrillation, Cardiac electrophysiogy, Eikonal Equation, Electro-anatomic mapping, machine   learning, Physics-informed neural networks, uncertainty quantification},
	file = {Full Text PDF:/Users/shaan/Zotero/storage/2KV6R98I/Sahli Costabal et al. - 2020 - Physics-Informed Neural Networks for Cardiac Activ.pdf:application/pdf}
}

@article{wang_machine_2019-1,
	title = {Machine learning magnetic parameters from spin configurations},
	url = {http://arxiv.org/abs/1908.05829},
	abstract = {Hamiltonian parameter estimation is crucial in condensed matter physics, but time and cost consuming in terms of resources used. With advances in observation techniques, high-resolution images with more detailed information are obtained, which can serve as an input to machine learning (ML) algorithms to extract Hamiltonian parameters. However, the number of labeled images is rather limited. Here, we provide a protocol for Hamiltonian parameter estimation based on a machine learning architecture, which is trained on a small amount of simulated images and applied to experimental spin configuration images. Sliding windows on the input images enlarges the number of training images; therefore we can train well a neural network on a small dataset of simulated images which are generated adaptively using the same external conditions such as temperature and magnetic field as the experiment. The neural network is applied to the experimental image and estimates magnetic parameters efficiently. We demonstrate the success of the estimation by reproducing the same configuration from simulation and predict a hysteresis loop accurately. Our approach paves a way to a stable and general parameter estimation.},
	urldate = {2020-09-21},
	journal = {arXiv:1908.05829 [cond-mat, physics:physics]},
	author = {Wang, Dingchen and Wei, Songrui and Yuan, Anran and Tian, Fanghua and Cao, Kaiyan and Zhao, Qizhong and Xue, Dezhen and Yang, Sen},
	month = nov,
	year = {2019},
	note = {arXiv: 1908.05829},
	keywords = {Physics - Computational Physics, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Materials Science},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/V4AFV5GX/Wang et al. - 2019 - Machine learning magnetic parameters from spin con.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/8X42HS8R/1908.html:text/html}
}

@article{yang_enforcing_2019-1,
	title = {Enforcing {Deterministic} {Constraints} on {Generative} {Adversarial} {Networks} for {Emulating} {Physical} {Systems}},
	url = {http://arxiv.org/abs/1911.06671},
	abstract = {Generative adversarial networks (GANs) are initially proposed to generate images by learning from a large number of samples. Recently, GANs have been used to emulate complex physical systems such as turbulent flows. However, a critical question must be answered before GANs can be considered trusted emulators for physical systems: do GANs-generated samples conform to the various physical constraints? These include both deterministic constraints (e.g., conservation laws) and statistical constraints (e.g., energy spectrum in turbulent flows). The latter has been studied in a companion paper (Wu et al. 2019. Enforcing statistical constraints in generative adversarial networks for modeling chaotic dynamical systems. arxiv:1905.06841). In the present work, we enforce deterministic yet approximate constraints on GANs by incorporating them into the loss function of the generator. We evaluate the performance of physics-constrained GANs on two representative tasks with geometrical constraints (generating points on circles) and differential constraints (generating divergence-free flow velocity fields), respectively. In both cases, the constrained GANs produced samples that precisely conform to the underlying constraints, even though the constraints are only enforced approximately. More importantly, the imposed constraints significantly accelerate the convergence and improve the robustness in the training. These improvements are noteworthy, as the convergence and robustness are two well-known obstacles in the training of GANs.},
	urldate = {2020-09-22},
	journal = {arXiv:1911.06671 [physics, stat]},
	author = {Yang, Zeng and Wu, Jin-Long and Xiao, Heng},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.06671},
	keywords = {Statistics - Machine Learning, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/UG95C579/Yang et al. - 2019 - Enforcing Deterministic Constraints on Generative .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/KIXCNULZ/1911.html:text/html}
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-09-22},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/XTMP9G7Z/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/7AURH3RL/1810.html:text/html}
}

@article{he_mask_2018,
	title = {Mask {R}-{CNN}},
	url = {http://arxiv.org/abs/1703.06870},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron},
	urldate = {2020-09-22},
	journal = {arXiv:1703.06870 [cs]},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	month = jan,
	year = {2018},
	note = {arXiv: 1703.06870},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6EPHB6LV/He et al. - 2018 - Mask R-CNN.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/2UWJKLSJ/1703.html:text/html}
}

@inproceedings{palm_warren_1986,
	address = {Berlin, Heidelberg},
	title = {Warren {McCulloch} and {Walter} {Pitts}: {A} {Logical} {Calculus} of the {Ideas} {Immanent} in {Nervous} {Activity}},
	isbn = {978-3-642-70911-1},
	shorttitle = {Warren {McCulloch} and {Walter} {Pitts}},
	doi = {10.1007/978-3-642-70911-1_14},
	abstract = {The classical paper by McCulloch and Pitts on “a logical calculus of the ideas immanent in nervous activity” had an enormous impact on the development of brain theory in the broadest sense. It appeared in 1943 and was the starting point for many theoretical investigations up to the present day: its basic idea was that the activation of a neuron inside a brain stands for the actual truth of a proposition about the outside world. Elementary propositions about the outside world are verified through sensors. The neurons to which these sensors are connected may themselves represent more complicated combinations of these propositions. Since it is possible to implement the logical connections not, and, and or by means of neural connections and appropriate thresholds of the neurons, one can represent every conceivable finite logical combination of the elementary propositions in a neural network.},
	language = {en},
	booktitle = {Brain {Theory}},
	publisher = {Springer},
	author = {Palm, G.},
	editor = {Palm, Günther and Aertsen, Ad},
	year = {1986},
	pages = {229--230}
}

@article{hornik_multilayer_1989,
	title = {Multilayer feedforward networks are universal approximators},
	volume = {2},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
	doi = {10.1016/0893-6080(89)90020-8},
	abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
	language = {en},
	number = {5},
	urldate = {2020-09-22},
	journal = {Neural Networks},
	author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
	month = jan,
	year = {1989},
	keywords = {Back-propagation networks, Feedforward networks, Mapping networks, Network representation capability, Sigma-Pi networks, Squashing functions, Stone-Weierstrass Theorem, Universal approximation},
	pages = {359--366},
	file = {ScienceDirect Snapshot:/Users/shaan/Zotero/storage/N5GDZDFU/0893608089900208.html:text/html}
}

@article{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2020-09-22},
	journal = {arXiv:1512.03385 [cs]},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.03385},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/TCH63X2U/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/CNZIC7SU/1512.html:text/html}
}

@article{rhone_data-driven_2018,
	title = {Data-driven studies of magnetic two-dimensional materials},
	url = {http://arxiv.org/abs/1806.07989},
	abstract = {We use a data-driven approach to study the magnetic and thermodynamic properties of van der Waals (vdW) layered materials. We investigate monolayers of the form A\$\_2\$B\$\_2\$X\$\_6\$, based on the known material Cr\$\_2\$Ge\$\_2\$Te\$\_6\$, using density functional theory (DFT) calculations and machine learning methods to determine their magnetic properties, such as magnetic order and magnetic moment. We also examine formation energies and use them as a proxy for chemical stability. We show that machine learning tools, combined with DFT calculations, can provide a computationally efficient means to predict properties of such two-dimensional (2D) magnetic materials. Our data analytics approach provides insights into the microscopic origins of magnetic ordering in these systems. For instance, we find that the X site strongly affects the magnetic coupling between neighboring A sites, which drives the magnetic ordering. Our approach opens new ways for rapid discovery of chemically stable vdW materials that exhibit magnetic behavior.},
	urldate = {2020-09-22},
	journal = {arXiv:1806.07989 [cond-mat]},
	author = {Rhone, Trevor David and Chen, Wei and Desai, Shaan and Yacoby, Amir and Kaxiras, Efthimios},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.07989},
	keywords = {Condensed Matter - Materials Science},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/JINNFJU8/Rhone et al. - 2018 - Data-driven studies of magnetic two-dimensional ma.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/DZSV2LLQ/1806.html:text/html}
}

@article{fedus_revisiting_nodate,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	language = {en},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	pages = {34},
	file = {Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:/Users/shaan/Zotero/storage/HY6ZZ3NH/Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:application/pdf}
}

@article{fedus_revisiting_nodate-1,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	language = {en},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	pages = {34},
	file = {Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:/Users/shaan/Zotero/storage/X95K8KQR/Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:application/pdf}
}

@article{fedus_revisiting_nodate-2,
	title = {Revisiting {Fundamentals} of {Experience} {Replay}},
	language = {en},
	author = {Fedus, William and Ramachandran, Prajit and Agarwal, Rishabh and Bengio, Yoshua and Larochelle, Hugo and Rowland, Mark and Dabney, Will},
	pages = {34},
	file = {Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:/Users/shaan/Zotero/storage/U4FP7UI3/Fedus et al. - Revisiting Fundamentals of Experience Replay.pdf:application/pdf}
}

@article{shlomi_graph_2020,
	title = {Graph {Neural} {Networks} in {Particle} {Physics}},
	url = {http://arxiv.org/abs/2007.13681},
	abstract = {Particle physics is a branch of science aiming at discovering the fundamental laws of matter and forces. Graph neural networks are trainable functions which operate on graphs -- sets of elements and their pairwise relations -- and are a central method within the broader field of geometric deep learning. They are very expressive and have demonstrated superior performance to other classical deep learning approaches in a variety of domains. The data in particle physics are often represented by sets and graphs and as such, graph neural networks offer key advantages. Here we review various applications of graph neural networks in particle physics, including different graph constructions, model architectures and learning objectives, as well as key open problems in particle physics for which graph neural networks are promising.},
	urldate = {2020-09-28},
	journal = {arXiv:2007.13681 [hep-ex, physics:hep-ph]},
	author = {Shlomi, Jonathan and Battaglia, Peter and Vlimant, Jean-Roch},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.13681},
	keywords = {High Energy Physics - Experiment, High Energy Physics - Phenomenology},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/5ACTDZ2K/Shlomi et al. - 2020 - Graph Neural Networks in Particle Physics.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/EZSULQFR/2007.html:text/html}
}

@article{hundsdorfer_error_2015,
	title = {Error {Analysis} of {Explicit} {Partitioned} {Runge}–{Kutta} {Schemes} for {Conservation} {Laws}},
	volume = {63},
	issn = {1573-7691},
	url = {https://doi.org/10.1007/s10915-014-9906-1},
	doi = {10.1007/s10915-014-9906-1},
	abstract = {An error analysis is presented for explicit partitioned Runge–Kutta methods and multirate methods applied to conservation laws. The interfaces, across which different methods or time steps are used, lead to order reduction of the schemes. Along with cell-based decompositions, also flux-based decompositions are studied. In the latter case mass conservation is guaranteed, but it will be seen that the accuracy may deteriorate.},
	language = {en},
	number = {3},
	urldate = {2020-09-29},
	journal = {Journal of Scientific Computing},
	author = {Hundsdorfer, Willem and Ketcheson, David I. and Savostianov, Igor},
	month = jun,
	year = {2015},
	pages = {633--653},
	file = {Springer Full Text PDF:/Users/shaan/Zotero/storage/624JY7IP/Hundsdorfer et al. - 2015 - Error Analysis of Explicit Partitioned Runge–Kutta.pdf:application/pdf}
}

@article{hessel_rainbow_2017,
	title = {Rainbow: {Combining} {Improvements} in {Deep} {Reinforcement} {Learning}},
	shorttitle = {Rainbow},
	url = {http://arxiv.org/abs/1710.02298},
	abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
	urldate = {2020-10-01},
	journal = {arXiv:1710.02298 [cs]},
	author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.02298},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, RL},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/R3JJ9WNC/Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/WQL5I6CJ/1710.html:text/html}
}

@misc{noauthor_redirecting_nodate,
	title = {Redirecting},
	url = {https://www.google.com/url?q=https://simons.berkeley.edu/talks/tbd-217&sa=D&source=hangouts&ust=1601627182559000&usg=AFQjCNGhxNiM6zqlXH15T8TA3STONsvDqA},
	urldate = {2020-10-01},
	file = {Redirecting:/Users/shaan/Zotero/storage/FZ4Z36G2/url.html:text/html}
}

@misc{noauthor_temporally-extended_nodate,
	title = {Temporally-{Extended} ε-{Greedy} {Exploration} {\textbar} {Simons} {Institute} for the {Theory} of {Computing}},
	url = {https://simons.berkeley.edu/talks/tbd-217},
	urldate = {2020-10-01},
	file = {Temporally-Extended ε-Greedy Exploration | Simons Institute for the Theory of Computing:/Users/shaan/Zotero/storage/UUZI8ANJ/tbd-217.html:text/html}
}

@article{dabney_temporally-extended_2020,
	title = {Temporally-{Extended} \{{\textbackslash}epsilon\}-{Greedy} {Exploration}},
	url = {http://arxiv.org/abs/2006.01782},
	abstract = {Recent work on exploration in reinforcement learning (RL) has led to a series of increasingly complex solutions to the problem. This increase in complexity often comes at the expense of generality. Recent empirical studies suggest that, when applied to a broader set of domains, some sophisticated exploration methods are outperformed by simpler counterparts, such as \{{\textbackslash}epsilon\}-greedy. In this paper we propose an exploration algorithm that retains the simplicity of \{{\textbackslash}epsilon\}-greedy while reducing dithering. We build on a simple hypothesis: the main limitation of \{{\textbackslash}epsilon\}-greedy exploration is its lack of temporal persistence, which limits its ability to escape local optima. We propose a temporally extended form of \{{\textbackslash}epsilon\}-greedy that simply repeats the sampled action for a random duration. It turns out that, for many duration distributions, this suffices to improve exploration on a large set of domains. Interestingly, a class of distributions inspired by ecological models of animal foraging behaviour yields particularly strong performance.},
	urldate = {2020-10-01},
	journal = {arXiv:2006.01782 [cs, stat]},
	author = {Dabney, Will and Ostrovski, Georg and Barreto, André},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.01782},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/UCGCGAJA/Dabney et al. - 2020 - Temporally-Extended epsilon -Greedy Exploration.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/BTL3C38W/2006.html:text/html}
}

@misc{noauthor_deepminddqn_zoo_2020,
	title = {deepmind/dqn\_zoo},
	copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
	url = {https://github.com/deepmind/dqn_zoo},
	abstract = {Contribute to deepmind/dqn\_zoo development by creating an account on GitHub.},
	urldate = {2020-10-01},
	publisher = {DeepMind},
	month = oct,
	year = {2020},
	note = {original-date: 2020-09-22T11:57:54Z}
}

@misc{huang_steeve_introduction_2018,
	title = {Introduction to {Various} {Reinforcement} {Learning} {Algorithms}. {Part} {I} ({Q}-{Learning}, {SARSA}, {DQN}, {DDPG})},
	url = {https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287},
	abstract = {Reinforcement Learning (RL) refers to a kind of Machine Learning method in which the agent receives a delayed reward in the next time step…},
	language = {en},
	urldate = {2020-10-01},
	journal = {Medium},
	author = {Huang (Steeve), Kung-Hsiang},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/shaan/Zotero/storage/AL29WL47/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6.html:text/html}
}

@article{zahavy_self-tuning_2020,
	title = {A {Self}-{Tuning} {Actor}-{Critic} {Algorithm}},
	url = {http://arxiv.org/abs/2002.12928},
	abstract = {Reinforcement learning algorithms are highly sensitive to the choice of hyperparameters, typically requiring significant manual effort to identify hyperparameters that perform well on a new domain. In this paper, we take a step towards addressing this issue by using metagradients to automatically adapt hyperparameters online by meta-gradient descent (Xu et al., 2018). We apply our algorithm, Self-Tuning Actor-Critic (STAC), to self-tune all the differentiable hyperparameters of an actor-critic loss function, to discover auxiliary tasks, and to improve off-policy learning using a novel leaky V-trace operator. STAC is simple to use, sample efficient and does not require a significant increase in compute. Ablative studies show that the overall performance of STAC improved as we adapt more hyperparameters. When applied to the Arcade Learning Environment (Bellemare et al. 2012), STAC improved the median human normalized score in \$200\$M steps from \$243{\textbackslash}\%\$ to \$364{\textbackslash}\%\$. When applied to the DM Control suite (Tassa et al., 2018), STAC improved the mean score in \$30\$M steps from \$217\$ to \$389\$ when learning with features, from \$108\$ to \$202\$ when learning from pixels, and from \$195\$ to \$295\$ in the Real-World Reinforcement Learning Challenge (Dulac-Arnold et al., 2020).},
	urldate = {2020-10-01},
	journal = {arXiv:2002.12928 [cs, stat]},
	author = {Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Singh, Satinder},
	month = jul,
	year = {2020},
	note = {arXiv: 2002.12928},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/EM4X7GZP/Zahavy et al. - 2020 - A Self-Tuning Actor-Critic Algorithm.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/ADKTQRKS/2002.html:text/html}
}

@misc{noauthor_stellardnn_nodate,
	title = {{StellarDNN} {Research} {Lab} {Presentations}},
	url = {https://docs.google.com/spreadsheets/d/1qAGvphJHzVjZJQmZM8LI9rDOPm_Qd6Iq8KBjekjO9jE/edit?usp=embed_facebook},
	abstract = {Sheet1

Date,Long-ish 1,Long-ish 2,Short-ish 1 (2-3 slides),Short-ish 2 (2-3 slides),Single slide 1,Single slide 2
10/2/2020,Graphene NN: Henry,Global/Local Neuron: Tiago,EHT BH: Tao Tsui
10/9/2020,Irregular Time Series Forecasting: Cristobal
10/16/2020,PP: Gentle introduction to transformers 
10...},
	language = {en},
	urldate = {2020-10-02},
	journal = {Google Docs},
	file = {Snapshot:/Users/shaan/Zotero/storage/K5V3EWKH/edit.html:text/html}
}

@article{huang_learning_2020,
	title = {Learning {Interpretable} and {Thermodynamically} {Stable} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2009.13415},
	abstract = {In this work, we develop a method for learning interpretable and thermodynamically stable partial differential equations (PDEs) based on the Conservation-dissipation Formalism of irreversible thermodynamics. As governing equations for non-equilibrium flows in one dimension, the learned PDEs are parameterized by fully-connected neural networks and satisfy the conservation-dissipation principle automatically. In particular, they are hyperbolic balance laws. The training data are generated from a kinetic model with smooth initial data. Numerical results indicate that the learned PDEs can achieve good accuracy in a wide range of Knudsen numbers. Remarkably, the learned dynamics can give satisfactory results with randomly sampled discontinuous initial data although it is trained only with smooth initial data.},
	urldate = {2020-10-07},
	journal = {arXiv:2009.13415 [physics]},
	author = {Huang, Juntao and Ma, Zhiting and Zhou, Yizhou and Yong, Wen-An},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.13415},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Mathematics - Numerical Analysis, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/AD429DR4/Huang et al. - 2020 - Learning Interpretable and Thermodynamically Stabl.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/QIQXIE5W/2009.html:text/html}
}

@article{sedaghat_machines_2020,
	title = {Machines {Learn} to {Infer} {Stellar} {Parameters} {Just} by {Looking} at a {Large} {Number} of {Spectra}},
	url = {http://arxiv.org/abs/2009.12872},
	abstract = {Machine learning has been widely applied to clearly defined problems of astronomy and astrophysics. However, deep learning and its conceptual differences to classical machine learning have been largely overlooked in these fields. The broad hypothesis behind our work is that letting the abundant real astrophysical data speak for itself, with minimal supervision and no labels, can reveal interesting patterns which may facilitate discovery of novel physical relationships. Here as the first step, we seek to interpret the representations a deep convolutional neural network chooses to learn, and find correlations in them with current physical understanding. We train an encoder-decoder architecture on the self-supervised auxiliary task of reconstruction to allow it to learn general representations without bias towards any specific task. By exerting weak disentanglement at the information bottleneck of the network, we implicitly enforce interpretability in the learned features. We develop two independent statistical and information-theoretical methods for finding the number of learned informative features, as well as measuring their true correlation with astrophysical validation labels. As a case study, we apply this method to a dataset of {\textasciitilde}270000 stellar spectra, each of which comprising {\textasciitilde}300000 dimensions. We find that the network clearly assigns specific nodes to estimate (notions of) parameters such as radial velocity and effective temperature without being asked to do so, all in a completely physics-agnostic process. This supports the first part of our hypothesis. Moreover, we find with high confidence that there are {\textasciitilde}4 more independently informative dimensions that do not show a direct correlation with our validation parameters, presenting potential room for future studies.},
	urldate = {2020-10-07},
	journal = {arXiv:2009.12872 [astro-ph]},
	author = {Sedaghat, Nima and Romaniello, Martino and Carrick, Jonathan E. and Pineau, François-Xavier},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12872},
	keywords = {Astrophysics - Instrumentation and Methods for Astrophysics},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/2UX6TLLL/Sedaghat et al. - 2020 - Machines Learn to Infer Stellar Parameters Just by.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/HAAHJ22B/2009.html:text/html}
}

@misc{noauthor_elsevier_nodate,
	title = {Elsevier {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S2590054420300270?token=966F4BB067D2BD69A962650E13F7CAC91F9071470ADC013E1A804054B3B03F85E7C6326CDB4A0A0BAA9DC091E51FC5AB},
	language = {en},
	urldate = {2020-10-07},
	doi = {10.1016/j.csfx.2020.100046},
	file = {Snapshot:/Users/shaan/Zotero/storage/YJBW6VRU/S2590054420300270.html:text/html}
}

@article{queiruga_continuous--depth_2020,
	title = {Continuous-in-{Depth} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2008.02389},
	abstract = {Recent work has attempted to interpret residual networks (ResNets) as one step of a forward Euler discretization of an ordinary differential equation, focusing mainly on syntactic algebraic similarities between the two systems. Discrete dynamical integrators of continuous dynamical systems, however, have a much richer structure. We first show that ResNets fail to be meaningful dynamical integrators in this richer sense. We then demonstrate that neural network models can learn to represent continuous dynamical systems, with this richer structure and properties, by embedding them into higher-order numerical integration schemes, such as the Runge Kutta schemes. Based on these insights, we introduce ContinuousNet as a continuous-in-depth generalization of ResNet architectures. ContinuousNets exhibit an invariance to the particular computational graph manifestation. That is, the continuous-in-depth model can be evaluated with different discrete time step sizes, which changes the number of layers, and different numerical integration schemes, which changes the graph connectivity. We show that this can be used to develop an incremental-in-depth training scheme that improves model quality, while significantly decreasing training time. We also show that, once trained, the number of units in the computational graph can even be decreased, for faster inference with little-to-no accuracy drop.},
	urldate = {2020-10-07},
	journal = {arXiv:2008.02389 [cs, math, stat]},
	author = {Queiruga, Alejandro F. and Erichson, N. Benjamin and Taylor, Dane and Mahoney, Michael W.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.02389},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/6LBGGPNA/Queiruga et al. - 2020 - Continuous-in-Depth Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/IQ29K8IQ/2008.html:text/html}
}

@article{wang_multi-scale_2020,
	title = {Multi-scale {Deep} {Neural} {Network} ({MscaleDNN}) {Methods} for {Oscillatory} {Stokes} {Flows} in {Complex} {Domains}},
	url = {http://arxiv.org/abs/2009.12729},
	abstract = {In this paper, we study a multi-scale deep neural network (MscaleDNN) as a meshless numerical method for computing oscillatory Stokes flows in complex domains. The MscaleDNN employs a multiscale structure in the design of its DNN using radial scalings to convert the approximation of high frequency components of the highly oscillatory Stokes solution to one of lower frequencies. The MscaleDNN solution to the Stokes problem is obtained by minimizing a loss function in terms of \$L{\textasciicircum}2\$ norm of the residual of the Stokes equation. Three forms of loss functions are investigated based on vorticity-velocity-pressure, velocity-stress-pressure, and velocity gradient-velocity-pressure formulations of the Stokes equation. We first conduct a systematic study of the MscaleDNN methods with various loss functions on the Kovasznay flow in comparison with normal fully connected DNNs. Then, Stokes flows with highly oscillatory solutions in a 2-D domain with six randomly placed holes are simulated by the MscaleDNN. The results show that MscaleDNN has faster convergence and consistent error decays than normal fully connnected DNNs in the simulation of Kovasznay flow for all four tested loss functions. More importantly, the MscaleDNN is capable of learning highly oscillatory solutions while the normal DNNs fail to converge.},
	urldate = {2020-10-07},
	journal = {arXiv:2009.12729 [physics]},
	author = {Wang, Bo and Zhang, Wenzhong and Cai, Wei},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12729},
	keywords = {Physics - Computational Physics, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/QNQTPPL9/Wang et al. - 2020 - Multi-scale Deep Neural Network (MscaleDNN) Method.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/CB9QMYQ3/2009.html:text/html}
}

@article{rudy_deep_2019,
	title = {Deep learning of dynamics and signal-noise decomposition with time-stepping constraints},
	volume = {396},
	issn = {00219991},
	url = {http://arxiv.org/abs/1808.02578},
	doi = {10.1016/j.jcp.2019.06.056},
	abstract = {A critical challenge in the data-driven modeling of dynamical systems is producing methods robust to measurement error, particularly when data is limited. Many leading methods either rely on denoising prior to learning or on access to large volumes of data to average over the effect of noise. We propose a novel paradigm for data-driven modeling that simultaneously learns the dynamics and estimates the measurement noise at each observation. By constraining our learning algorithm, our method explicitly accounts for measurement error in the map between observations, treating both the measurement error and the dynamics as unknowns to be identified, rather than assuming idealized noiseless trajectories. We model the unknown vector field using a deep neural network, imposing a Runge-Kutta integrator structure to isolate this vector field, even when the data has a non-uniform timestep, thus constraining and focusing the modeling effort. We demonstrate the ability of this framework to form predictive models on a variety of canonical test problems of increasing complexity and show that it is robust to substantial amounts of measurement error. We also discuss issues with the generalizability of neural network models for dynamical systems and provide open-source code for all examples.},
	urldate = {2020-10-08},
	journal = {Journal of Computational Physics},
	author = {Rudy, Samuel H. and Kutz, J. Nathan and Brunton, Steven L.},
	month = nov,
	year = {2019},
	note = {arXiv: 1808.02578},
	keywords = {Mathematics - Numerical Analysis},
	pages = {483--506},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/CP2N3GWD/Rudy et al. - 2019 - Deep learning of dynamics and signal-noise decompo.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/EBIVK2KH/1808.html:text/html}
}

@article{franke_sample-efficient_2020,
	title = {Sample-{Efficient} {Automated} {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2009.01555},
	abstract = {Despite significant progress in challenging problems across various domains, applying state-of-the-art deep reinforcement learning (RL) algorithms remains challenging due to their sensitivity to the choice of hyperparameters. This sensitivity can partly be attributed to the non-stationarity of the RL problem, potentially requiring different hyperparameter settings at various stages of the learning process. Additionally, in the RL setting, hyperparameter optimization (HPO) requires a large number of environment interactions, hindering the transfer of the successes in RL to real-world applications. In this work, we tackle the issues of sample-efficient and dynamic HPO in RL. We propose a population-based automated RL (AutoRL) framework to meta-optimize arbitrary off-policy RL algorithms. In this framework, we optimize the hyperparameters and also the neural architecture while simultaneously training the agent. By sharing the collected experience across the population, we substantially increase the sample efficiency of the meta-optimization. We demonstrate the capabilities of our sample-efficient AutoRL approach in a case study with the popular TD3 algorithm in the MuJoCo benchmark suite, where we reduce the number of environment interactions needed for meta-optimization by up to an order of magnitude compared to population-based training.},
	urldate = {2020-10-08},
	journal = {arXiv:2009.01555 [cs, stat]},
	author = {Franke, Jörg K. H. and Köhler, Gregor and Biedenkapp, André and Hutter, Frank},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.01555},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/2FWQG35U/Franke et al. - 2020 - Sample-Efficient Automated Deep Reinforcement Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/N6K8BI4P/2009.html:text/html}
}

@article{yu_onsagernet_2020,
	title = {{OnsagerNet}: {Learning} {Stable} and {Interpretable} {Dynamics} using a {Generalized} {Onsager} {Principle}},
	shorttitle = {{OnsagerNet}},
	url = {http://arxiv.org/abs/2009.02327},
	abstract = {We propose a systematic method for learning stable and interpretable dynamical models using sampled trajectory data from physical processes based on a generalized Onsager principle. The learned dynamics are autonomous ordinary differential equations parameterized by neural networks that retain clear physical structure information, such as free energy, dissipation, conservative interaction and external force. The neural network representations for the hidden dynamics are trained by minimizing the empirical risk based on an embedded Runge-Kutta method. For high dimensional problems with a low dimensional slow manifold, an autoencoder with isometric regularization is introduced to find generalized coordinates on which we learn the Onsager dynamics. We apply the method to learn reduced order models for the Rayleigh-B{\textbackslash}'\{e\}nard convection problem, where we obtain low dimensional autonomous equations that capture both qualitative and quantitative properties of the underlying dynamics. In particular, this validates the basic approach of Lorenz, although we also discover that the dimension of the learned autonomous model required for faithful representation increases with the Rayleigh number.},
	urldate = {2020-10-09},
	journal = {arXiv:2009.02327 [physics]},
	author = {Yu, Haijun and Tian, Xinyuan and E, Weinan and Li, Qianxiao},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.02327},
	keywords = {Computer Science - Machine Learning, Physics - Computational Physics, Mathematics - Dynamical Systems, 76E30, 34D20, 68T05/07, 82C35},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/8NSC9UMJ/Yu et al. - 2020 - OnsagerNet Learning Stable and Interpretable Dyna.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/6BZPQ4WV/2009.html:text/html}
}

@article{ho_population_nodate,
	title = {Population {Based} {Augmentation}:  {Efficient} {Learning} of {Augmentation} {Policy} {Schedules}},
	abstract = {A key challenge in leveraging data augmentation for neural network training is choosing an effective augmentation policy from a large search space of candidate operations. Properly chosen augmentation policies can lead to signiﬁcant generalization improvements; however, state-of-theart approaches such as AutoAugment are computationally infeasible to run for the ordinary user. In this paper, we introduce a new data augmentation algorithm, Population Based Augmentation (PBA), which generates nonstationary augmentation policy schedules instead of a ﬁxed augmentation policy. We show that PBA can match the performance of AutoAugment on CIFAR-10, CIFAR100, and SVHN, with three orders of magnitude less overall compute. On CIFAR-10 we achieve a mean test error of 1.46\%, which is a slight improvement upon the current state-of-the-art. The code for PBA is open source and is available at https://github.com/arcelien/pba.},
	language = {en},
	author = {Ho, Daniel and Liang, Eric and Stoica, Ion and Abbeel, Pieter and Chen, Xi},
	pages = {11},
	file = {Ho et al. - Population Based Augmentation  Efficient Learning.pdf:/Users/shaan/Zotero/storage/HM4GTGZI/Ho et al. - Population Based Augmentation  Efficient Learning.pdf:application/pdf}
}

@article{raileanu_automatic_2020,
	title = {Automatic {Data} {Augmentation} for {Generalization} in {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2006.12862},
	abstract = {Deep reinforcement learning (RL) agents often fail to generalize to unseen scenarios, even when they are trained on many instances of semantically similar environments. Data augmentation has recently been shown to improve the sample efficiency and generalization of RL agents. However, different tasks tend to benefit from different kinds of data augmentation. In this paper, we compare three approaches for automatically finding an appropriate augmentation. These are combined with two novel regularization terms for the policy and value function, required to make the use of data augmentation theoretically sound for certain actor-critic algorithms. We evaluate our methods on the Procgen benchmark which consists of 16 procedurally-generated environments and show that it improves test performance by {\textasciitilde}40\% relative to standard RL algorithms. Our agent outperforms other baselines specifically designed to improve generalization in RL. In addition, we show that our agent learns policies and representations that are more robust to changes in the environment that do not affect the agent, such as the background. Our implementation is available at https://github.com/rraileanu/auto-drac.},
	urldate = {2020-10-29},
	journal = {arXiv:2006.12862 [cs]},
	author = {Raileanu, Roberta and Goldstein, Max and Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.12862},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/MJS3H4BM/Raileanu et al. - 2020 - Automatic Data Augmentation for Generalization in .pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/RT9SEU4A/2006.html:text/html}
}

@article{li_fourier_2020,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and the Navier-Stokes equation (including the turbulent regime). Our Fourier neural operator shows state-of-the-art performance compared to existing neural network methodologies and it is up to three orders of magnitude faster compared to traditional PDE solvers.},
	urldate = {2020-10-31},
	journal = {arXiv:2010.08895 [cs, math]},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08895},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/SPUL9E6Y/Li et al. - 2020 - Fourier Neural Operator for Parametric Partial Dif.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/AY25JHQZ/2010.html:text/html}
}

@article{finzi_simplifying_2020,
	title = {Simplifying {Hamiltonian} and {Lagrangian} {Neural} {Networks} via {Explicit} {Constraints}},
	url = {http://arxiv.org/abs/2010.13581},
	abstract = {Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.},
	urldate = {2020-10-31},
	journal = {arXiv:2010.13581 [physics, stat]},
	author = {Finzi, Marc and Wang, Ke Alexander and Wilson, Andrew Gordon},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.13581},
	keywords = {Computer Science - Machine Learning, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning, Physics - Computational Physics, Mathematics - Dynamical Systems},
	file = {arXiv Fulltext PDF:/Users/shaan/Zotero/storage/S7YSHLIR/Finzi et al. - 2020 - Simplifying Hamiltonian and Lagrangian Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/Users/shaan/Zotero/storage/PA9YSE49/2010.html:text/html}
}
